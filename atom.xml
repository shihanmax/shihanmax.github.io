<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shihanmax</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shihanmax.github.io/"/>
  <updated>2020-06-21T05:53:48.086Z</updated>
  <id>https://shihanmax.github.io/</id>
  
  <author>
    <name>MaShihan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>序列标注实践</title>
    <link href="https://shihanmax.github.io/2020/05/10/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%AE%9E%E8%B7%B5/"/>
    <id>https://shihanmax.github.io/2020/05/10/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%AE%9E%E8%B7%B5/</id>
    <published>2020-05-10T09:16:03.000Z</published>
    <updated>2020-06-21T05:53:48.086Z</updated>
    
    <content type="html"><![CDATA[<p>序列标注问题是一类典型的NLP问题，具体应用有：分词、词性标注、命名实体识别等。</p><p>序列标注问题的定义是：给定序列$S$，期望通过模型M得到序列中每一个token对应的标签序列$Z$，这里定义，$S$中每个词token的可能情况有$n_{word}$中，标签序列$Z$中的每一个标签tag的可能情况有$n_{tag}$种。</p><p>本文从噪声信道模型的角度对序列标注问题进行建模，并通过维特比算法优化最优路径的搜索。</p><a id="more"></a><p>定义：$S=w_1w_2w_3…w_{N}$，对应的任一种标签序列为$Z=z_1z_2z_3…z_{N}$，目标是寻找最优的标签序列$\hat{Z}$。</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}  \hat{Z}&=\mathop{argmax}_\limits{z}(P(Z\vert S)) \\&=\mathop{argmax}_\limits{z}(P(S\vert Z)\cdot P(Z)) \\&=\mathop{argmax}_\limits{z}(P(w_1w_2...w_N\vert z_1z_2...z_N) \cdot  P(z_1z_2...z_N))   \end{aligned}  \end{equation}</script><p>上式中乘积的第一项为Translation model（TM）、第二项为Language model（LM）。</p><p>在TM中引入独立假设，在LM中引入bi-gram假设，可得：</p><script type="math/tex; mode=display">\hat{Z}=\mathop{argmax}_\limits{z}( \prod_\limits{i=1}^{N}P(w_i\vert z_i)\cdot P(z_1)\cdot \prod_\limits{j=2}^N P(z_j\vert z_{j-1}))</script><p>对概率进行对数化：</p><script type="math/tex; mode=display">\hat{Z}=\mathop{argmax}_\limits{z}( \sum_\limits{i=1}^{N} logP(w_i\vert z_i)+ log P(z_1)+ \sum_\limits{j=2}^N logP(z_j\vert z_{j-1}))</script><p>相加的三项中，第一项为由标签到词的条件概率，记为$A$；第二项为标签出现在句首的概率，记为$B$，第三项为标签之间的转移概率，记为$\pi$。</p><p>在有标注的数据集上，上述三项的概率能够通过统计得到。</p><p>对于一条待标注的文本$S_u$，长度为$N$，其所有可能的标签序列有$n_{tag}^N$种，对所有情况可以计算概率$P(Z_i\vert S)$，取概率最大的标签序列即可。</p><p>实际应用中，枚举所有种可能的效率是非常低的，因此需要使用更高效的算法来对路径进行剪枝。维特比算法是一种动态规划算法，用于多个步骤且每个步骤中有多种选择的问题的最优路径选择。维特比算法通过计算所有前序步骤到当前步骤的最小代价（或最大收益），以及当前步骤做选择时的代价（收益）来进行步骤的选择，最后，通过回溯法来选择最优路径。</p><p>下面是上述过程在词性标注任务上的一种实现。</p><p>训练数据的格式为：每行：词/词性</p><p>代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_mapper</span><span class="params">(train_data)</span>:</span></span><br><span class="line">    <span class="string">""" build token mapper and tag mapper """</span></span><br><span class="line">    tag2id = &#123;&#125;</span><br><span class="line">    id2tag = &#123;&#125;</span><br><span class="line">    word2id = &#123;&#125;</span><br><span class="line">    id2word = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    word_count = <span class="number">0</span></span><br><span class="line">    tag_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(train_data) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">            sp_line = line.strip().split(<span class="string">"/"</span>)</span><br><span class="line">            <span class="keyword">if</span> len(sp_line) != <span class="number">2</span>:</span><br><span class="line">                tag = sp_line[<span class="number">-1</span>]</span><br><span class="line">                word = <span class="string">"/"</span>.join(sp_line[:<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                word = sp_line[<span class="number">0</span>]</span><br><span class="line">                tag = sp_line[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word2id:</span><br><span class="line">                word2id[word] = word_count</span><br><span class="line">                id2word[word_count] = word</span><br><span class="line"></span><br><span class="line">                word_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> tag <span class="keyword">not</span> <span class="keyword">in</span> tag2id:</span><br><span class="line">                tag2id[tag] = tag_count</span><br><span class="line">                id2tag[tag_count] = tag</span><br><span class="line"></span><br><span class="line">                tag_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    n_word = len(word2id)</span><br><span class="line">    n_tag = len(tag2id)</span><br><span class="line">    print(<span class="string">"word size:&#123;&#125;"</span>.format(n_word))</span><br><span class="line">    print(<span class="string">"tag size:&#123;&#125;"</span>.format(n_tag))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> word2id, id2word, tag2id, id2tag, n_word, n_tag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_params</span><span class="params">(word2id, tag2id, n_tag, n_word, train_data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    build params of HMM: theta = (pi, A, B)</span></span><br><span class="line"><span class="string">    - pi is a vector marks the probe of each tag to be the first tag</span></span><br><span class="line"><span class="string">      of a sentence, size: [1, n_tag]</span></span><br><span class="line"><span class="string">    - A is a matrix of condition probe of words given tags,size: [n_tag, n_word]</span></span><br><span class="line"><span class="string">    - B is a matrix of transition probe between tags, size: [n_tag, n_tag]"""</span></span><br><span class="line"></span><br><span class="line">    pi = np.zeros(n_tag)  <span class="comment"># probe of starting tag</span></span><br><span class="line">    a = np.zeros((n_tag, n_word))  <span class="comment"># condition probe p(word|tag)</span></span><br><span class="line">    b = np.zeros((n_tag, n_tag))  <span class="comment"># transition probe p(tag_i|tag_j)</span></span><br><span class="line"></span><br><span class="line">    at_start_of_sentence = <span class="literal">True</span></span><br><span class="line">    last_tag_id = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(train_data) <span class="keyword">as</span> fr:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">            word, tag = line.strip().split(<span class="string">"/"</span>)</span><br><span class="line">            word_id = word2id.get(word)</span><br><span class="line">            tag_id = tag2id.get(tag)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> at_start_of_sentence:</span><br><span class="line">                pi[tag_id] += <span class="number">1</span>  <span class="comment"># starting prob</span></span><br><span class="line">                a[tag_id, word_id] += <span class="number">1</span>  <span class="comment"># cond. prob</span></span><br><span class="line">                at_start_of_sentence = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a[tag_id, word_id] += <span class="number">1</span>  <span class="comment"># cond. prob</span></span><br><span class="line">                b[last_tag_id, tag_id] += <span class="number">1</span>  <span class="comment"># trans. probe</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> word == <span class="string">"."</span>:</span><br><span class="line">                at_start_of_sentence = <span class="literal">True</span></span><br><span class="line">                last_tag_id = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                last_tag_id = tag_id</span><br><span class="line"></span><br><span class="line">    <span class="comment"># done counting, normalize...</span></span><br><span class="line">    pi /= sum(pi)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_tag):</span><br><span class="line">        a[i] /= sum(a[i])</span><br><span class="line">        b[i] /= sum(b[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pi, a, b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> num == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> np.log(<span class="number">1e-8</span>)</span><br><span class="line">    <span class="keyword">return</span> np.log(num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(sentence, pi, a, b, word2id, id2tag)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    decode with viterbi</span></span><br><span class="line"><span class="string">    :param sentence: sentence to decode</span></span><br><span class="line"><span class="string">    :param pi: init probe of tag</span></span><br><span class="line"><span class="string">    :param a: cond probe of words given tags</span></span><br><span class="line"><span class="string">    :param b: trans probe between tags</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = [word2id[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split()]  <span class="comment"># words of sentence</span></span><br><span class="line"></span><br><span class="line">    t = len(x)</span><br><span class="line">    n_tag = len(tag2id)</span><br><span class="line"></span><br><span class="line">    dp = np.zeros((t, n_tag))</span><br><span class="line">    path_record = np.array([[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_tag)] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_tag)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n_tag):</span><br><span class="line">        dp[<span class="number">0</span>][j] = log(pi[j]) + log(a[j, x[<span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dp</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, t):  <span class="comment"># 每个单词</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n_tag):  <span class="comment"># 每个词性</span></span><br><span class="line">            dp[i][j] = <span class="number">-1e6</span></span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> range(n_tag):</span><br><span class="line">                score = dp[i<span class="number">-1</span>][k] + log(b[k][j]) + log(a[j, x[i]])</span><br><span class="line">                <span class="keyword">if</span> score &gt; dp[i, j]:</span><br><span class="line">                    dp[i, j] = score</span><br><span class="line">                    path_record[i, j] = k</span><br><span class="line"></span><br><span class="line">    <span class="comment"># find best sequence</span></span><br><span class="line">    best_tag_sequence = [<span class="number">0</span>] * t</span><br><span class="line"></span><br><span class="line">    best_tag_sequence[t<span class="number">-1</span>] = np.argmax(dp[t<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(t<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        best_tag_sequence[i] = path_record[i + <span class="number">1</span>, best_tag_sequence[i+<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    tag_sequence = [id2tag[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> best_tag_sequence]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tag_sequence</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_data_path = <span class="string">"./resource/traindata.txt"</span></span><br><span class="line">test_sentence = <span class="string">"The big question is whether the president will have the strength ."</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. build mapper of words and tags</span></span><br><span class="line">word2id, id2word, tag2id, id2tag, n_word, n_tag = build_mapper(train_data_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. \theta = (\pi, A, B), build them</span></span><br><span class="line">pi, a, b = build_params(word2id, tag2id, n_tag, n_word, train_data_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. find optimal path with viterbi</span></span><br><span class="line"></span><br><span class="line">tag_sequence = viterbi(test_sentence, pi, a, b, word2id, id2tag)</span><br><span class="line">print(tag_sequence)  <span class="comment"># ['DT', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'MD', 'VB', 'DT', 'NN', '.']</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;序列标注问题是一类典型的NLP问题，具体应用有：分词、词性标注、命名实体识别等。&lt;/p&gt;
&lt;p&gt;序列标注问题的定义是：给定序列$S$，期望通过模型M得到序列中每一个token对应的标签序列$Z$，这里定义，$S$中每个词token的可能情况有$n_{word}$中，标签序列$Z$中的每一个标签tag的可能情况有$n_{tag}$种。&lt;/p&gt;
&lt;p&gt;本文从噪声信道模型的角度对序列标注问题进行建模，并通过维特比算法优化最优路径的搜索。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://shihanmax.github.io/categories/NLP/"/>
    
    
      <category term="pos tagging" scheme="https://shihanmax.github.io/tags/pos-tagging/"/>
    
  </entry>
  
  <entry>
    <title>动态时间规整（DTW）算法</title>
    <link href="https://shihanmax.github.io/2020/02/25/%E5%8A%A8%E6%80%81%E6%97%B6%E9%97%B4%E8%A7%84%E6%95%B4DTW%E7%AE%97%E6%B3%95/"/>
    <id>https://shihanmax.github.io/2020/02/25/%E5%8A%A8%E6%80%81%E6%97%B6%E9%97%B4%E8%A7%84%E6%95%B4DTW%E7%AE%97%E6%B3%95/</id>
    <published>2020-02-25T10:02:20.000Z</published>
    <updated>2020-03-18T05:07:32.550Z</updated>
    
    <content type="html"><![CDATA[<p>工作中遇到了涉及不等长序列的近似度计算的问题，这里把阅读过的资料整理一下，方便以后参考。</p><a id="more"></a><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>在序列相似度比较的任务中，对于一些在某个维度上可以延展或者压缩的信号来说，如果直接使用传统的序列距离计算方法（如欧式距离）来衡量序列的相似度的话，是不合适的。（如同一个人慢速和快速讲同一句话。）</p><p>这个时候，我们需要对待比较的若干组序列信息从某个维度上进行延展或压缩，使得序列之间更好地对其（如波峰、波谷的对其），对语音信号来说，也就是对语音波形在时间轴上进行规整，DTW算法称这种操作为Wrapping。</p><p><img src="http://shihanmax.top/20200225190714_4seWYG_Screenshot.jpeg" alt="dtw_1"></p><h2 id="2-DTW"><a href="#2-DTW" class="headerlink" title="2. DTW"></a>2. DTW</h2><p>这里以两个待比较的时间序列$P$、$Q$为例：$P$的长度为$m$、$Q$的长度为$n$，则序列$P$、$Q$分别为：</p><script type="math/tex; mode=display">P=p_1,p_2,...p_i,...p_m</script><script type="math/tex; mode=display">Q=q_1,q_2,...,q_j,...q_n</script><p>目标：合理地计算出序列$P$、$Q$的相似度</p><p>无论$m$和$n$是否相等，我们都需要对序列中的元素进行对齐，（比如线性缩放，但线性缩放明显不满足我们的要求。）</p><p>为了对其$P$和$Q$，需要构造一个$m \times n$的矩阵$A$，其中$A(i,j)$代表$P_i$和$Q_j$两个元素的距离$d(P_i,Q_j)$，这个距离计算函数$d(\cdot)$可以是适合的相似度计算方法，如简单的欧氏距离。对齐点的寻找相当于在矩阵A中寻找一个经过若干个格点的一条路径，该路径上任一点$P(x,y)$表示序列中$P_x$与$Q_y$对齐。</p><p><img src="http://shihanmax.top/20200225190809_bC3SkL_Screenshot.jpeg" alt="dtw_matrix"></p><p>我们将；路径定义为Wrapping Path，用$W$表示，$W$的第$k$个元素为：</p><script type="math/tex; mode=display">W_k=(i,j)_k</script><script type="math/tex; mode=display">W=w_1,w_2,...,w_K\quad \quad max(m,n)\le K\lt m+n-1</script><p>路径W应满足一下约束：</p><ol><li>边界条件：$W_1=(1,1)$，$W_K=(m,n)$，即两序列的起始和终止位置必须对齐</li><li>连续性：如果$W_{k-1}=(a’,b’)$，则$W_k=(a,b)$必须满足$(a-a’) \le 1$且$(b-b’) \le 1$</li><li>单调性：如果$W_{k-1}=(a’,b’)$，则$W_k=(a,b)$必须满足$(a-a’) \ge 0$ 且$(b-b’) \ge 0$，此条保证路径随时间单调进行</li></ol><p>通过2、3两个条件的约束，在路径$W$的任意位置，寻找下一个路径点时，路径的行进方向仅有三种可能：</p><p><img src="http://shihanmax.top/20200225174006_6P6yzF_20130620200949125.jpeg" alt="下一个路径点的寻找方向"></p><p>满足上述约束条件的路径数量有指数多种，我们要寻找的是，使得规整代价最小的路径:</p><script type="math/tex; mode=display">DTW(P,Q)=min(\sqrt{\sum_{k=1}^K} /K)</script><p>DTW的思想是将两个序列在时间维度上进行延伸和压缩，最终得到一个最短的路径，这个距离也即是两个序列的距离度量。</p><p>最短路径可以使用动态规划计算，定义累加距离$\gamma$，在矩阵$A$的某一格$A(i,j)$上，累加距离的递推公式为：</p><script type="math/tex; mode=display">\gamma_{i,j}=d(p_i,q_j)+min(\gamma_{i-1,j-1},\gamma_{i-1,j},\gamma_{i, j-1})</script><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://blog.csdn.net/zouxy09/article/details/9140207" target="_blank" rel="noopener">1. 语音信号处理之（一）动态时间规整（DTW）</a></p><p><a href="https://en.wikipedia.org/wiki/Dynamic_time_warping" target="_blank" rel="noopener">2. Wikipedia-DTW</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工作中遇到了涉及不等长序列的近似度计算的问题，这里把阅读过的资料整理一下，方便以后参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithm" scheme="https://shihanmax.github.io/categories/Algorithm/"/>
    
    
      <category term="序列相似度" scheme="https://shihanmax.github.io/tags/%E5%BA%8F%E5%88%97%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    
      <category term="DTW" scheme="https://shihanmax.github.io/tags/DTW/"/>
    
  </entry>
  
  <entry>
    <title>How transferable are features in deep neural networks</title>
    <link href="https://shihanmax.github.io/2019/08/22/How%20transferable%20are%20features%20in%20deep%20neural%20networks/"/>
    <id>https://shihanmax.github.io/2019/08/22/How%20transferable%20are%20features%20in%20deep%20neural%20networks/</id>
    <published>2019-08-22T11:38:33.000Z</published>
    <updated>2020-03-29T13:59:03.850Z</updated>
    
    <content type="html"><![CDATA[<p>Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson</p><p>本文通过实验考察从网络第一层的强泛化能力的特征到最后一层的任务相关的特征之间的转变过程。对神经元的泛化能力（generality）以及特异性（specificity）进行量化分析。</p><p>迁移能力 受两个因素影响：</p><ul><li>后层的神经元更专注于特定的任务</li><li>优化自适应（co-adapted）神经元的困难</li></ul><a id="more"></a><p>我们在 ImageNet 上的实验表明，以上两个因素均有占主导的时候，取决于特征的转移过程发生在网络的前、中、后层；另外发现，两个任务的差异性越大，泛化能力就越差，但仍然超过随机特征的效果。最后的发现是，无论初始化时使用了多少层与训练权重，在 finetune 到特定数据集之后，仍然保持泛化能力。</p><p>我们称第一层的特征为general特征，最后一层的特征为specific特征。那么在两层中间一定存在着从 genral到specific之间的转变，那么问题来了：</p><ul><li>我们可以量化某一层的general或者specific的度吗？</li><li>转变发生在某一层，还是分布在几层之间？</li><li>转变发生在网络的前部、中部、还是后部？</li></ul><p>我们关心这些问题的原因是，如果我们能够掌握上述转变发生的位置，我们就能够更有效地利用迁移学习，其有效性已经得到广泛认可。在迁移学习中，我们可以冻结transferred feature，仅训练后层随机初始化的网络，或者将它们随后层任务一起微调（finetune），这取决于新领域的数据集规模。</p><h4 id="Experiment-results"><a href="#Experiment-results" class="headerlink" title="Experiment results"></a>Experiment results</h4><p><img src="http://shihanmax.top/截屏2019-08-22下午7.35.42.png" alt="The results from this paper’s main experiment."></p><p><img src="http://shihanmax.top/截屏2019-08-22下午7.36.29.png" alt="Performance degradation vs. layer."></p><h4 id="本文贡献："><a href="#本文贡献：" class="headerlink" title="本文贡献："></a>本文贡献：</h4><ol><li>定义一种量化各层泛化能力的方法</li><li>发现以下两种会破坏性能的点：<ul><li>特征自身的专一性</li><li>optimization difficulties due to splitting the base network between co-adapted neurons on neighboring layers</li></ul></li><li>量化研究了随着任务差异性的提升，泛化能力如何下降</li><li>对比了浅层随机初始化和迁移学习的效果，后者更好</li><li>我们发现前层权重在新的数据集上微调后，在旧的数据集上的效果依然保持</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson&lt;/p&gt;
&lt;p&gt;本文通过实验考察从网络第一层的强泛化能力的特征到最后一层的任务相关的特征之间的转变过程。对神经元的泛化能力（generality）以及特异性（specificity）进行量化分析。&lt;/p&gt;
&lt;p&gt;迁移能力 受两个因素影响：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后层的神经元更专注于特定的任务&lt;/li&gt;
&lt;li&gt;优化自适应（co-adapted）神经元的困难&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://shihanmax.github.io/categories/NLP/"/>
    
    
      <category term="transfer learning" scheme="https://shihanmax.github.io/tags/transfer-learning/"/>
    
  </entry>
  
  <entry>
    <title>青甘环线</title>
    <link href="https://shihanmax.github.io/2019/06/20/%E9%9D%92%E7%94%98%E7%8E%AF%E7%BA%BF/"/>
    <id>https://shihanmax.github.io/2019/06/20/%E9%9D%92%E7%94%98%E7%8E%AF%E7%BA%BF/</id>
    <published>2019-06-20T08:18:22.000Z</published>
    <updated>2020-03-18T17:34:34.700Z</updated>
    
    <content type="html"><![CDATA[<p>以此文纪念印象深刻的西北毕业旅行。</p><a id="more"></a><h2 id="1-出发"><a href="#1-出发" class="headerlink" title="1. 出发"></a>1. 出发</h2><p>2019年6月17号，拍完毕业照后下起小雨，我们排着队去领学校发放的“毕业西瓜”。</p><p><img src="http://shihanmax.top/20200319005529_OhSbW6_IMG_4668.jpeg" alt="领西瓜咯"></p><p>队排得很长，不过比起吃不上西瓜，我们更担心的是，这突然下起的雨会不会耽误计划已久的行程。</p><p><img src="http://shihanmax.top/20200319005225_bkbdM5_2E88D3EC-0BB3-4A88-A28E-5B703748E3E6_1_105_c.jpeg" alt="天气" style="zoom:50%;"></p><p>好在西瓜会有的，彩虹也会有的。</p><p><img src="http://shihanmax.top/20200319005915_LpKcpX_EAD8EE97-CB4C-45AE-BE1F-BE4FB8810D9A_1_105_c.jpeg" alt="西瓜"></p><p>出发前趴在宿舍阳台吃西瓜时，天空升起一道彩虹。</p><p><img src="http://shihanmax.top/20200319010029_bkRhfj_4D69EC7B-A684-422E-94C9-23C1B1ECCA87_1_105_c.jpeg" alt="彩虹"></p><center>彩虹</center><h2 id="2-路上"><a href="#2-路上" class="headerlink" title="2. 路上"></a>2. 路上</h2><p><img src="http://shihanmax.top/20200319010246_qcLwMO_4386ADC1-7472-47A8-A922-00805CFF7C3E_1_105_c.jpeg" alt="途中气定神闲的牦牛"></p><center>几只“闲云野牛”</center><p><img src="http://shihanmax.top/20200319010317_tBLehU_1A552BB9-FB8C-44A7-83C8-2C512DD80A3B_1_105_c.jpeg" alt="四个人四个动作"></p><center>四个人四种姿势</center><p>低低的云和长长的路。</p><p><img src="http://shihanmax.top/20200319010735_La3UR2_AAC762F9-51BA-4C35-ABFA-08FE2C1123C0_1_105_c.jpeg" alt="1"></p><center>途中停歇</center><p>一路上的天气总是那么好，绿绿的草原、蔚蓝的天。</p><p><img src="http://shihanmax.top/20200319010851_chCT5t_6A7E6C39-A634-4C37-B620-69BAEEF42D37_1_105_c.jpeg" alt="1"></p><center>草原和蓝天</center><p>白云就在头上。</p><p><img src="http://shihanmax.top/20200319011050_iWYB0M_E0976F6F-0054-4D0F-98B3-C1384AC6C812_1_105_c.jpeg" alt="2"></p><center>刮胡子</center><p>这条路至今印象深刻。</p><p><img src="http://shihanmax.top/20200319011216_qqmXiF_636C6BB0-CEFF-409D-8D1C-B528EF026047_1_105_c.jpeg" alt="2"></p><center>去往茶卡盐湖的路上</center><p><img src="http://shihanmax.top/20200319011552_7xCymR_12CDA5E1-8987-4DFB-B10A-ACD93AD44953_1_105_c.jpeg" alt="caka"></p><center>茶卡盐湖摆渡小火车的迷你轨道</center><p>离开茶卡盐湖，我们向青海湖进发。</p><p><img src="http://shihanmax.top/20200319011656_8hOMdA_30636723-B9F6-41B4-AF58-C70769A8F985_1_105_c.jpeg" alt="1"></p><center>随处可见的羊群</center><p>深邃的湖水使人沉静。</p><p><img src="http://shihanmax.top/20200319011821_cc1LlL_8284A566-0A4D-4EB9-8BAE-B593451A6400_1_105_c.jpeg" alt="1"></p><center>青海湖</center><p>祁连县海拔4120的垭口。</p><p><img src="http://shihanmax.top/20200319012006_nSBqwB_41A362AB-CBDC-476F-8BA5-729C3384FB47_1_105_c.jpeg" alt></p><center>4120</center><p><img src="http://shihanmax.top/20200319012140_DfpDZx_DE413579-C229-47E6-B946-1B5C63D434C2_1_105_c.jpeg" alt></p><center>蜿蜒曲折的山路</center><p>到达五彩斑斓的张掖。</p><p><img src="http://shihanmax.top/20200319012248_KzN1sw_02CD9ACD-5722-432A-90BB-BB433E0A50BD_1_105_c.jpeg" alt></p><center>七彩丹霞</center><p>画完心形环线，回到此程第一个目的地——兰州市。</p><p><img src="http://shihanmax.top/20200319012420_oVozrn_F05627FE-3A59-40B9-A584-A8CA0C310B72_1_105_c.jpeg" alt></p><center>心环线</center><p>5天1892公里，短暂的旅行结束了，一路上说说笑笑，不知不觉一起走了那么远的路。</p><p>三年的读研生涯也是一样，开学时的场景还在眼前，转眼就各奔东西了。</p><p><img src="http://shihanmax.top/20200319013244_dDYgMX_DB8C497F-1EBA-4F17-9219-259EAF27AE61_1_105_c.jpeg" alt="1"></p><center>2020年6月16日于图书馆前的草坪</center><p>各自珍重！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以此文纪念印象深刻的西北毕业旅行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="旅行" scheme="https://shihanmax.github.io/categories/%E6%97%85%E8%A1%8C/"/>
    
    
      <category term="青海" scheme="https://shihanmax.github.io/tags/%E9%9D%92%E6%B5%B7/"/>
    
      <category term="甘肃" scheme="https://shihanmax.github.io/tags/%E7%94%98%E8%82%83/"/>
    
      <category term="青海湖" scheme="https://shihanmax.github.io/tags/%E9%9D%92%E6%B5%B7%E6%B9%96/"/>
    
  </entry>
  
  <entry>
    <title>语义相似度</title>
    <link href="https://shihanmax.github.io/2019/03/28/%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <id>https://shihanmax.github.io/2019/03/28/%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6/</id>
    <published>2019-03-27T16:00:00.000Z</published>
    <updated>2020-03-19T13:37:44.530Z</updated>
    
    <content type="html"><![CDATA[<p>论文《APPLYING DEEP LEARNING TO ANSWER SELECTION:<br>A STUDY AND AN OPEN TASK》阅读笔记</p><p>文章整理并开源了首个保险领域的问答数据集，在此数据集上探讨了不同CNN架构以及不同的相似度评价方法对语义相似度模型效果的影响。</p><a id="more"></a><h1 id="1-问题设定"><a href="#1-问题设定" class="headerlink" title="1. 问题设定"></a>1. 问题设定</h1><p>在QA任务中，给定问题$q$和答案候选集$\{a_1, a_2, …, a_n\}$，目标是从候选集中选出最正确的答案$a_k$（如果存在）。这个问题可以转化为一个文本二分类任务：针对一个问题-答案对$\{q,a_i\}$，确定是否是一对正确的QA对，而关于”正确”，则需要一种文本距离衡量手段，经典的有cosine similarity，本文提出了两种相似度计算方案GESD和 AESD（包含超参数），并在数据集上对比了超参数不同的情况下，各种相似度计算方案的效果。</p><h1 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2. 模型架构"></a>2. 模型架构</h1><p>在训练时，使用预训练的词向量，将词向量求和作为问题$q$，候选答案$a$的表示向量。</p><p>总体思路是：首先通过CNN计算出问题$q$和答案$a$的向量表示，然后使用相似度衡量指标来计算二者之间的相似度。</p><p>本文提出了六种基于CNN的文本分类架构：</p><ol><li>$q$和$a$进入独立的隐藏层和卷积层，进入pooling层后得到各自的表示向量</li><li>$q$和$a$进入共享的隐藏层和卷积层，进入pooling层后得到各自的表示向量</li><li>$q$和$a$进入共享的隐藏层和卷积层，进入pooling层后，进入独立的隐藏层，得到各自的表示向量</li><li>$q$和$a$进入共享的隐藏层和卷积层，进入pooling层后，进入共享的隐藏层，得到各自的表示向量</li><li>$q$和$a$进入共享的隐藏层和卷积层1、卷积层2，进入pooling层后得到各自的表示向量</li><li>$q$和$a$进入共享的隐藏层和卷积层1、卷积层2，将卷积层1的跳远连接和卷积层2求和后进入pooling层后得到各自的表示向量</li></ol><h1 id="3-相似度计算方法汇总"><a href="#3-相似度计算方法汇总" class="headerlink" title="3. 相似度计算方法汇总"></a>3. 相似度计算方法汇总</h1><ol><li><p>Cosine</p><script type="math/tex; mode=display">k(x,y)=\cfrac{xy^T}{\lVert x\rVert \lVert y\rVert}</script></li><li><p>Polinomial</p><script type="math/tex; mode=display">k(x,y)=(\gamma xy^T+c)d</script></li><li><p>Sigmoid</p><script type="math/tex; mode=display">k(x,y)=tanh(\gamma xy^T+c)</script></li><li><p>RBF</p><script type="math/tex; mode=display">k(x,y)=exp(-\gamma \lVert x-y \rVert_2)</script></li><li><p>euclidean</p><script type="math/tex; mode=display">k(x,y)=\cfrac{1}{1+\lVert x-y \rVert}</script></li><li><p>exponential</p><script type="math/tex; mode=display">k(x,y)=exp(-\gamma \lVert x-y \rVert_1)</script></li><li><p>manhattan</p><script type="math/tex; mode=display">k(x,y)=\cfrac{1}{1+\lVert x-y \rVert}_1</script></li><li><p>GESD（Geometric mean of Euclidean and Sigmoid Dot product）</p><script type="math/tex; mode=display">k(x,y)=\cfrac{1}{1+\lVert x-y \rVert} \cdot \cfrac{1}{1+exp(-\gamma (xy^T+c))}</script></li><li><p>AESD（Arithmetic mean of Euclidean and Sigmoid Dot product）</p><script type="math/tex; mode=display">k(x,y)=\cfrac{0.5}{1+\lVert x-y \rVert} \cdot \cfrac{0.5}{1+exp(-\gamma (xy^T+c))}</script></li></ol><h1 id="4-结果讨论"><a href="#4-结果讨论" class="headerlink" title="4. 结果讨论"></a>4. 结果讨论</h1><ol><li><p>CNN模型较baseline模型有很大的提升</p></li><li><p>在现有数据上，结构2的表现最优，分析原因：$q$与$a$共享卷积层能够保证抽取的特征在同一个空间中，如果不共享卷积层，在数据集有限的情况下，模型很难将两个并列的卷积层的统一；</p></li><li><p>卷积层后再加一层隐藏层会降低模型的性能，卷积层已经将有用的特征抽取出来了，没有必要再将特征映射到另一个空间中</p></li><li><p>增加卷积核数量可以抽取出更多的特征，对模型性能提升有明显帮助</p></li><li><p>两层CNN能够对特征进行更高层次的抽象</p></li><li><p>深层次的DNN训练有一定难度，加入类似于resNet中的跳远连接结构有助于缓解这个现象</p></li><li><p>相似度衡量方法也很重要</p><p>第3节中讨论的相似度计算方法大致分类三类：</p><ol><li>基于L1范数的：semantic distance of Q and A summed from each coordinate axis</li><li>基于L2范数的：straight-line semantic distance of Q and A</li><li>基于内积的：the angle between Q and A</li></ol><p>我们提出的*ESD是基于L2和基于内积的结合，在模型评估中表现最好。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文《APPLYING DEEP LEARNING TO ANSWER SELECTION:&lt;br&gt;A STUDY AND AN OPEN TASK》阅读笔记&lt;/p&gt;
&lt;p&gt;文章整理并开源了首个保险领域的问答数据集，在此数据集上探讨了不同CNN架构以及不同的相似度评价方法对语义相似度模型效果的影响。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://shihanmax.github.io/categories/NLP/"/>
    
    
      <category term="text classification" scheme="https://shihanmax.github.io/tags/text-classification/"/>
    
  </entry>
  
  <entry>
    <title>Recent advances in conversational NLP</title>
    <link href="https://shihanmax.github.io/2019/03/26/Chatbot%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/"/>
    <id>https://shihanmax.github.io/2019/03/26/Chatbot%E7%A0%94%E7%A9%B6%E8%BF%9B%E5%B1%95/</id>
    <published>2019-03-25T16:00:00.000Z</published>
    <updated>2019-08-17T07:44:11.890Z</updated>
    
    <content type="html"><![CDATA[<p>论文《Recent advances in conversational NLP :<br>Towards the standardization of Chatbot building》阅读笔记</p><p>From：Maali Mnasri（maali@opla.ai）</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>chatbot在日常生活中越来越重要，易用程度也在不断提升。本文针对目前的对话系统作了综述性研究，再文章最后讨论了各个实现方法的优缺点对比，并提出了对话系统标准化的思考。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>Conversational agents（与后文dialogue system、以及工业和媒体使用的chatbots义同，简称CA）越来越常见，如个人手持设备的个人助理、商业网站的售货机器人等，这些系统旨在通过语音或文本与人类进行流畅的对话沟通。在AI领域，建立智能的CA仍是一个未解决的难题。本文总结了目前流行的对话系统实现方法，并对主流实现方法进行了对比。</p><p>尽管距第一个CA被提出已经过去很多年了，但这个领域仍然越来越活跃，这与近年来AI、NLP领域技术的迅速发展有关。目前有很多丰富的方法来实现CA，但能够使研究者专注于CA性能提升的工具却很缺乏，本文第6节会讨论相关问题。</p><a id="more"></a><p>下文如下组织：</p><ul><li>分析当下CA设计的SOTA技术</li><li>关于CA设计标准化提出了新的思考</li></ul><h1 id="2-Chatbots-categories"><a href="#2-Chatbots-categories" class="headerlink" title="2 Chatbots categories"></a>2 Chatbots categories</h1><p>我们将CA分为两类：聊天机器人（social chatbots）和任务型对话机器人（task oriented chatbots）。</p><h2 id="2-1-Social-chatbots"><a href="#2-1-Social-chatbots" class="headerlink" title="2.1 Social chatbots"></a>2.1 Social chatbots</h2><p>聊天机器人（也常常被称为chit-chat bots）的娱乐意义多一些，但在最开始，它们的设计目的是作为心理诊疗顾问，甚至有一些现在仍在使用。ELIZA,PARRY,ALICE,CLEVER,微软小冰等在CA领域迈出了第一步。</p><h2 id="2-2-Task-oriented-chatbots"><a href="#2-2-Task-oriented-chatbots" class="headerlink" title="2.2 Task oriented chatbots"></a>2.2 Task oriented chatbots</h2><p>我们将任务型对话机器人也分为两类：通用任务型和特殊任务型。</p><h3 id="2-2-1-Generalist-task-oriented-chatbots"><a href="#2-2-1-Generalist-task-oriented-chatbots" class="headerlink" title="2.2.1 Generalist task oriented chatbots"></a>2.2.1 Generalist task oriented chatbots</h3><p>能够回答一些通用问题（和chit-chat类似），可以进行简单的对话，并能够完成一些日常的任务，如设置闹钟、打电话、发短信等。</p><h3 id="2-2-2-Specialist-task-oriented-chatbots"><a href="#2-2-2-Specialist-task-oriented-chatbots" class="headerlink" title="2.2.2 Specialist task oriented chatbots"></a>2.2.2 Specialist task oriented chatbots</h3><p>针对某些特殊领域设计，通常以来这些领域的知识来解决稍微复杂的问题，如预定航班、订餐、分析健康问题等。</p><h1 id="3-Chatbot-building-approches"><a href="#3-Chatbot-building-approches" class="headerlink" title="3 Chatbot building approches"></a>3 Chatbot building approches</h1><p>本章介绍两种主流的CA设计方法：基于规则的和基于数据的。前者在设计时主要依赖模式-动作规则（<a href="https://stackoverflow.com/questions/380724/what-is-the-action-design-pattern" target="_blank" rel="noopener">pattern-action</a>），而后者则需要大量的对话语料。</p><h2 id="3-1-rule-based-chatbots"><a href="#3-1-rule-based-chatbots" class="headerlink" title="3.1 rule-based chatbots"></a>3.1 rule-based chatbots</h2><p>基于规则的CA使用前缀规则进行设计，它按照特定的规则与用户交互，例如，如果用户的话中包含 [Hello”, ”Good morning”, ”Hi”] 中的一个，则CA应该回复”Hello”。规则对话机器人由于设计简单、能够完成简单的额任务，目前很受欢迎，但对复杂任务则需要设计大量的人工规则，很费时费力。ELIZA是第一个规则对话系统，设计目的是为了模仿一位心理诊疗师，它的设计原则是应用模式和转变规则（applying pattern and transform rules），每一条转换规则都对应一个关键词，关键词按”特别-&gt;一般”来排序。针对每一条用户的话，chatbot通过对照知识库，按照转变规则来寻找排序最靠前的关键词。以下面的一句话为例：</p><script type="math/tex; mode=display">You \space hate\space me</script><p>这句话和如下的模式匹配：</p><script type="math/tex; mode=display">(0\space YOU\space 0\space ME)</script><p>0表示一段文字的长度变量，我们假设关键词YOU使用下述的转变规则：</p><script type="math/tex; mode=display">(\space YOU\space 0\space ME) \rightarrow (WHAT\space MAKES \space YOU \space THINK\space I\space 3\space YOU)</script><p>3表示用户的话中的第3个词，本例中也就是”hate”，应用转变规则后，规则对话机器人应该返回如下答案：</p><script type="math/tex; mode=display">WHAT\space MAKES \space YOU \space THINK\space I\space hate\space YOU</script><p>在ELIZA后，PARRY系统也使用了类似的规则系统，不同的是，PARRY在设计时加入了影响变量（affect variables），能够表现出生气、害怕等情绪。</p><h2 id="3-2-Data-driven-chatbots"><a href="#3-2-Data-driven-chatbots" class="headerlink" title="3.2 Data-driven chatbots"></a>3.2 Data-driven chatbots</h2><p>目前多数CA使用基于数据驱动的手段。它依赖大量的对话语料，不需要手工规则设计，我们接下来介绍基于信息抽取的对话机器人和基于机器学习的对话机器人。</p><h3 id="3-2-1-Information-retrieval-based-chatbots"><a href="#3-2-1-Information-retrieval-based-chatbots" class="headerlink" title="3.2.1 Information retrieval based chatbots"></a>3.2.1 Information retrieval based chatbots</h3><p>Clever-Bot设计于1988年，1997年发布，它依赖数据库中与问题相似的答案来回答人类的问题。就像一个搜索引擎一样。针对用户的问题Q，IR CA会从数据库中寻找与Q最相近的问答对(Q’,R’)，将其回答返回给用户。那么如何衡量语句之间的相似性？一种方式是基于字向量计算余弦距离；另外一些基于TF-IDF的检索模型使用discourse tree和RST（Rhetorical Structure Theory）来对语句生成或相似答案进行建模。除了一些对话语料之外，也可以使用WikiAnswers、YahooAnswers、推特对话等构建语料库。</p><h3 id="3-2-2-Machine-learning-based-chatbots"><a href="#3-2-2-Machine-learning-based-chatbots" class="headerlink" title="3.2.2 Machine learning based chatbots"></a>3.2.2 Machine learning based chatbots</h3><p>目前，主要的深度学习方法有seq2seq learning和强化学习两种，下面分别介绍。</p><h4 id="3-2-2-1-Seq2seq-learning"><a href="#3-2-2-1-Seq2seq-learning" class="headerlink" title="3.2.2.1 Seq2seq learning"></a>3.2.2.1 Seq2seq learning</h4><p>Seq2seq使用RNN对复杂语段进行建模，用于机器翻译、图片描述、语音识别、文本摘要、自动问答等领域。在机器翻译领域的成功显示出Seq2seq技术的优越性。在对话机器人设计中，类似地，可以看作将用户的话翻译为机器人的回答。seq2seq模型可以对输入和输出进行映射，二者的长度可以不同，技术上，它由一个编码器和一个解码器组成，编码器接收输入，并把他转化为中间向量（context vector），中间向量被输入到解码器中，得到seq2seq的输出。seq2seq可以回答简单的问题，抽取相关信息，并具有一些简单的推理能力，但还不能进行合理的对话。（显然，由于RNN的设计特点，长距离特征的保持和记忆的能力与Attention模型有一定的差距。）</p><p>后来也有研究者在seq2seq模型中加入对用户情绪的建模，在语句生成阶段考虑到了情绪的影响。</p><h4 id="3-2-2-2-Reinforcement-learning"><a href="#3-2-2-2-Reinforcement-learning" class="headerlink" title="3.2.2.2 Reinforcement learning"></a>3.2.2.2 Reinforcement learning</h4><p>强化学习使机器能够像人一样学习，也即通过与环境的交互，最大化某种累计奖励。</p><p>在神经网络建模流行之前，人们将对话系统建模为一个马尔可夫决策过程（Markov Decision Processes<br>，MDPs），为了使用强化学习，使用一组与对话相关的状态来表示对话系统。目标是最大化通过满足用户请求而得到的奖励。后来，使用部分可见马尔可夫决策过程来对对话建模（Partially Observed Markov Decision Pro-<br>cesses ，POMDPs），此方法假设对话有一个初始状态$s_0$，后续的状态由前一个状态和前一轮action $a_t$决定：</p><script type="math/tex; mode=display">p(s_t \mid s_{t-1}, a_{t-1})</script><p>为了将SLU模块误差考虑进来，$s_t$部分可见，至此，每一轮的用户输入转化为概率$p(o_t \mid s_t)$，其中$o_t$是$t$时刻的观察状态。观察概率和转移概率由对话模型$M$决定。可能的系统action将转移给策略模型，在对话过程中，如果系统执行了正确的action，会得到相应的奖励。</p><h3 id="3-2-3-Hybrid-approches"><a href="#3-2-3-Hybrid-approches" class="headerlink" title="3.2.3 Hybrid approches"></a>3.2.3 Hybrid approches</h3><p>有研究将上述方法结合，如Alibaba购物机器人，使用IR手段检索最合适的几个答案，并通过seq2seq对它们进行重排序，并生成答案。</p><h1 id="4-Chatbots-evaluation"><a href="#4-Chatbots-evaluation" class="headerlink" title="4 Chatbots evaluation"></a>4 Chatbots evaluation</h1><p>对对话系统进行人工评估耗费巨大，一些IR based CA可以使用简单的PR指标进行评估。自动化的方法BLEU、ROUGE等分别用于机器翻译和文本摘要，他们通过计算系统输出和参考答案之间的n-gram重合度来评估。对话系统中，这些评估方式可以计算系统回答和人类回答的匹配度。但这种方式也有缺点，在一些场景下，两段文本可能n-gram完全没有重合，但语义却是相似的。</p><p>困惑度（Perplexity）也可以用来做CA评估，它最开始用于评估语言模型，我们假设测试集由$w_1,w_2,…,w_n$组成，那么语言模型预测出这些词的困惑度为：</p><script type="math/tex; mode=display">Perplexity=e^{-\frac{1}{N} \sum log( P_{w_i})}</script><p>意味着，Perplexity越小越好。</p><h1 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5 Discussion"></a>5 Discussion</h1><p>各种类型的CA都各有优缺点，应该根据已有的数据（数据量、结构化/非结构化）和CA的用途来决定使用哪一种。</p><p>机器学习占据了目前对话领域的主流，而强化学习在对话系统设计中已经广泛使用，编码-解码学习成为主流。</p><p>强化学习使得模型更能对长距离多轮回话进行建模，因为如果长轮次会话中模型出现错误决策时，也会受到惩罚。但强化学习有两个缺点：</p><ol><li>需要大量的训练时间和训练轮次</li><li>不是语言生成的最佳选项</li></ol><p>生成式seq2seq模型已经部分克服了上述缺点，但他也有一些缺点，如倾向于生成安全回答，而且无法考虑重复现象，如：</p><blockquote><p>Cus: Goodbye!</p><p>Bot: Goodbye!</p><p>Cus: Goodbye!</p><p>Bot: Goodbye!</p><p>…</p></blockquote><p>强化学习和seq2seq结合的策略有望解决上述问题。</p><h1 id="6-Opinion-piece-towards-the-standardization-of-the-field"><a href="#6-Opinion-piece-towards-the-standardization-of-the-field" class="headerlink" title="6 Opinion piece : towards the standardization of the field"></a>6 Opinion piece : towards the standardization of the field</h1><p>关于当前对话机器人设计的思考：目前许多研究者在设计对话系统时，由于没有统一框架和标准，大家都是在重复造轮子，其实有一些模块是可以复用的，所以下面的对话机器人研究是否能够着眼于基础架构的搭建，然后使研究者以插件的形式设计对话机器人。</p><h1 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h1><p>单一模型不能解决实际问题，模型整合才能更好适应工业需求。</p><p>对话框架的标准化。</p><h1 id="后记-阅读心得"><a href="#后记-阅读心得" class="headerlink" title="后记-阅读心得"></a>后记-阅读心得</h1><ol><li>关于chatbot设计方面作了综述，基本框架与《自然语言处理综论》中相关章节基本相似，其中seq2seq相关部分并没有提到目前SOTA的Attention模型；</li><li>多轮对话没有涉及；</li><li>副标题是Towards the standardization of Chatbot building，但仅指出了目前chatbot设计不统一的现象，关于对话系统设计标准化，作者其实可以写的更丰富些。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文《Recent advances in conversational NLP :&lt;br&gt;Towards the standardization of Chatbot building》阅读笔记&lt;/p&gt;
&lt;p&gt;From：Maali Mnasri（maali@opla.ai）&lt;/p&gt;
&lt;h1 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h1&gt;&lt;p&gt;chatbot在日常生活中越来越重要，易用程度也在不断提升。本文针对目前的对话系统作了综述性研究，再文章最后讨论了各个实现方法的优缺点对比，并提出了对话系统标准化的思考。&lt;/p&gt;
&lt;h1 id=&quot;1-Introduction&quot;&gt;&lt;a href=&quot;#1-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1 Introduction&quot;&gt;&lt;/a&gt;1 Introduction&lt;/h1&gt;&lt;p&gt;Conversational agents（与后文dialogue system、以及工业和媒体使用的chatbots义同，简称CA）越来越常见，如个人手持设备的个人助理、商业网站的售货机器人等，这些系统旨在通过语音或文本与人类进行流畅的对话沟通。在AI领域，建立智能的CA仍是一个未解决的难题。本文总结了目前流行的对话系统实现方法，并对主流实现方法进行了对比。&lt;/p&gt;
&lt;p&gt;尽管距第一个CA被提出已经过去很多年了，但这个领域仍然越来越活跃，这与近年来AI、NLP领域技术的迅速发展有关。目前有很多丰富的方法来实现CA，但能够使研究者专注于CA性能提升的工具却很缺乏，本文第6节会讨论相关问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://shihanmax.github.io/categories/NLP/"/>
    
    
      <category term="Chatbot" scheme="https://shihanmax.github.io/tags/Chatbot/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法之逻辑回归(Logistic Regression)</title>
    <link href="https://shihanmax.github.io/2019/03/19/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(Logistic%20Regression)/"/>
    <id>https://shihanmax.github.io/2019/03/19/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92(Logistic%20Regression)/</id>
    <published>2019-03-18T16:00:00.000Z</published>
    <updated>2020-05-13T16:22:15.110Z</updated>
    
    <content type="html"><![CDATA[<h3 id="A-模型推导"><a href="#A-模型推导" class="headerlink" title="A. 模型推导"></a>A. 模型推导</h3><p>现有样本集合$\{X,y\}$，$X$为特征$\{x_1,x_2,…,x_n\}$，$y$为实数，线性回归中，$X$与$y$之间存在映射关系$y=h_\theta(x)$，其中，</p><script type="math/tex; mode=display">h_\theta(x)=\sum \theta_ ix_i</script><p>在逻辑回归中，$y$属于集合$\{0,1\}$，$1$表示属于某类别，而$0$表示不属于该类别。</p><p>期望得到模型$M$，使得$M(x)\in[0,1]$，这样能够较为方便地衡量$x$属于该类别的概率。</p><a id="more"></a><p>$sigmoid$函数可以满足这个需求：</p><script type="math/tex; mode=display">sigmoid(x)=\cfrac{1}{1+e^{-x}}</script><p>逻辑回归的$h_\theta(x)$可以写成：</p><script type="math/tex; mode=display">h_ \theta (x)=\cfrac{1}{1+e^ {- \theta x}}</script><p>对正样本:</p><script type="math/tex; mode=display">P({y=1 \mid x, \theta})=h_ \theta(x)</script><p>对负样本:</p><script type="math/tex; mode=display">P({y=0 \mid x, \theta})=1-h_ \theta(x)</script><p>将二者合并，有:</p><script type="math/tex; mode=display">P({y \mid \theta})={h_ \theta(x)}^y{1-h_ \theta(x)}^{1-y}</script><p>为方便求解，采用最大化似然函数求解参数$\theta$，对$m$个样本，似然函数$L(\theta)$为：</p><script type="math/tex; mode=display">L(\theta)=\prod P(y \mid \theta)=\prod {h_ \theta(x)}^y{1-h_ \theta(x)}^{1-y}</script><p>对似然函数取对数并取负，可得损失函数$J(\theta)$：</p><script type="math/tex; mode=display">J(\theta)=-lnL(\theta)=-\sum {yh_ \theta(x)}+{(1-y)(1-h_ \theta(x))}</script><p>将$J(\theta)$对$\theta$求导，可得</p><script type="math/tex; mode=display">\cfrac{\partial{}}{\partial{\theta}}J(\theta)=x^T(h_\theta (x)-y)</script><p>使用梯度下降法迭代求解：</p><script type="math/tex; mode=display">\theta = \theta - \alpha x^T(h_\theta(x)-y)</script><p>其中$\alpha$为学习率。</p><h3 id="B-正则化"><a href="#B-正则化" class="headerlink" title="B. 正则化"></a>B. 正则化</h3><p>为避免过拟合问题，可对模型施加$L1$或$L2$正则化项，$Loss$的定义：</p><p>$Loss_{L1}=\beta \mid\mid\theta\mid\mid_1$</p><p>$Loss_{L2}=\cfrac{1}{2}\beta \mid\mid\theta\mid\mid_2^2$</p><p>其中，$\beta$为正则化超参数。</p><p>训练时，将$Loss_{L1/2}$与$J(\theta)$相加，并执行梯度下降。</p><h3 id="C-多分类"><a href="#C-多分类" class="headerlink" title="C. 多分类"></a>C. 多分类</h3><p>逻辑回归应用于$n$类多分类时，可以针对每一个类别分别训练$n$个逻辑回归模型$\{h_{\theta1},h_{\theta2},…h_{\theta n}\}$，在模型$h _{\theta i}$中，将属于类别$i$的数据视为正类，其余类别设为负类（1 vs others），预测时对样本$i$，计算所有模型的分数，取分数最大的模型的正样本类别作为该样本$i$的类别。</p><h3 id="D-优缺点"><a href="#D-优缺点" class="headerlink" title="D. 优缺点"></a>D. 优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>形式简单，模型可解释性强（从权重可以看到各个特征的影响）</li><li>效果较好，尤其是在特征选取适当的情况下，工程上经常作为baseline模型</li><li>训练速度快，计算量仅与特征量和数据量有关，占用资源小</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>应对数据不均衡情况的能力不强</li><li>不能解决非线性的问题，因为LR的决策面是线性的</li><li>数据特征缺失或特征空间很大时效果不好</li><li>包含高度线性相关特征时，不适合使用逻辑回归，会影响特征的解释性</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;A-模型推导&quot;&gt;&lt;a href=&quot;#A-模型推导&quot; class=&quot;headerlink&quot; title=&quot;A. 模型推导&quot;&gt;&lt;/a&gt;A. 模型推导&lt;/h3&gt;&lt;p&gt;现有样本集合$\{X,y\}$，$X$为特征$\{x_1,x_2,…,x_n\}$，$y$为实数，线性回归中，$X$与$y$之间存在映射关系$y=h_\theta(x)$，其中，&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_\theta(x)=\sum \theta_ ix_i&lt;/script&gt;&lt;p&gt;在逻辑回归中，$y$属于集合$\{0,1\}$，$1$表示属于某类别，而$0$表示不属于该类别。&lt;/p&gt;
&lt;p&gt;期望得到模型$M$，使得$M(x)\in[0,1]$，这样能够较为方便地衡量$x$属于该类别的概率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://shihanmax.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="LR" scheme="https://shihanmax.github.io/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>使用Tensotflow同时加载多个模型</title>
    <link href="https://shihanmax.github.io/2019/03/18/Tensorflow%E5%90%8C%E6%97%B6%E5%8A%A0%E8%BD%BD%E5%A4%9A%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://shihanmax.github.io/2019/03/18/Tensorflow%E5%90%8C%E6%97%B6%E5%8A%A0%E8%BD%BD%E5%A4%9A%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2019-03-18T09:38:00.000Z</published>
    <updated>2020-03-18T05:08:54.930Z</updated>
    
    <content type="html"><![CDATA[<p>使用单个模型时，一种模型的保存和加载的方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入/输出定义</span></span><br><span class="line">x = tf.placeholder(dtype, name)</span><br><span class="line">y = tf.placeholder(dtype, name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重定义</span></span><br><span class="line">weight = tf.Variable(shape, dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># op定义</span></span><br><span class="line">output = some_operation(x, weight)</span><br><span class="line">loss = tf.calc_loss(output, y)</span><br><span class="line">train_op = optimizer.minimize(loss, name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># do some train 训练</span></span><br><span class="line">    </span><br><span class="line">    saver.save(sess, <span class="string">"./model/model_path"</span>)  <span class="comment"># 保存模型</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>针对上述模型，恢复的方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    sess = tf.Session():</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    saver.restore(sess, <span class="string">"./model/model_path"</span>)  <span class="comment"># 将模型恢复到sess中</span></span><br><span class="line">        </span><br><span class="line">    output = sess.run([output], feed_dict=feed_dict)  <span class="comment"># 使用恢复的模型进行预测</span></span><br></pre></td></tr></table></figure><p>对单个模型来说，这么做没有问题，但如果我们训练了多个相同结构的模型，我们期待以如下方式恢复它们：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">all_sessions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(model_nums):</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">sess = tf.Session():</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">saver.restore(sess, <span class="string">"./model/model_path"</span>)  <span class="comment"># 将模型恢复到sess中</span></span><br><span class="line"></span><br><span class="line">all_sessions.append(sess)</span><br></pre></td></tr></table></figure><br>使用上述恢复的session进行预测：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_result = []</span><br><span class="line"><span class="keyword">for</span> sess <span class="keyword">in</span> all_sessions:</span><br><span class="line">all_result.append(sess.run([output], feed_dict=feed_dict))</span><br></pre></td></tr></table></figure><br>但这么做会导致参数错误，预测结果异常，原因是多个模型中的变量会发生冲突，原因是将所有的模型变量都加载到同一个线程的默认图中，解决方法是，针对不同的model使用不同的默认图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImportGraph</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, loc)</span>:</span></span><br><span class="line">        self.graph = tf.Graph()</span><br><span class="line">        self.sess = tf.Session(graph=self.graph)</span><br><span class="line">        <span class="keyword">with</span> self.graph.as_default():</span><br><span class="line">            saver = tf.train.import_meta_graph(<span class="string">"./model/model_path"</span> + <span class="string">'.meta'</span>)</span><br><span class="line">            saver.restore(self.sess, <span class="string">"./model/model_path"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, data)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.sess.run([output], feed_dict=feed_dict)</span><br></pre></td></tr></table></figure><br>上述方式是从<a href="https://blog.csdn.net/lc013/article/details/84202901" target="_blank" rel="noopener">博客</a>看到的，在我的实验中，并有有成功地将多个模型恢复，我的恢复方式是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImportGraph</span><span class="params">()</span>:</span></span><br><span class="line">tf.reset_default_graph()  <span class="comment"># The default graph is a property of the current thread. 重置当前线程中的默认图</span></span><br><span class="line">self.sess = tf.Session()</span><br><span class="line">    self.sess.run(tf.global_variables_initializer())</span><br><span class="line">    self.saver = tf.train.Saver()</span><br><span class="line">    self.saver.restore(self.sess, <span class="string">"./model/model_path"</span>)</span><br></pre></td></tr></table></figure></p><p>重要的地方在于tf.reset_default_graph()，tf官方文档给出的解释是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line">Defined in tensorflow&#x2F;python&#x2F;framework&#x2F;ops.py.</span><br><span class="line"></span><br><span class="line">Clears the default graph stack and resets the global default graph.</span><br><span class="line"></span><br><span class="line">NOTE: The default graph is a property of the current thread. This function applies only to the current thread. Calling this function while a tf.Session or tf.InteractiveSession is active will result in undefined behavior. Using any previously created tf.Operation or tf.Tensor objects after calling this function will result in undefined behavior.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用单个模型时，一种模型的保存和加载的方式如下：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输入/输出定义&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = tf.placeholder(dtype, name)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y = tf.placeholder(dtype, name)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 权重定义&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;weight = tf.Variable(shape, dtype)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# op定义&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output = some_operation(x, weight)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;loss = tf.calc_loss(output, y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;train_op = optimizer.minimize(loss, name)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;with&lt;/span&gt; tf.Session() &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; sess:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    sess.run(tf.global_variables_initializer())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    saver = tf.train.Saver()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# do some train 训练&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    saver.save(sess, &lt;span class=&quot;string&quot;&gt;&quot;./model/model_path&quot;&lt;/span&gt;)  &lt;span class=&quot;comment&quot;&gt;# 保存模型&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="https://shihanmax.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Tensorflow" scheme="https://shihanmax.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>RNN Training Tips and Tricks</title>
    <link href="https://shihanmax.github.io/2019/01/25/RNN-Training-Tips-and-Tricks/"/>
    <id>https://shihanmax.github.io/2019/01/25/RNN-Training-Tips-and-Tricks/</id>
    <published>2019-01-24T16:00:00.000Z</published>
    <updated>2019-08-17T07:38:58.860Z</updated>
    
    <content type="html"><![CDATA[<p>本文是一些在训练RNN模型的技巧和建议，来自<a href="https://github.com/karpathy/char-rnn#tips-and-tricks" target="_blank" rel="noopener">Andrej Karpathy</a>.</p><h2 id="监视训练集和验证集损失"><a href="#监视训练集和验证集损失" class="headerlink" title="监视训练集和验证集损失"></a>监视训练集和验证集损失</h2><p>机器学习、神经网络方面的新手可能需要掌握更多的技巧才能训练出一个好的模型。首要关注的是训练集损失（训练时输出）和验证集损失（指定迭代次数后将模型在验证集上跑一遍，得到验证集损失），特别地：</p><ul><li>如果训练集损失比验证集损失低很多，这意味着我们的模型可能过拟合了，解决办法是，减小网络的规模，或者增大dropout率，比如调整到0.5</li><li>如果训练集/测试集误差处在同一个水平上，模型很有可能欠拟合，此时需要增大网络规模（网络层数或者每层的神经元个数）</li></ul><a id="more"></a><h2 id="参数预估"><a href="#参数预估" class="headerlink" title="参数预估"></a>参数预估</h2><p>LSTM模型的两个重要参数是lstm_size和num_layers，我建议将num_layers设置为2或者3，lstm_size可以基于训练数据量来调整，你需要关注两个数：</p><ul><li>模型一共包含多少参数</li><li>数据规模，1MB的数据大约包含一百万个字符</li></ul><p>这两者应该有着相同的数量集，可能不太好描述，下面是几个例子：</p><ul><li>如果我的数据集是100MB的，我使用默认的参数设置（包含150k个参数），由于我的数据集数量远大于参数数量（100百万&gt;&gt;0.15百万），模型很有可能欠拟合，这个时候，可以放心地将lstm_size调大一些</li><li>如果我有10MB的数据，而模型拥有一千万个参数，这时候我会密切关注验证集损失，如果它比训练集损失大，我会将dropout调大一些，这样有助于降低验证集损失</li></ul><h2 id="最好的模型策略"><a href="#最好的模型策略" class="headerlink" title="最好的模型策略"></a>最好的模型策略</h2><p>寻找最优模型参数的“制胜法宝“是，在我们可以忍受的训练时间下，尽可能在更大的网络上试错，并且在(0, 1)之间调整dropout，在验证集上拥有最好的性能的模型，就是我们最终想要的。</p><p>通过调整不同的超参数，运行多个模型在深度学习领域十分普遍，最终我们要选择在验证集上表现最好的那个。</p><p>另外，训练集/测试集分割比率也是个超参数，要保证验证集包含足够的数据，否则，验证集性能可能会因为有噪声而不能反映出模型真实的信息。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文是一些在训练RNN模型的技巧和建议，来自&lt;a href=&quot;https://github.com/karpathy/char-rnn#tips-and-tricks&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Andrej Karpathy&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;监视训练集和验证集损失&quot;&gt;&lt;a href=&quot;#监视训练集和验证集损失&quot; class=&quot;headerlink&quot; title=&quot;监视训练集和验证集损失&quot;&gt;&lt;/a&gt;监视训练集和验证集损失&lt;/h2&gt;&lt;p&gt;机器学习、神经网络方面的新手可能需要掌握更多的技巧才能训练出一个好的模型。首要关注的是训练集损失（训练时输出）和验证集损失（指定迭代次数后将模型在验证集上跑一遍，得到验证集损失），特别地：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果训练集损失比验证集损失低很多，这意味着我们的模型可能过拟合了，解决办法是，减小网络的规模，或者增大dropout率，比如调整到0.5&lt;/li&gt;
&lt;li&gt;如果训练集/测试集误差处在同一个水平上，模型很有可能欠拟合，此时需要增大网络规模（网络层数或者每层的神经元个数）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://shihanmax.github.io/categories/NLP/"/>
    
    
      <category term="RNN" scheme="https://shihanmax.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>CRF,HMM,MEMM的区别和联系</title>
    <link href="https://shihanmax.github.io/2018/12/22/CRF,HMM,MEMM%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB/"/>
    <id>https://shihanmax.github.io/2018/12/22/CRF,HMM,MEMM%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB/</id>
    <published>2018-12-21T16:00:00.000Z</published>
    <updated>2019-08-17T16:31:16.060Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生成式模型和判别式模型"><a href="#生成式模型和判别式模型" class="headerlink" title="生成式模型和判别式模型"></a>生成式模型和判别式模型</h2><p>已知输入$x$和标签$y$：</p><ul><li>判别式模型估计条件概率分布$p(y\mid x)$，常见的算法有：LR，SVM，神经网络，KNN，CRF，LDA，线性回归</li><li>生成式模型估计联合概率$p(x,y)$，常见的的有Naive Bayes，HMM</li></ul><h2 id="CRF、HMM、MEMM的对比"><a href="#CRF、HMM、MEMM的对比" class="headerlink" title="CRF、HMM、MEMM的对比"></a>CRF、HMM、MEMM的对比</h2><p>CRF、HMM、MEMM（最大熵隐马模型）是序列标注任务中常用的三种模型，他们各有优缺点，以下从不同的角度对比这三者。</p><a id="more"></a><h1 id="Finding-multiple-core-periphery-pairs-in-networks"><a href="#Finding-multiple-core-periphery-pairs-in-networks" class="headerlink" title="Finding multiple core-periphery pairs in networks"></a>Finding multiple core-periphery pairs in networks</h1><h3 id="1-模型种类"><a href="#1-模型种类" class="headerlink" title="1.模型种类"></a>1.模型种类</h3><ul><li>HMM：对转移概率和状态概率进行建模，是生成式模型</li><li>CRF：在有限样本下建立判别函数（预测函数），是判别式模型</li><li>MEMM：是一种基于状态分类的有限状态模型，是判别式模型</li></ul><h3 id="2-拓扑结构"><a href="#2-拓扑结构" class="headerlink" title="2.拓扑结构"></a>2.拓扑结构</h3><p>HMM和MEMM是一种有向图，CRF是无向图</p><h3 id="3-全局最优-or-局部最优"><a href="#3-全局最优-or-局部最优" class="headerlink" title="3.全局最优 or 局部最优"></a>3.全局最优 or 局部最优</h3><p>全局最优与否要看我们构造出的loss function是否是convex，如是，则存在全局最优解。</p><ul><li>HMM对转移概率和状态概率直接建模，是一种全局最优模型</li><li>MEMM是对转移概率和状态概率建立联合概率，统计是使用条件概率，由于其只在局部做归一化，容易陷入局部最优</li><li>CRF是全局范围内统计归一化概率，可以得到全局最优解，解决了MEMM中标注偏置的问题</li></ul><h3 id="4-比较"><a href="#4-比较" class="headerlink" title="4.比较"></a>4.比较</h3><ul><li>与HMM比较，CRF不需要严格的独立性假设条件，可以容纳任意范围内的上下文信息，特征设计灵活</li><li>与MEMM比较，由于CRF计算全局最优输出节点的条件概率，克服了MEMM模型标记偏置的缺点</li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="HMM和RNN的比较"><a href="#HMM和RNN的比较" class="headerlink" title="HMM和RNN的比较"></a>HMM和RNN的比较</h3><p>HMM和RNN结构相似，都是通过hidden state来刻画序列建的依赖关系，但二者有较大不同，体现在：</p><ul><li>HMM本质上是一个概率模型，而RNN不是，RNN没有马尔科夫性假设，可以考虑很长的历史信息</li><li>隐状态的表示不同：HMM是one hot表示，而RNN则是distributed representation，其表示能力较HMM强很多，在面对高维时，表示效率更高（类似于one hot 和 word2vec）</li><li>隐状态的转移方式：HMM是线性的，而RNN则是高度非线性</li><li>深度不同：RNN的堆叠可以使得其表达能力指数提高</li></ul><p><a href="https://www.zhihu.com/question/53458773/answer/330396666" target="_blank" rel="noopener">1. 概率图模型笔记</a>（详细）<br><a href="https://blog.csdn.net/xingchenhy/article/details/72847543" target="_blank" rel="noopener">2. CRF，HMM和MEHMM区别</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;生成式模型和判别式模型&quot;&gt;&lt;a href=&quot;#生成式模型和判别式模型&quot; class=&quot;headerlink&quot; title=&quot;生成式模型和判别式模型&quot;&gt;&lt;/a&gt;生成式模型和判别式模型&lt;/h2&gt;&lt;p&gt;已知输入$x$和标签$y$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;判别式模型估计条件概率分布$p(y\mid x)$，常见的算法有：LR，SVM，神经网络，KNN，CRF，LDA，线性回归&lt;/li&gt;
&lt;li&gt;生成式模型估计联合概率$p(x,y)$，常见的的有Naive Bayes，HMM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;CRF、HMM、MEMM的对比&quot;&gt;&lt;a href=&quot;#CRF、HMM、MEMM的对比&quot; class=&quot;headerlink&quot; title=&quot;CRF、HMM、MEMM的对比&quot;&gt;&lt;/a&gt;CRF、HMM、MEMM的对比&lt;/h2&gt;&lt;p&gt;CRF、HMM、MEMM（最大熵隐马模型）是序列标注任务中常用的三种模型，他们各有优缺点，以下从不同的角度对比这三者。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://shihanmax.github.io/categories/Machine-Learning/"/>
    
    
      <category term="概率图模型" scheme="https://shihanmax.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="PGM" scheme="https://shihanmax.github.io/tags/PGM/"/>
    
  </entry>
  
  <entry>
    <title>一种检测CP结构和社区结构的联合算法</title>
    <link href="https://shihanmax.github.io/2018/12/20/%E4%B8%80%E7%A7%8D%E6%A3%80%E6%B5%8BCP%E7%BB%93%E6%9E%84%E5%92%8C%E7%A4%BE%E5%8C%BA%E7%BB%93%E6%9E%84%E7%9A%84%E8%81%94%E5%90%88%E7%AE%97%E6%B3%95/"/>
    <id>https://shihanmax.github.io/2018/12/20/%E4%B8%80%E7%A7%8D%E6%A3%80%E6%B5%8BCP%E7%BB%93%E6%9E%84%E5%92%8C%E7%A4%BE%E5%8C%BA%E7%BB%93%E6%9E%84%E7%9A%84%E8%81%94%E5%90%88%E7%AE%97%E6%B3%95/</id>
    <published>2018-12-19T16:00:00.000Z</published>
    <updated>2019-08-17T07:27:29.810Z</updated>
    
    <content type="html"><![CDATA[<p>Bing-Bing Xiang et.al</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>C-P结构和社区结构是复杂网络中的两种典型的中尺度结构。社区发现已经被广泛研究，而C-P结构的定义和检测相关的研究却不多。并且，目前社区结构的检测和C-P结构的检测是分开进行的。本文中，我们提出了一种同时检测复杂网络中的C-P结构和社区结构的联合框架（在两种结构之间搭建了一个桥梁）。我们的算法具有以下优点：可以检测多C-P结构；可以检测社区之间的重叠节点；通过调整参数，可以检测大小不同的核心（core）。我们在人工网络和真实网络上验证了我们算法的优良效果。总结一下：我们提出了一种同时检测复杂网络中的C-P结构和社区结构的联合框架。</p><a id="more"></a><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>真实世界中，通讯、社会、运输、生物等等领域都可以描述为一个网络，网络的中尺度结构对于了解网络特性和动态（dynamics）非常重要。一种典型的中尺度结构——社区已经被广泛研究：同一个社区中的结点间紧密相连，而社区间则链接稀疏。社区发现有多种算法，如基于模块化的<sup><a href="#fn_6" id="reffn_6">6</a></sup>，基于随机游走的<sup><a href="#fn_7" id="reffn_7">7</a></sup>，基于谱聚类的<sup><a href="#fn_8" id="reffn_8">8</a></sup>，基于层级聚类的<sup><a href="#fn_9,10" id="reffn_9,10">9,10</a></sup>，基于非负矩阵分解方法的，等等。也有很多用来研究社区重叠（overlapping community detection）的算法，如标签传播<sup><a href="#fn_12,13" id="reffn_12,13">12,13</a></sup>，连接分块（link partition）<sup><a href="#fn_14,15" id="reffn_14,15">14,15</a></sup>，团渗透理论<sup><a href="#fn_16,17" id="reffn_16,17">16,17</a></sup>，多目标演化算法<sup><a href="#fn_18" id="reffn_18">18</a></sup>等。</p><p>另一种中尺度结构是C-P结构，它没有受到太多的关注和研究，但研究表明，在社会网络<sup><a href="#fn_19" id="reffn_19">19</a></sup>，科学引文网络<sup><a href="#fn_20" id="reffn_20">20</a></sup>，国际贸易<sup><a href="#fn_21,22" id="reffn_21,22">21,22</a></sup>，以及其他领域<sup><a href="#fn_23-25" id="reffn_23-25">23-25</a></sup>都包含C-P结构。</p><p>虽然这种结构都或多或少受到了研究，但它们之间的关系却不明确，事实上，从直觉来看，它们之间是有一定联系的：一方面，具有单C-P结构的网络一般都不包含社区结构；另一方面，具有多个C-P结构的网络一般暗示着存在社区结构。然而，存在社区则不一定存在C-P结构。因此，正确理解两种结构之间的关系有助于我们设计有效地方法来同时识别二者。</p><p>目前较为流行和正式的对C-P结构的定义来自B和E<sup><a href="#fn_27" id="reffn_27">27</a></sup>，在定义中，一个节点如果既和其他核心节点相连，又和边缘节点相连，则该节点也属于核心；而边缘节点之间则没有连接。由于理想的C-P结构太严格了，一个稍微宽松一些的定义是：核心节点大部分紧密相连，而边缘节点则呈树状（frays into a tree，有待商榷），当然，这只是一个描述性的定义，并没有很严谨的数学定义。目前已有的C-P结构检测方法主要是通过检测一个网络究竟和理想C-P结构有多大相似度来进行的<sup><a href="#fn_7,28,31" id="reffn_7,28,31">7,28,31</a></sup>，或者是将节点按照一定的中心性（coreness）来排序。最近，Zhang等人首次提出了通过最大似然方法来拟合随机块矩阵来检测C-P结构的方法。通过对网络数据的拟合，模型可以给出最佳的关于C-P结构的划分。这个方法不需要额外参数，能够适用于大型网络和弱C-P结构网络。然而，这些方法往往会有以下缺点：事先需要给定核的大小；将网络看做单C-P结构来处理（实际情况表明许多网络含有不只一个C-P结构）；然而，在许多网络中可能存在着社区结构和读个C-P结构，这些算法并不能提供一个同时检测这两种结构的框架。</p><p>受上述原因启发，这篇文章的目标是提出一个可同时检测C-P结构和社区结构的联合框架，为了达到这个目标，我们的实现方案是：</p><ol><li>基于<strong>连接密度</strong>（connection density indictor）对所有节点重新排序</li><li>通过我们定义的<strong>区域密度曲线</strong>判断网络是否显示含有单C-P、多C-P或社区结构</li></ol><p>多说一句，多尺度的C-P结构以及节点重叠的情况，也能检测。</p><h2 id="II-THE-PROPOSED-ALGORITHM"><a href="#II-THE-PROPOSED-ALGORITHM" class="headerlink" title="II. THE PROPOSED ALGORITHM"></a>II. THE PROPOSED ALGORITHM</h2><p>这一节里，我们会介绍一下我们的算法，考虑一个无向无权图（我们的算法会在附录中介绍），算法主要分为三个步骤：</p><ol><li>将所有节点重新排序</li><li>计算每个节点的区域密度</li><li>通过区域密度曲线来检测这些中尺度结构</li></ol><h3 id="A-Re-rank-nodes-in-a-new-sequence-对节点重新排序"><a href="#A-Re-rank-nodes-in-a-new-sequence-对节点重新排序" class="headerlink" title="A. Re-rank nodes in a new sequence 对节点重新排序"></a>A. Re-rank nodes in a new sequence 对节点重新排序</h3><p>N考虑一个图$G(V,E)$，$V$是节点集合，$E$是边的集合。已知核心之间的连接很紧密，所以我们希望把所有的节点重新排序，使得具有公共连接的结点互相之间更接近（such that nodes with more common connections approch each other in the new seqence）。为了达到这个目的，我们定义了集合$U$和集合$V’$，$V’=V-U$（这两个集合都是节点集合，原图中的结点如果不在$U$中，就在$V’$中，$V’$是$U$的补集）。初始状态，$U$为空集，$V’=V$为节点集合，为了启动排序过程，我们需要选择一个初始的节点，我们可以选择中心性强的那个节点作为初始节点（因为这个节点更有可能是核心节点），这里我们选择closeness最大的那个节点（我们后来发现，最初这个节点按照度最大、betweeness最大等等来选择，结果都不会有很大影响），并将它重新编号为$u1$，这时$U=\left \{u1\right\}$，现在我们需要从$V’$里再选择一个节点加入到$U$中，而且要保证，新加入的这个节点一定要和$U$中的结点的连边数最大，如果这样的节点不止一个，那我们选择度最大的那个，放到$U$集合中。（如果两个节点和集合$U$中的连边数相同，度也相同呢？这种情况下，随机选择一个）。持续上述过程，直到集合$V’$为空，我们就得到了一个新的节点序列$U=\left\{u_1,u_2,…,u_N\right\}$</p><h3 id="B-Plot-region-density-curve-绘制区域密度曲线"><a href="#B-Plot-region-density-curve-绘制区域密度曲线" class="headerlink" title="B. Plot region density curve 绘制区域密度曲线"></a>B. Plot region density curve 绘制区域密度曲线</h3><p>现在，我们提出一个局部指示器——连接密度$CD$（connection density）来代表子图$S$中的连接密度情况：</p><script type="math/tex; mode=display">CD(S)=\cfrac{2m'}{n'(n'-1)}</script><p>其中，$m’$表示$S$子图中的连边数；$n’$表示子图节点个数。</p><p>这里定义一个参数$\alpha$用来衡量核所包含的最少的节点数。对给定$\alpha$，一个节点$u_i$的区域密度定义为：</p><script type="math/tex; mode=display">RD(U_i)=\left\{ \begin{aligned} &CD (\left\{u_1,...,u_i\right\})\ \ \ \ \   i\le\alpha;\\ &CD(\left\{u_{i-\alpha+1,...,u_i}\right\})\ \ \ \ i>\alpha. \end{aligned} \right.</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bing-Bing Xiang et.al&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;C-P结构和社区结构是复杂网络中的两种典型的中尺度结构。社区发现已经被广泛研究，而C-P结构的定义和检测相关的研究却不多。并且，目前社区结构的检测和C-P结构的检测是分开进行的。本文中，我们提出了一种同时检测复杂网络中的C-P结构和社区结构的联合框架（在两种结构之间搭建了一个桥梁）。我们的算法具有以下优点：可以检测多C-P结构；可以检测社区之间的重叠节点；通过调整参数，可以检测大小不同的核心（core）。我们在人工网络和真实网络上验证了我们算法的优良效果。总结一下：我们提出了一种同时检测复杂网络中的C-P结构和社区结构的联合框架。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://shihanmax.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="CP" scheme="https://shihanmax.github.io/tags/CP/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="https://shihanmax.github.io/2018/12/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>https://shihanmax.github.io/2018/12/19/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</id>
    <published>2018-12-18T16:00:00.000Z</published>
    <updated>2020-03-19T13:38:23.880Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、优缺点"><a href="#一、优缺点" class="headerlink" title="一、优缺点"></a>一、优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul><li><p>过程简单，速度快</p></li><li><p>适用于多类分类，且不会造成复杂度大幅上升</p></li><li><p>在样本分布独立的假设下，效果很好</p></li><li><p>与逻辑回归相比，需要更少的样本</p></li><li><p>对类别型的特征变量和符合正态分布的数值型变量效果好</p>  <a id="more"></a></li></ul><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul><li>对训练集中没有出现的变量，概率为0（需要应用平滑技术）</li><li>计算出的概率不太具有解释性</li><li>现实情况中，样本并不总是能够满足独立这一前提假设</li></ul><h2 id="二、常见应用场景"><a href="#二、常见应用场景" class="headerlink" title="二、常见应用场景"></a>二、常见应用场景</h2><ul><li>文本分类/垃圾文本过滤/情感识别</li><li>多分类实时预测</li><li>推荐系统（协同过滤是强相关性，泛化能力弱，朴素贝叶斯与之结合，能够增强推荐的覆盖度和效果）</li></ul><h2 id="三、应用注意点"><a href="#三、应用注意点" class="headerlink" title="三、应用注意点"></a>三、应用注意点</h2><ul><li>连续数值型特征，要变换成满足正态分布的形式</li><li>对零频项，需要做平滑</li><li>相关特征需要去除（高相关的特征，相当于double同一个特征的效果）</li><li>可调参数较少，需要关注数据预处理和特征选择</li><li>不适用bagging、boosting等增强方法，因为这些方法是用来减少过拟合（减少方差）的，但朴素贝叶斯没有方差可以减少。</li></ul><h2 id="四、在垃圾邮件识别中的应用"><a href="#四、在垃圾邮件识别中的应用" class="headerlink" title="四、在垃圾邮件识别中的应用"></a>四、在垃圾邮件识别中的应用</h2><h3 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h3><script type="math/tex; mode=display">P(Y \mid X)=\cfrac{P(X\mid Y)P(Y)}{P(X)}</script><p>以垃圾邮件识别为例（以H表示事件“是垃圾邮件”，N表示事件“不是垃圾邮件”）：</p><script type="math/tex; mode=display">P(H\mid mail)=\cfrac{P(mail\mid H)P(H)}{P(mail)}</script><p>其中：</p><ul><li>$P(H)$称先验概率</li><li>$P(H\mid mail)$称后验概率</li></ul><p>假设某一封邮件的内容是“我司可办理正规发票”，该邮件是垃圾邮件的概率是：</p><script type="math/tex; mode=display">P(H\mid 我司可办理正规发票)=\cfrac{P(我司可办理正规发票\mid H)P(H)}{P(我司可办理正规发票)}</script><p>由于邮件中包含的句子多种多样，不太可能将所有句子在正常邮件和垃圾邮件中出现的次数，考虑将句子特征转化为词的特征，上式变为：</p><script type="math/tex; mode=display">P(H\mid 我\;司\;可\;办理\;正规\;发票)=\cfrac{P(我\;司\;可\;办理\;正规\;发票\mid H)P(H)}{P(我\;司\;可\;办理\;正规\;发票)}  \tag{1}</script><h4 id="独立性假设的引入"><a href="#独立性假设的引入" class="headerlink" title="独立性假设的引入"></a>独立性假设的引入</h4><p>针对$P(我\;司\;可\;办理\;正规\;发票)$，引入“朴素”的<strong>独立性</strong>假设：</p><script type="math/tex; mode=display">P(我\;司\;可\;办理\;正规\;发票\mid H) \approx P(我\mid H)*P(司\mid H)*P(可\mid H)*P(办理\mid H)*P(正规\mid H)*P(发票\mid H)</script><p>上式中的分量$P(我\mid H)$表示，垃圾邮件中，“我”出现的概率。</p><script type="math/tex; mode=display">P(我\mid H)=\cfrac{P(我,H)}{P(H)} \approx \cfrac{count(我,H)}{count(H)} \tag{2}</script><p>令$C = P(H\mid 我\;司\;可\;办理\;正规\;发票)$，$\overline C = P(N\mid 我\;司\;可\;办理\;正规\;发票)$</p><p>判断该邮件是否是垃圾邮件，只需要比较$(1)$式中的分子即可：</p><p>即比较：</p><script type="math/tex; mode=display">P(我\mid H)*P(司\mid H)*P(可\mid H)*P(办理\mid H)*P(正规\mid H)*P(发票\mid H)*P(H) \tag{3}</script><p>与</p><script type="math/tex; mode=display">P(我\mid N)*P(司\mid N)*P(可\mid N)*P(办理\mid N)*P(正规\mid N)*P(发票\mid N)*P(N) \tag{4}</script><p>的大小，$(3)、(4)$式中的各项，可以通过对语料的统计，通过式$(2)$得到。</p><h2 id="五、其他问题"><a href="#五、其他问题" class="headerlink" title="五、其他问题"></a>五、其他问题</h2><h3 id="5-1-重复词语的三种方式"><a href="#5-1-重复词语的三种方式" class="headerlink" title="5.1 重复词语的三种方式"></a>5.1 重复词语的三种方式</h3><ol><li>多项式模型，重复项的概率为重复项出现次数的指数次方</li><li>伯努利模型，认为任何项仅出现1次</li><li>混合模型，计算句子概率时，不考虑词语重复出现；而在计算词语出现的概率$P(词语 \mid H)$时，考虑词语重复</li></ol><h3 id="5-2-停用词和关键词"><a href="#5-2-停用词和关键词" class="headerlink" title="5.2 停用词和关键词"></a>5.2 停用词和关键词</h3><p>去掉语料中的停用词，可以减少模型训练和判断分类的时间，可以通过对照停用词表实现；而关键词相对普通的词，在训练中往往具有更大的权重，如上述例子中的“发票”。关键词需要人工经验进行指定。</p><h3 id="5-3-平滑技术"><a href="#5-3-平滑技术" class="headerlink" title="5.3 平滑技术"></a>5.3 平滑技术</h3><p>在上述例子中，如果“发票”一词以前没有出现过，则计算概率时，会导致整个分子的值变为0（这种情况很常见）。</p><script type="math/tex; mode=display">P(word \mid H)=\cfrac{count(word)+1}{\sum (count(word)+k)}</script><p>其中，$k$是邮件类别数，这里取2。</p><h2 id="六、实验"><a href="#六、实验" class="headerlink" title="六、实验"></a>六、实验</h2><p>待补充</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/han_xiaoyang/article/details/50629608" target="_blank" rel="noopener">1. 朴素贝叶斯实战与进阶</a></p><p><a href="https://blog.csdn.net/suibianshen2012/article/details/51613759" target="_blank" rel="noopener">2. 用朴素贝叶斯进行文本分类</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、优缺点&quot;&gt;&lt;a href=&quot;#一、优缺点&quot; class=&quot;headerlink&quot; title=&quot;一、优缺点&quot;&gt;&lt;/a&gt;一、优缺点&lt;/h2&gt;&lt;h3 id=&quot;优点&quot;&gt;&lt;a href=&quot;#优点&quot; class=&quot;headerlink&quot; title=&quot;优点&quot;&gt;&lt;/a&gt;优点&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;过程简单，速度快&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;适用于多类分类，且不会造成复杂度大幅上升&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在样本分布独立的假设下，效果很好&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;与逻辑回归相比，需要更少的样本&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对类别型的特征变量和符合正态分布的数值型变量效果好&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Machine learning" scheme="https://shihanmax.github.io/categories/Machine-learning/"/>
    
    
      <category term="NB" scheme="https://shihanmax.github.io/tags/NB/"/>
    
  </entry>
  
  <entry>
    <title>网络中的多C-P结构检测</title>
    <link href="https://shihanmax.github.io/2018/12/13/Finding%20multiple%20core-periphery%20pairs%20in%20networks/"/>
    <id>https://shihanmax.github.io/2018/12/13/Finding%20multiple%20core-periphery%20pairs%20in%20networks/</id>
    <published>2018-12-12T16:00:00.000Z</published>
    <updated>2019-08-17T16:34:21.410Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Finding-multiple-core-periphery-pairs-in-networks"><a href="#Finding-multiple-core-periphery-pairs-in-networks" class="headerlink" title="Finding multiple core-periphery pairs in networks"></a>Finding multiple core-periphery pairs in networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在一个包含C-P结构的网络中，core节点之间紧密连接，periphery节点与core节点紧密相连，而periphery节点之间稀疏连接。目前单核识别已有实现方法，在实际网络中，就像“社区”由多个密集团组成一样，网络也应该由多个C-P结构组成。本文提出了一个可扩展的、可以检测多核心结构（不重合）的方法，我们借助人工网络和真实网络阐述我们的算法，例如，在政治论坛网络中的政治倾向以及世界航线网络中发现了明显的C-P结构。</p><a id="more"></a><h2 id="I-Introduction"><a href="#I-Introduction" class="headerlink" title="I. Introduction"></a>I. Introduction</h2><p>许多复杂系统都可以看做由node和edge组成的图（network），edge表示node之间的关系。网络有微尺度、中尺度和宏观尺度的结构特征，如度，聚类系数，直径等，在中尺度结构中，社区结构（community structure）比较具有代表性，社区是指一组节点：该组组内节点间紧密连接，而与其他组节点稀疏连接。一个社区中的节点通常会共享一些特征（share a role）。社区发现有助于节点分类和网络可视化。</p><p>C-P结构是网络的另一个中尺度结构，在这种结构的视角下，我们认为网络是由两组分别称为core和periphery的节点集合构成。尽管定义多种多样，core一般是一组互相紧密连接的节点集合，periphery一般是一组与core节点紧密连接，但组内稀疏链接的节点集合。core和社区都由紧密连接的节点构成，但它们之间有区别：core和它的periphery紧密连接，而社区与社区外的节点则不是。在许多网络中都发现了C-P结构<sup><a href="#fn_11、12、13、15、16、17、18、22、24、25、26、27、28" id="reffn_11、12、13、15、16、17、18、22、24、25、26、27、28">11、12、13、15、16、17、18、22、24、25、26、27、28</a></sup>。例如，在研究者网络中，顶尖研究者往往与顶尖研究者一起发表论文，他们形成一个core，而其他研究者往往会与某个顶尖研究者合作发表论文，这些研究者形成一个periphery。</p><p>Borgatti与Everett（以下简称BE）首先定量地提出C-P结构，在离散形式（discrete）C-P模型中，他们提出一种理想化的C-P结构，在这种结构中，core节点与所有节点相连，periphery节点与所有core节点相连，而与periphery节点无连接。虽然“C-P之间的连接程度（connectivity）仅比C-C之间更稀疏”这一说法更加现实，但在本文中，我们仅关注理想化的C-P结构。BE期望将网络中所有节点分配给C或P（assignment<br>of all nodes in a given network to a core or periphery ），使得网络与理想化的C-P模型最为接近。在BE提出C-P结构的定义之后，许多研究者提出了C-P结构检测的算法<sup><a href="#fn_11-13,15-18,20-22,25,27" id="reffn_11-13,15-18,20-22,25,27">11-13,15-18,20-22,25,27</a></sup>这些算法主要关注识别网络中的单C-P结构 。然而，将网络看做由多个C-P结构组成，或许更加合适<sup><a href="#fn_11,14,16,19-21" id="reffn_11,14,16,19-21">11,14,16,19-21</a></sup>，这也是当前研究的重点。例如，论文合作网络（co-authorship）可能就由多个研究成员小组（researcher group）组成，研究者往往倾向于与组内的顶尖研究者合作，而不与组内其他研究者合作。这会导致在组内形成C-P结构<sup><a href="#fn_16" id="reffn_16">16</a></sup>，前述研究未能提供一个可扩展的多核检测方法。也有研究关注于相似但类型不同的多C-P结构的检测<sup><a href="#fn_30" id="reffn_30">30</a></sup>。其他算法仅仅关注多核心的检测但并不假定“边缘节点间稀疏连接”<sup><a href="#fn_17,31,32" id="reffn_17,31,32">17,31,32</a></sup>。网络可以以k-score<sup><a href="#fn_33" id="reffn_33">33</a></sup>、k-trusses<sup><a href="#fn_34" id="reffn_34">34</a></sup>或密集子图（dense subgraphs）<sup><a href="#fn_35,36" id="reffn_35,36">35,36</a></sup>的形式拥有多个互斥的core，但是，相关的算法不能指出，边缘节点之间的联系的紧密程度或者，边缘节点属于哪一个核心（to which core a peripheral node belongs）。这个能够检测网络中多个中尺度结构（包括C-P pair）的算法<sup><a href="#fn_19" id="reffn_19">19</a></sup>的计算复杂度很高，而且只适用于小型网络（参见Appendix A）。</p><p>我们提出了一个灵活的检测网络中多个无重叠C-P结构的算法，在这些C-P结构中，每个都尽量与理想C-P结构接近，而且还可以自动确定C-P结构的数量和尺寸。许多检测C-P结构的算法都被归结为density-based和transport-based算法<sup><a href="#fn_15,21,25" id="reffn_15,21,25">15,21,25</a></sup>。density-based算法假定core是紧密连接的节点集合，而transport-based算法认为core是别的节点可以通过较短的路径到达（can be reached from other nodes along short paths ）的节点集合。本研究中，我们使用前者，即density-based算法。</p><h2 id="II-Methods"><a href="#II-Methods" class="headerlink" title="II. Methods"></a>II. Methods</h2><h3 id="A-Algorithm"><a href="#A-Algorithm" class="headerlink" title="A. Algorithm"></a>A. Algorithm</h3><p>我们针对多核心情况，对BE<sup><a href="#fn_11" id="reffn_11">11</a></sup>提出的C-P结构进行了扩展。在BE算法中，我们考虑一个具有$N$个节点、$M$条边的网络（以下与图同义），$A =(A_{ij})$是邻接矩阵，如果节点$i$和节点$j$之间有边，令$A_{ij}=1$，否则$A_{ij}=0$。我们假设图是无向无权图，没有自环路（self-loop），也即对所有的$i$和$j$，有$A_{ij}=A_{ji}$并且$A_{ii}=0$。定义$x=(x_1,x_2, …, x_N)$是长度为$N$的向量，如果节点$i$是边缘节点，则$x_i=0$，否则$x_i=1$。我们定义，网络中理想的C-P结构中：每个的core节点都和core节点或periphery节点相连，每个periphery节点都和所有的core节点相连但不和其他的periphery节点相连。对应的灵界矩阵为$B(x)=(b_{ij}(x))$，如下给出：</p><script type="math/tex; mode=display">B_{ij}(x)=\left\{\begin{aligned}&1&(x_i=1\ or\ x_j=1,\ and\ i\not=j), \\&0&(otherwise)\end{aligned}\right.      \quad (1)</script><p>我们使用BE提出的离散模型：即寻找$x$，使得$A$与$B$具有最高的相似度。我们将在Section II C中阐述相似度计算方法。针对多核情况，我们对理想的C-P结构进行扩展，令$C$是C-P pair的个数，$c=(c_1,c_2, …, c_N)$是长度为N的向量，$c_i\in\{1,2,…,C\}$表示节点$i$属于哪一个C-P pair，这里排除了C-P pair之间交叉以及C-P pair内，core与periphery部分交叉的情况。（也即，任何一个节点，只可能属于某一个C-P，且只可能属于该C-P中的C或P，而不能二者兼是。）对应的$B(c,x)$矩阵定义如下</p><script type="math/tex; mode=display">B_{ij}(c,x)=\left\{\begin{aligned}&\delta_{c_i,c_j}&(x_i=1\ or\ x_j=1,\ and\ i\not=j), \\&0&(otherwise)\end{aligned}\right.      \quad(2)</script><p>其中$\delta$是克罗内克（Kronecker ）delta。</p><p>我们通过最大化以下参数来寻找$(c,x)$使得$B(c,x)$与$A$最为接近：</p><script type="math/tex; mode=display">\begin{aligned}Q^{cp}(c,x)&=\sum^N_{i=1}\sum^{i-1}_{j=1}A_{ij}B_{ij}(c,x)-\sum^N_{i=1}\sum^{i-1}_{j=1}pB_{ij}(c,x)\\&=\sum^N_{i=1}\sum^{i-1}_{j=1}(A_{ij}-p)(x_i+x_j-x_ix_j)\delta_{c_i,c_j}\end{aligned}</script><p>其中，$p=M/[N(N-1)/2]$是网络中边的密度（density of edges）。项$\sum^N_{i=1}\sum^{i-1}_{j=1}A_{ij}B_{ij}(c,x)$表示同时在给定图与理想C-P图共现边的个数。null-model项$\sum^N_{i=1}\sum^{i-1}_{j=1}pB_{ij}(c,x)$是期望的理想C-P图和Erdős Rényi随机图（在该图中，节点以概率$p$互相连接，以下简称ER随机图）的<strong>共现边</strong>（如果给定图的1#和2#节点有边，且理想C-P图的1#和2#节点有边，称该边为两个图的共现边）的个数。$Q^{cp}$的范围是$-M$到$M$。较大的$Q^{cp}$表示给定图与理想C-P图之间的共现边个数比随机情况更多。ER随机图在C-P结构分析中广泛使用<sup><a href="#fn_13,24,28,29,38,39" id="reffn_13,24,28,29,38,39">13,24,28,29,38,39</a></sup>，与社区发现中的模块化类似（similar to modularity for community detection），我们的定义允许使用多种null model，如configuration model，更多讨论详见section V。</p><h3 id="B-Maximisation-of-Q-cp"><a href="#B-Maximisation-of-Q-cp" class="headerlink" title="B. Maximisation of $Q^{cp}$"></a>B. Maximisation of $Q^{cp}$</h3><p>我们使用标签切换的启发式算法（label switching heuristic ）<sup><a href="#fn_40, 41" id="reffn_40, 41">40, 41</a></sup>来最大化$Q^{cp}$。开始时，我们通过设置$(c_i,x_i)=(i,1)(1\le i\le N)$把每一个节点都分配到一个不同的核心，然后乱序地扫描所有的节点，扫描到的节点$i$时，暂时性地更新$(c_i,x_i)$到它的邻接节点所属的C-P结构，比如$(c_j,1)$，然后计算$Q^{cp}$的增量。我们也会计算将$(c_i,x_i)$更新到$(c_j,0)$时，$Q^{cp}$的增量。注意到，不论$x_i=0$或$x_i=1$，上述两种情况（cases）都会执行。在每种情况下，我们对节点i的所有邻居（neighbours）都执行此方法来计算$Q^{cp}$的增量（increment）。最终，我们将$(c_i,x_i)$更新到使得$Q^{cp}$暂时增量最大的标签（例如，对neighbour $j$，$(c_j,0)$或$(c_j,1)$）。如果某个标签切换过程（relabelling）未能使得$Q^{cp}$增加，就不会更新$(c_i,x_i)$。当所有节点扫描完毕的时候，如果当前轮（round）所有节点的标签都未改变，算法流程结束；否则，重新随机产生一组节点顺序，依照该顺序，重复执行上述过程。</p><p>当节点$i$的标签（label）由$(c,x)$更新到$(c’,x’)$时，$Q^{cp}$的增量由下式给出：</p><script type="math/tex; mode=display">[待填写]</script><p>这里，$\tilde{d}_{i,(c’,1)}$是节点i的所有邻居中标签为$(c,x)$的节点个数；$\tilde{N}_{c,x}$是标签为$(c,x)$的节点的数量，对扫描到的节点$i$，公式$(4)$最多会被计算$2d_i$次（$d_i$是节点$i$的度），因此，每轮（round）扫描所有节点的时间复杂度是$O(\sum^N_{i=1}d_i)=O(M)$，所以整个算法的时间复杂度是$O(rM)$（$r$是轮数）。在相同初始条件的情况下，执行20次算法过程，并取使$Q^{cp}$最大的那个。</p><h3 id="C-Significance-of-the-core-periphery-structure（C-P结构的显著性）"><a href="#C-Significance-of-the-core-periphery-structure（C-P结构的显著性）" class="headerlink" title="C. Significance of the core-periphery structure（C-P结构的显著性）"></a>C. Significance of the core-periphery structure（C-P结构的显著性）</h3><p>检测出的C-P结构在统计意义上可能是非显著的<sup><a href="#fn_11,38" id="reffn_11,38">11,38</a></sup>，因此，我们把针对单C-P结构检测的统计测试方法推广到多C-P情况中。</p><p>在单C-P结构的统计测试中<sup><a href="#fn_38" id="reffn_38">38</a></sup>，我们使用基于皮尔森相关系数<sup><a href="#fn_11" id="reffn_11">11</a></sup>的质量函数来衡量一个C-P结构的显著性，该质量函数的定义如下：</p><script type="math/tex; mode=display">【待补充】</script><p>这里，$p_B=\sum^N_{i=1} \sum^{i-1}_{j=1} B_{ij}(x) / [N(N-1)/2]$。对检测出的C-P结构，如果计算出的$Q^{cp}_{BE}$比在ER随机图模型上计算出的$Q^{cp}_{BE}$值更大，则认为该C-P结构是显著的。（注意，该ER随机图模型的边的数量要与待检测图的边数相等，另外，可以生成多个参数相同的ER图，计算出最大的$Q^{cp}_{BE}$）。使用KL算法（Kernighan-Lin ）<sup><a href="#fn_42" id="reffn_42">42</a></sup>来最大化$Q^{cp}_{BE}$。如果待检测图的$Q^{cp}_{BE}$比随机图的$Q^{cp}_{BE}$大$1-\alpha$，认为该C-P结构的显著性水平为$\alpha$。</p><p>在多C-P结构的情形下，我们实质上对检测出的每一个C-P结构均作了类似的显著性测试。对每一个检测到的C-P结构，我们首先计算$Q^{cp}_{BE}$；然后，我们生成3000个与C-P结构图（注：该C-P结构包含的节点所组成的小图）的边和节点数相同的ER随机图模型，在统计边数时，我们只考虑与C-P结构相连的边的个数；其次，我们使用KL算法，通过最大化$Q^{cp}_{BE}$来检测随机矩阵中的单C-P结构；最后，我们对比待检测图C-P结构和随机矩阵的$Q^{cp}_{BE}$，如果某个C-P结构被认定为是不显著的，我们称该结构中包含的节点为残余节点。它们不属于任何一个C-P结构。</p><p>如果我们以$\alpha$的显著性水平检测到了C个C-P pair，则至少有一个flase positive（假正例，如，一个非显著的C-P对被检测为显著的）的概率是$1-(1-\alpha)^C$，这个概率随着$C$的增大而增大。为了减轻这种多对比的问题（To remedy this multiple comparison problem ），我们使用Šidák correction方法，我们以$\alpha_1=1-(1-\alpha)^{1/C}$（也即$1-(1-\alpha)^C=\alpha$）的显著性水平检测每一个C-P。并将$\alpha$设为0.01。</p><p>我们使用由KL算法得到的最大化的$Q^{cp}_{BE}$来进行显著性检测，其实也可以使用其它的算法来最大化$Q^{cp}_{BE}$。当然，我们也可以使用不同的统计测试方案，<em>比如，限制在单C-P结构中的$Q^{cp}$</em>。</p><h2 id="III-Variation-of-information"><a href="#III-Variation-of-information" class="headerlink" title="III. Variation of information"></a>III. Variation of information</h2><p>对于包含C-P结构的人工网络，我们通过计算$VI$的值来对比网络中的真实C-P结构$(c,x)$和用本算法检测出的C-P结构$(\hat{c},\hat{x})$，$VI$的计算方法如下：</p><script type="math/tex; mode=display">[daubuh]</script><p>这里$P(c,x;\hat{c},\hat{x})$是真实标签为$(c,x)$，预测标签为$(\hat{c},\hat{x})$的节点的比例（the fraction of nodes that have the true label $ (c, x)$ and inferred label $(\hat{c},\hat{x})$）当且仅当$VI$的值为0的时候，检测到的C-P结构和真实的C-P结构相同。我们通过取100次计算的平均值来衡量算法在人工网络上的效果。</p><h2 id="IV-Result"><a href="#IV-Result" class="headerlink" title="IV. Result"></a>IV. Result</h2><h2 id="V-Discussion"><a href="#V-Discussion" class="headerlink" title="V. Discussion"></a>V. Discussion</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Finding-multiple-core-periphery-pairs-in-networks&quot;&gt;&lt;a href=&quot;#Finding-multiple-core-periphery-pairs-in-networks&quot; class=&quot;headerlink&quot; title=&quot;Finding multiple core-periphery pairs in networks&quot;&gt;&lt;/a&gt;Finding multiple core-periphery pairs in networks&lt;/h1&gt;&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;在一个包含C-P结构的网络中，core节点之间紧密连接，periphery节点与core节点紧密相连，而periphery节点之间稀疏连接。目前单核识别已有实现方法，在实际网络中，就像“社区”由多个密集团组成一样，网络也应该由多个C-P结构组成。本文提出了一个可扩展的、可以检测多核心结构（不重合）的方法，我们借助人工网络和真实网络阐述我们的算法，例如，在政治论坛网络中的政治倾向以及世界航线网络中发现了明显的C-P结构。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithm" scheme="https://shihanmax.github.io/categories/Algorithm/"/>
    
    
      <category term="CP" scheme="https://shihanmax.github.io/tags/CP/"/>
    
  </entry>
  
  <entry>
    <title>HMM在中文分词中的应用</title>
    <link href="https://shihanmax.github.io/2018/12/07/HMM%E5%9C%A8%E5%88%86%E8%AF%8D%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>https://shihanmax.github.io/2018/12/07/HMM%E5%9C%A8%E5%88%86%E8%AF%8D%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</id>
    <published>2018-12-06T16:00:00.000Z</published>
    <updated>2019-08-17T07:35:51.290Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>分词，命名实体识别，关系抽取，甚至是指代消解，都可以看成是一个序列标注的问题，这是NLP领域的基本问题，掌握序列标注对于更好地理解NLP任务有很大的帮助。这篇文章主要参考<sup><a href="#fn_1" id="reffn_1">1</a></sup>，是对这篇博文和网上其他一些资料的阅读笔记。</p><a id="more"></a><h3 id="分词的背景"><a href="#分词的背景" class="headerlink" title="分词的背景"></a>分词的背景</h3><p>分词是指将一句话中的字串切分成合理的块，组成合理的词语的过程。</p><p>一些语言（如英文）使用空格分割词汇，词和词之间有着天然的分割线，对这类语言，一般只需要按空格切词，去除停词，再考虑特殊的词汇（如Mr’s）即可，但中文的字和字之间没有任何分隔符，对于一句话“今天天气很好”，分词的任务是寻找一组最佳的分割方式，使得字和字之间的搭配达到语义上和语法上的概率最大化。</p><p>对于示例语句“今天天气很好”，一个合理的分词结果为“今天/天气/很/好”。</p><p>目前常用的分词方法有以下三种：</p><ol><li>基于语言学知识的规则化方法（最大匹配、最小切分等）</li><li>基于大量预料的机器学习方法（如HMM、CRF等模型）</li><li>规则与统计模型相结合的方法</li></ol><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><p>对于分词这种序列标注任务，定义原语句为观测序列O，分词结果为标注序列L。我们的目标是寻找最优的标注序列，使得给定O时，L出现的概率最大，即$argmaxP(L\mid O)$。</p><p>一种标注序列的定义方式是BOI标签，B代表词开头，O代表其它，I代表词中间，在分词任务中，可以仅适用B和I，如对示例文本的标注结果为：“今/B天/I天/B气/I很/B好/B”。</p><p>对于给定的长度为n的文本，所有可能的序列标注情况为$2^n$种。</p><p>待完成。。。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote id="fn_1"><sup>1</sup>. <a href="http://www.52nlp.cn/itenyh%E7%89%88-%E7%94%A8hmm%E5%81%9A%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E4%B8%80%EF%BC%9A%E5%BA%8F" target="_blank" rel="noopener">用HMM做中文分词</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;分词，命名实体识别，关系抽取，甚至是指代消解，都可以看成是一个序列标注的问题，这是NLP领域的基本问题，掌握序列标注对于更好地理解NLP任务有很大的帮助。这篇文章主要参考&lt;sup&gt;&lt;a href=&quot;#fn_1&quot; id=&quot;reffn_1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;，是对这篇博文和网上其他一些资料的阅读笔记。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="https://shihanmax.github.io/categories/NLP/"/>
    
    
      <category term="机器学习" scheme="https://shihanmax.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch初探</title>
    <link href="https://shihanmax.github.io/2018/11/23/Elasticsearch/"/>
    <id>https://shihanmax.github.io/2018/11/23/Elasticsearch/</id>
    <published>2018-11-22T16:00:00.000Z</published>
    <updated>2019-08-17T16:42:09.660Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h2><p><strong>Elasticsearch</strong>是一个高伸缩的开源全文搜索和分析引擎，它可以快速地、近实时的存储，搜索和分析大规模的数据。一般被用作底层引擎/技术，为具有复杂搜索功能和要求的应用提供强有力的支撑。</p><a id="more"></a><h2 id="使用场景："><a href="#使用场景：" class="headerlink" title="使用场景："></a>使用场景：</h2><ul><li>电商网站，商品的检索</li><li>日志或交易数据的统计、汇报、总结</li><li>在百万或十亿条数据中查找特定的问题</li></ul><h2 id="基本概念："><a href="#基本概念：" class="headerlink" title="基本概念："></a>基本概念：</h2><h3 id="近实时-NRT"><a href="#近实时-NRT" class="headerlink" title="近实时 NRT"></a>近实时 NRT</h3><p>ES平台接近实时，从文档索引到可搜索的时间非常短暂。</p><h3 id="集群-Cluster"><a href="#集群-Cluster" class="headerlink" title="集群 Cluster"></a>集群 Cluster</h3><p>集群是一个或多个<strong>节点（Server）</strong>的集合，它们联合起来保存所有的数据，并且可以在任何一个节点上进行索引和搜索操作。集群由唯一的名称标识，默认是elasticsearch。一个节点只能属于一个集群。你也可以有多个集群，每个集群都有一个唯一的名字。</p><h3 id="节点-Node"><a href="#节点-Node" class="headerlink" title="节点 Node"></a>节点 Node</h3><p>一台独立的服务器就是一个节点，是集群的一部分，参与集群的索引与检索。同样地，节点拥有一个唯一的名字，默认是一个随机的UUID，当然也可以定制。节点名称很重要，可以帮助区分集群服务器和节点的对应关系。</p><p>节点通过配置集群名称之后，就可以加入指定的集群，默认情况下节点都会加入到默认集群elasticsearch。</p><h3 id="索引-Index"><a href="#索引-Index" class="headerlink" title="索引 Index"></a>索引 Index</h3><p>索引是具有相似特点的文档集合，如客户数据，产品目录等。</p><p>索引由名称（只能使用小写字母）来标识，该名称用于对文档的索引、搜索、更新和删除等操作。单个集群中，可以定制任意数量的索引。</p><h3 id="文档-Document"><a href="#文档-Document" class="headerlink" title="文档 Document"></a>文档 Document</h3><p>文档是可以被索引的基本单位，如用一个文档保存客户的数据，或单个商品的数据。文档用JSON表示，在索引中可以存储大量文档。</p><h3 id="分片和副本-Shards-amp-Replicas"><a href="#分片和副本-Shards-amp-Replicas" class="headerlink" title="分片和副本 Shards &amp; Replicas"></a>分片和副本 Shards &amp; Replicas</h3><p>一个索引可能存储海量数据，有可能超过单个节点的硬盘容量，为了解决这个问题，ES提供了分片功能，即索引细分。即创建索引时，可以定义分片数，每个分片具备索引的全部功能，可以存放在集群中的任何一个节点上。</p><p>分片很重要，原因如下：</p><ul><li>允许水平分割/缩放内容量</li><li>允许并行地分发操作到多个节点的分片上，从而提高性能和吞吐量</li></ul><p>分片分发机制，以及在检索中如何汇总到搜索响应的过程完全由ES管理，并且是透明的。</p><p>在网络/云环境中，故障可能会发生，此时，分片会非常有用，强烈建议使用故障转移机制，以防止节点脱机。ES允许将索引的分片复制多份，即副本。</p><p>副本的重要性：</p><ul><li>在分片/节点故障时，提高可用性（注意，副本与原始/主分片不能分配在同一个节点上）</li><li>允许扩展搜索量，可对所有副本执行搜索</li></ul><p>每个索引可以分为多个分片。每个索引也可以被复制零次（意味着没有副本）或多次。一旦复制，每个索引将具有主分片（原始分片）和副分片（主分片的副本）。可以在创建索引时根据索引定义分片和副本的数量。创建索引后，您可以随时动态更改副本数，但不能更改分片数。</p><p>默认情况下，每个索引都会被分配5个主分片和1一个复制分片，这意味着如果你的集群中有两个节点，你的索引将会有5个主分片和5个复制分片，总共有10个分片。</p><p>每个分片是一个Lucene index，一个Lucene index中可以有很多的文档，截至 LUCENE-5843，最多2147483519(= Integer.MAX_VALUE - 128) 个文档。可以使用 _cat/shards api监视分片大小。</p><p><a href="http://www.cnblogs.com/xing901022/p/3933675.html" target="_blank" rel="noopener">关于全文检索，Lucene和倒排索引</a></p><h2 id="ES初探"><a href="#ES初探" class="headerlink" title="ES初探"></a>ES初探</h2><h3 id="安装、启动"><a href="#安装、启动" class="headerlink" title="安装、启动"></a>安装、启动</h3><p>安装过程参考ES官网，注意ES依赖Java8环境。</p><p>启动方法：</p><pre><code>cd elasticsearch-5.4.1./bin/elasticsearch</code></pre><h3 id="REST-API"><a href="#REST-API" class="headerlink" title="REST API"></a>REST API</h3><p>ES提供了非常强大的REST API与集群进行通讯。</p><p>API能力：</p><ul><li>检查集群、节点、索引的运行状态和统计信息</li><li>管理集群、节点、索引和元数据</li><li>执行CRUD，针对索引进行搜索</li><li>执行高级操作，如分页、排序、过滤、脚本、聚合等</li></ul><h4 id="查看集群、节点、索引"><a href="#查看集群、节点、索引" class="headerlink" title="查看集群、节点、索引"></a>查看集群、节点、索引</h4><pre><code>GET /_cat/health?v    # 查看节点健康GET /_cat/nodes?v    # 查看集群节点列表GET /_cat/indices?v    # 查看所有索引，如果集群中只有一个节点，则此索引的健康值为yellow（副本未分配）</code></pre><h4 id="索引的创建和删除"><a href="#索引的创建和删除" class="headerlink" title="索引的创建和删除"></a>索引的创建和删除</h4><pre><code>PUT /bank?pretty    # 创建名为testindex的索引DELETE /bank?pretty    # 删除testindex索引</code></pre><h4 id="文档的创建、修改和删除"><a href="#文档的创建、修改和删除" class="headerlink" title="文档的创建、修改和删除"></a>文档的创建、修改和删除</h4><pre><code>PUT /bank/account/1?pretty{    &quot;name&quot;: &quot;John Doe&quot;}    # 索引一篇文档，类型为sometype，ID为1（此ID也可不指定）索引PUT /bank/account/1?pretty{    &quot;name&quot;: &quot;Maxan&quot;}    # 如果对已存在的文档修改内容后，再次执行索引，ES会使用一个新文档取代旧文档（重建索引）POST /bank/account?pretty{    &quot;name&quot;: &quot;Tom&quot;}    # 没有指定ID，使用POST方法取代PUTPOST /bank/account/1/_update?pretty{    &quot;doc&quot;: { &quot;name&quot;: &quot;Maxan&quot;, &quot;age&quot;: 24}}    # 对id为1的文档执行更新，ES会删除旧文档，索引新文档DELETE /bank/account/1?pretty    # 删除ID为1的文档</code></pre><h4 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h4><pre><code>POST /bank/account/_bulk?pretty{&quot;index&quot;: {&quot;_id&quot;: &quot;1&quot;}}{&quot;name&quot;: &quot;Tom hanks&quot;}{&quot;index&quot;: {&quot;_id&quot;: &quot;2&quot;}}{&quot;name&quot;: &quot;Jack&quot;}{&quot;update&quot;: {&quot;_id&quot;: &quot;1&quot;}}{&quot;doc&quot;: {&quot;name&quot;: &quot;Hello World my name is ES&quot;}}{&quot;delete&quot;: {&quot;_id&quot;: &quot;2&quot;}}    # 上面的请求批量索引了两篇文档，注意：最后要有空行</code></pre><h4 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h4><pre><code>GET /_search?q=*&amp;sort=account_number:asc&amp;pretty    # 在所有索引下查询文档（方式1：通过REST request URI 方式发送查询参数）GET /bank/_search?q=*&amp;sort=account_number:asc&amp;pretty    #在bank下查询文档GET /bank/_search{  &quot;query&quot;: { &quot;match_all&quot;: {} },  &quot;sort&quot;: [        { &quot;account_number&quot;:&quot;asc&quot; }  ]}    # 使用方式二查询（通过REST request body）</code></pre><p>返回结果解释</p><ul><li>took： 查询时间（ms）</li><li>time_out: 是否超时</li><li>_shards: 查询了多少分片</li><li>hits: 查询结果</li><li>hits.total: 符合我们查询条件的文档总数</li><li>hits.hits: 实际查询结果数组（默认为前10个文档）</li><li>hits.sort: 对结果进行排序的键（如果没提供，则默认使用_score进行排序）</li><li>hits._score:</li><li>max_score:</li></ul><h3 id="查询语言介绍（Query-DSL）"><a href="#查询语言介绍（Query-DSL）" class="headerlink" title="查询语言介绍（Query DSL）"></a>查询语言介绍（Query DSL）</h3><p>match_all</p><pre><code>GET /bank/_search{  &quot;query&quot;: { &quot;match_all&quot;: {} }}    # &quot;query&quot;: 指定查询定义    # &quot;match_all&quot;: 查询类型    # &quot;size&quot;: 返回结果数    # &quot;from&quot;: 起始位置    # &quot;sort&quot;: {&quot;age&quot; : { &quot;order&quot;: &quot;desc&quot;}}  # 按age降序排序    # &quot;_source&quot;: {&quot;name&quot;, &quot;age&quot;}</code></pre><p>match</p><pre><code>GET /bank/_search{  &quot;query&quot;: { &quot;match&quot;: { &quot;address&quot;: &quot;mail Goodson&quot; } }}    # &quot;match&quot;: 指针对特定字段或一组字段进行搜索，上例返回address中包含&quot;mail&quot;或&quot;Goodson&quot;的文档</code></pre><p>match_phase</p><pre><code>GET /bank/_search{  &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;address&quot;: &quot;mail Goodson&quot; } }}    # 返回包含&quot;main Goodson&quot;短语的文档</code></pre><h4 id="bool-query"><a href="#bool-query" class="headerlink" title="bool query"></a>bool query</h4><p>must</p><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must&quot;: [        { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } },        { &quot;match&quot;: { &quot;address&quot;: &quot;lane&quot; } }      ]    }  }}    # 查询address中既包含&quot;mail&quot;又包含&quot;lane&quot;的文档</code></pre><p>should</p><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;should&quot;: [        { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } },        { &quot;match&quot;: { &quot;address&quot;: &quot;lane&quot; } }      ]    }  }}    # should条件中只要有一条满足就可返回</code></pre><p>must_not</p><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must_not&quot;: [        { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } },        { &quot;match&quot;: { &quot;address&quot;: &quot;lane&quot; } }      ]    }  }}    # must_not，所有条件都不应该满足</code></pre><h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>查询结果中_score字段代表查询分数，是一个数值，表示匹配度，越高说明越匹配。</p><p>布尔查询支持filter子句，它允许使用查询语句限制其它子句的匹配结果，同时不会计算文档的得分。</p><pre><code>GET /bank/_search{  &quot;query&quot;: {    &quot;bool&quot;: {      &quot;must&quot;: { &quot;match_all&quot;: {} },      &quot;filter&quot;: {        &quot;range&quot;: {          &quot;balance&quot;: {            &quot;gte&quot;: 20000,            &quot;lte&quot;: 30000          }        }      }    }  }}    # 查询余额balance在20000到30000之间的结果</code></pre><h4 id="执行聚合"><a href="#执行聚合" class="headerlink" title="执行聚合"></a>执行聚合</h4><pre><code>GET /bank/_search{  &quot;size&quot;: 0,  &quot;aggs&quot;: {    &quot;group_by_age&quot;: {      &quot;range&quot;: {        &quot;field&quot;: &quot;age&quot;,        &quot;ranges&quot;: [          {            &quot;from&quot;: 20,            &quot;to&quot;: 30          },          {            &quot;from&quot;: 30,            &quot;to&quot;: 40          },          {            &quot;from&quot;: 40,            &quot;to&quot;: 50          }        ]      },      &quot;aggs&quot;: {        &quot;group_by_gender&quot;: {          &quot;terms&quot;: {            &quot;field&quot;: &quot;gender.keyword&quot;          },          &quot;aggs&quot;: {            &quot;average_balance&quot;: {              &quot;avg&quot;: {                &quot;field&quot;: &quot;balance&quot;              }            }          }        }      }    }  }}    # 按年龄段分组，按性别分组，最终得到每个年龄段的男女平均账户余额</code></pre><p>2018/11/23 16:40:32</p><p>参考：[<a href="https://github.com/13428282016/elasticsearch-CN/wiki/es-gettting-started" target="_blank" rel="noopener">https://github.com/13428282016/elasticsearch-CN/wiki/es-gettting-started</a>]</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Elasticsearch&quot;&gt;&lt;a href=&quot;#Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch&quot;&gt;&lt;/a&gt;Elasticsearch&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Elasticsearch&lt;/strong&gt;是一个高伸缩的开源全文搜索和分析引擎，它可以快速地、近实时的存储，搜索和分析大规模的数据。一般被用作底层引擎/技术，为具有复杂搜索功能和要求的应用提供强有力的支撑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="工具" scheme="https://shihanmax.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="language" scheme="https://shihanmax.github.io/tags/language/"/>
    
      <category term="ElasticSearch" scheme="https://shihanmax.github.io/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>海量数据之随机选取</title>
    <link href="https://shihanmax.github.io/2018/10/25/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B9%8B%E9%9A%8F%E6%9C%BA%E9%80%89%E5%8F%96/"/>
    <id>https://shihanmax.github.io/2018/10/25/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B9%8B%E9%9A%8F%E6%9C%BA%E9%80%89%E5%8F%96/</id>
    <published>2018-10-24T16:00:00.000Z</published>
    <updated>2019-08-17T07:26:09.310Z</updated>
    
    <content type="html"><![CDATA[<p>问题：</p><p>常规情况下，从一个长度为n的数组中等概率选取k个元素的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">selectedIdx = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> k:</span><br><span class="line">    idx = random(<span class="number">1</span>, n)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> idx <span class="keyword">in</span> selectIdx:</span><br><span class="line">        selectIdx.append(idx)</span><br></pre></td></tr></table></figure><p>当数组长度未知时（如给定一个链表，长度未知），要求只扫描一遍链表的前提下，等概率选取k个数字，此时上述方法就不可用了。</p><a id="more"></a><p>先看一下基本的情况：</p><p>一、从长度未知的链表中等概率、随机选择1个数（Random Pick）</p><p>第一次直接选取头结点元素作为choice，而后以二分之一的概率决定是否用下一个替换它，以1/3的概率决定是否使用第三个元素替换choice…</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">randomSelect</span><span class="params">(pHead)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> select;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">    ListNode pCurr = pHead;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (pCurr != <span class="keyword">null</span>) &#123;</span><br><span class="line">        randomNum = random(<span class="number">1</span>, count)</span><br><span class="line">            <span class="keyword">if</span> (randomNum == <span class="number">1</span>) &#123;</span><br><span class="line">                select = pCurr.val;</span><br><span class="line">            &#125;</span><br><span class="line">        pCurr = pCurr.next;</span><br><span class="line">        count++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> select;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>二、从长度未知的链表中等概率、随机选择k个数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">randomSelect</span><span class="params">(pHead, k)</span> </span>&#123;</span><br><span class="line">    ListNode pCurr = pHead;</span><br><span class="line">    <span class="keyword">int</span>[] choice = <span class="keyword">new</span> <span class="keyword">int</span>[k];</span><br><span class="line">    i = k + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (pCurr != <span class="keyword">null</span>) &#123;</span><br><span class="line">        r = random(<span class="number">1</span>, i);</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">1</span> &lt;= r &lt;= k) &#123;</span><br><span class="line">            choice[r] = pCurr.val;</span><br><span class="line">        &#125;</span><br><span class="line">        pCurr = pCurr.next;</span><br><span class="line">        i++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>证明如下：</p><p>对于第1个结点，被选中，且未在后续选取中被替换：</p><p>p = k/k+1 <em> k+1/k+2 </em> k+2/k+3 <em> … </em> n-1/n = k/n</p><p>对于第2个结点：</p><p>p =           k/k+2 <em> k+2/k+3 </em> … * n-1/n = k/n</p><p>…</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;问题：&lt;/p&gt;
&lt;p&gt;常规情况下，从一个长度为n的数组中等概率选取k个元素的方法：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;selectedIdx = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; k:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    idx = random(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, n)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; idx &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; selectIdx:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        selectIdx.append(idx)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;当数组长度未知时（如给定一个链表，长度未知），要求只扫描一遍链表的前提下，等概率选取k个数字，此时上述方法就不可用了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="算法" scheme="https://shihanmax.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="概率" scheme="https://shihanmax.github.io/tags/%E6%A6%82%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Java通过JNI调用C库</title>
    <link href="https://shihanmax.github.io/2018/10/15/Java%E9%80%9A%E8%BF%87JNI%E8%B0%83%E7%94%A8C%E5%BA%93/"/>
    <id>https://shihanmax.github.io/2018/10/15/Java%E9%80%9A%E8%BF%87JNI%E8%B0%83%E7%94%A8C%E5%BA%93/</id>
    <published>2018-10-14T16:00:00.000Z</published>
    <updated>2019-08-17T07:36:42.540Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是库"><a href="#什么是库" class="headerlink" title="什么是库"></a>什么是库</h3><p>库是写好的现有的，成熟的，可以复用的代码。现实中每个程序都要依赖很多基础的底层库，不可能每个人的代码都从零开始，因此库的存在意义非同寻常。本质上来说库是一种可执行代码的二进制形式，可以被操作系统载入内存执行。库有两种：静态库（.a、.lib）和动态库（.so、.dll）。<br>所谓静态、动态是指链接。</p><a id="more"></a><h3 id="静态库"><a href="#静态库" class="headerlink" title="静态库"></a>静态库</h3><p>之所以成为【静态库】，是因为在链接阶段，会将汇编生成的目标文件.o与引用到的库一起链接打包到可执行文件中。因此对应的链接方式称为静态链接。<br>试想一下，静态库与汇编生成的目标文件一起链接为可执行文件，那么静态库必定跟.o文件格式相似。其实一个静态库可以简单看成是一组目标文件（.o/.obj文件）的集合，即很多目标文件经过压缩打包后形成的一个文件。静态库特点总结：</p><ul><li>静态库对函数库的链接是放在编译时期完成的。</li><li>程序在运行时与函数库再无瓜葛，移植方便。</li><li>浪费空间和资源，因为所有相关的目标文件与牵涉到的函数库被链接合成一个可执行文件。</li></ul><h3 id="动态库"><a href="#动态库" class="headerlink" title="动态库"></a>动态库</h3><p>通过上面的介绍发现静态库，容易使用和理解，也达到了代码复用的目的，那为什么还需要动态库呢？<br>为什么还需要动态库？<br>为什么需要动态库，其实也是静态库的特点导致。</p><ul><li>空间浪费是静态库的一个问题。</li><li>另一个问题是静态库对程序的更新、部署和发布页会带来麻烦。如果静态库liba.lib更新了，所以使用它的应用程序都需要重新编译、发布给用户（对于玩家来说，可能是一个很小的改动，却导致整个程序重新下载，全量更新）。<br>动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入。不同的应用程序如果调用相同的库，那么在内存里只需要有一份该共享库的实例，规避了空间浪费问题。动态库在程序运行是才被载入，也解决了静态库对程序的更新、部署和发布页会带来麻烦。用户只需要更新动态库即可，增量更新。</li></ul><h3 id="动态库特点总结："><a href="#动态库特点总结：" class="headerlink" title="动态库特点总结："></a>动态库特点总结：</h3><ul><li>动态库把对一些库函数的链接载入推迟到程序运行的时期。</li><li>可以实现进程之间的资源共享。（因此动态库也称为共享库）</li><li>将一些程序升级变得简单。</li><li>甚至可以真正做到链接载入完全由程序员在程序代码中控制（显示调用）。</li></ul><h3 id="实验-linux上编译动态链接库lib-so的过程-："><a href="#实验-linux上编译动态链接库lib-so的过程-：" class="headerlink" title="实验(linux上编译动态链接库lib*.so的过程)："></a>实验(linux上编译动态链接库lib*.so的过程)：</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> <span class="comment">//新建Demo.java</span></span><br><span class="line">    <span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Hello</span> &#123;</span></span><br><span class="line">        <span class="keyword">static</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                System.loadLibrary(<span class="string">"hello"</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span>(UnsatisfiedLinkError e) &#123;</span><br><span class="line">                System.err.<span class="built_in">println</span>(<span class="string">"cannot load library"</span> + e.toString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">Hello</span><span class="params">()</span></span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> native <span class="keyword">void</span> <span class="title">sayHello</span><span class="params">(<span class="keyword">String</span> name)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">String</span>[] args)</span> </span>&#123;</span><br><span class="line">            Hello hello = <span class="keyword">new</span> Hello();</span><br><span class="line">            hello.sayHello(<span class="string">"jack!"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> <span class="comment">//生成Demo.h</span></span><br><span class="line">i. javac Demo.java</span><br><span class="line">ii. javah Demo</span><br><span class="line">iii. <span class="comment">//此时目录下生成Demo.h，包含对函数sayHello()的声明</span></span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> <span class="comment">//新建Demo.cpp，按照Demo.h中的声明格式，实现函数sayHello()</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"Hello.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> <span class="comment">// 与 Hello.h 中函数声明相同</span></span><br><span class="line"><span class="function">JNIEXPORT <span class="keyword">void</span> JNICALL <span class="title">Java_Hello_sayHello</span>  <span class="params">(JNIEnv * env, jobject arg, jstring instring)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 从 instring 字符串取得指向字符串 UTF 编码的指针</span></span><br><span class="line">    <span class="keyword">const</span> jbyte *str = (<span class="keyword">const</span> jbyte *)env-&gt;GetStringUTFChars( instring, JNI_FALSE );</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello,%s\n"</span>,str);</span><br><span class="line">    <span class="comment">// 通知虚拟机本地代码不再需要通过 str 访问 Java 字符串。</span></span><br><span class="line">    env-&gt;ReleaseStringUTFChars( instring, (<span class="keyword">const</span> <span class="keyword">char</span> *)str );</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> <span class="comment">//编译Demo.cpp</span></span><br><span class="line">i. g++ -I/usr/lib/jvm/java<span class="number">-8</span>-oracle/include -I/usr/lib/jvm/java<span class="number">-8</span>-oracle/include/linux -fPIC -c Hello.cpp</span><br><span class="line">ii. g++ -shared Hello.o -o libhello.so</span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> <span class="comment">//将动态库libhello.so 放入usr/lib中，或者将其路径添加到动态库搜索路径</span></span><br><span class="line"></span><br><span class="line"><span class="number">6.</span> <span class="comment">//通过Demo.java测试动态库的调用</span></span><br><span class="line">i. javac Demo.java</span><br><span class="line">ii. java Demo</span><br><span class="line">Hello,jack!</span><br></pre></td></tr></table></figure><p>References</p><p><a href="https://www.cnblogs.com/i80386/p/4442330.html" target="_blank" rel="noopener">https://www.cnblogs.com/i80386/p/4442330.html</a><br><a href="https://blog.csdn.net/chlaws/article/details/7650378/" target="_blank" rel="noopener">https://blog.csdn.net/chlaws/article/details/7650378/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;什么是库&quot;&gt;&lt;a href=&quot;#什么是库&quot; class=&quot;headerlink&quot; title=&quot;什么是库&quot;&gt;&lt;/a&gt;什么是库&lt;/h3&gt;&lt;p&gt;库是写好的现有的，成熟的，可以复用的代码。现实中每个程序都要依赖很多基础的底层库，不可能每个人的代码都从零开始，因此库的存在意义非同寻常。本质上来说库是一种可执行代码的二进制形式，可以被操作系统载入内存执行。库有两种：静态库（.a、.lib）和动态库（.so、.dll）。&lt;br&gt;所谓静态、动态是指链接。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Java" scheme="https://shihanmax.github.io/categories/Java/"/>
    
    
      <category term="JNI" scheme="https://shihanmax.github.io/tags/JNI/"/>
    
  </entry>
  
  <entry>
    <title>Python垃圾回收机制（GC）</title>
    <link href="https://shihanmax.github.io/2018/10/10/Python%E4%B8%AD%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%EF%BC%88GC%EF%BC%89/"/>
    <id>https://shihanmax.github.io/2018/10/10/Python%E4%B8%AD%E7%9A%84%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%EF%BC%88GC%EF%BC%89/</id>
    <published>2018-10-09T16:00:00.000Z</published>
    <updated>2019-08-17T07:38:10.220Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Python中的GC机制："><a href="#Python中的GC机制：" class="headerlink" title="Python中的GC机制："></a>Python中的GC机制：</h3><ul><li>以引用计数为主</li><li>分代回收为辅</li></ul><p>python对象的核心是一个结构体：PyObject</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> struct_object &#123;</span><br><span class="line">    <span class="keyword">int</span> ob_refcnt;</span><br><span class="line">    struct_typeobject *ob_type;</span><br><span class="line">&#125; PyObject;</span><br><span class="line"></span><br><span class="line"><span class="meta">#def Py_INCREF(op) ((op)-&gt;ob_refcnt++)</span></span><br><span class="line"><span class="meta">#def Py_DECREF(op)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (--(op)-&gt;ob_refcnt != <span class="number">0</span>)</span><br><span class="line">    ;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    __Py_Dealloc((PyObject *)(op))</span><br></pre></td></tr></table></figure><p>ob_refcnt 就是为了做引用计数，当一个对象的引用为0时，会被清除。</p><a id="more"></a><p>Python在合适的时机会对处在链表中的有循环引用的对象引用减1，这样，原本有引用的对象不会被清除，而原本循环引用的对象，其引用计数被置零并回收。</p><p>零代链中的无引用对象，将剩余有引用对象挪到一代链，对一代链同理。这三代链子被清理的频率：零代 &gt;&gt; 一代 &gt;&gt; 二代</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> gc</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gc.get_count()  <span class="comment"># 查看当前隔代回收当前状态</span></span><br><span class="line">(<span class="number">154</span>, <span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gc.get_threshold()  <span class="comment"># 新创建的减去释放掉的如果大于此阈值，触发分代回收</span></span><br><span class="line">(<span class="number">700</span>, <span class="number">10</span>, <span class="number">10</span>)  <span class="comment"># 10和10表示：清理10次0代链表后清理一代链表，清理10次一代链表后清理一次二代链表。</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>gc.collect()  <span class="comment"># 显式执行垃圾回收</span></span><br></pre></td></tr></table></figure><p>Python和Ruby的标记-清除机制对比：</p><p>​    Ruby：一次性创建大量可用对象，用完后标记清除。</p><p>​    Python：初始化时才创建对象，一旦引用为0立即清除。</p><h3 id="一点注意"><a href="#一点注意" class="headerlink" title="一点注意"></a>一点注意</h3><p>如果类的<strong>del</strong>方法被重写（未调用父类的del方法），则执行垃圾回收时无法回收该对象。</p><h3 id="引用计数相关"><a href="#引用计数相关" class="headerlink" title="引用计数相关"></a>引用计数相关</h3><h4 id="导致引用计数-1-的情况"><a href="#导致引用计数-1-的情况" class="headerlink" title="导致引用计数 +1 的情况"></a>导致引用计数 +1 的情况</h4><ul><li>对象被创建</li><li>对象被引用</li><li>对象被传入函数中</li><li>对象作为一个元素存放在容器中</li></ul><h4 id="导致引用计数-1-的情况："><a href="#导致引用计数-1-的情况：" class="headerlink" title="导致引用计数 -1 的情况："></a>导致引用计数 -1 的情况：</h4><ul><li>对象被 del 显式销毁</li><li>对象被赋予新的对象</li><li>一个对象离开其作用域（如已经执行完毕的函数中的形参）</li><li>对象所在的容器被销毁</li></ul><h4 id="查看对象的引用计数："><a href="#查看对象的引用计数：" class="headerlink" title="查看对象的引用计数："></a>查看对象的引用计数：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">a = <span class="string">'Hello world'</span></span><br><span class="line">sys.getrefcount(a)</span><br></pre></td></tr></table></figure><h3 id="整数对象池"><a href="#整数对象池" class="headerlink" title="整数对象池"></a>整数对象池</h3><h4 id="小整数对象池"><a href="#小整数对象池" class="headerlink" title="小整数对象池"></a>小整数对象池</h4><p>为了避免整数的频繁申请和销毁内存空间，Python使用了小整数对象池。</p><p>[-5, 257)内的整数都在小整数对象池中，他们都已经被提前建立好了，常驻内存，不被回收。</p><h4 id="大整数对象池"><a href="#大整数对象池" class="headerlink" title="大整数对象池"></a>大整数对象池</h4><p>每一个大整数的定义都会创建一个新的对象。</p><h5 id="字符串共享机制（intern）"><a href="#字符串共享机制（intern）" class="headerlink" title="字符串共享机制（intern）"></a>字符串共享机制（intern）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a1 = <span class="string">'HelloWorld'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a2 = <span class="string">'HelloWorld'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a3 = <span class="string">'HelloWorld'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>id(a1) == id(a2) == id(a3)</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>a1-a5拥有共同的id（实际指向了同一块内存），但如果字符串中有特殊字符（如空格），则不会触发共享机制共享。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Python中的GC机制：&quot;&gt;&lt;a href=&quot;#Python中的GC机制：&quot; class=&quot;headerlink&quot; title=&quot;Python中的GC机制：&quot;&gt;&lt;/a&gt;Python中的GC机制：&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;以引用计数为主&lt;/li&gt;
&lt;li&gt;分代回收为辅&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;python对象的核心是一个结构体：PyObject&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;typedef&lt;/span&gt; struct_object &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ob_refcnt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    struct_typeobject *ob_type;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; PyObject;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#def Py_INCREF(op) ((op)-&amp;gt;ob_refcnt++)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#def Py_DECREF(op)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (--(op)-&amp;gt;ob_refcnt != &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    __Py_Dealloc((PyObject *)(op))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;ob_refcnt 就是为了做引用计数，当一个对象的引用为0时，会被清除。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://shihanmax.github.io/categories/Python/"/>
    
    
      <category term="GC" scheme="https://shihanmax.github.io/tags/GC/"/>
    
  </entry>
  
  <entry>
    <title>Python元类（MetaClass）</title>
    <link href="https://shihanmax.github.io/2018/10/02/Python%E5%85%83%E7%B1%BB%EF%BC%88MetaClass%EF%BC%89/"/>
    <id>https://shihanmax.github.io/2018/10/02/Python%E5%85%83%E7%B1%BB%EF%BC%88MetaClass%EF%BC%89/</id>
    <published>2018-10-01T16:00:00.000Z</published>
    <updated>2019-08-17T07:37:31.450Z</updated>
    
    <content type="html"><![CDATA[<h3 id="元类"><a href="#元类" class="headerlink" title="元类"></a>元类</h3><p>类也是对象。</p><p>类可以动态创建（不建议这样做）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_class</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">'foo'</span>:</span><br><span class="line">        <span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> Foo</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="class"><span class="keyword">class</span> <span class="title">Bar</span><span class="params">(object)</span>:</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> Bar</span><br></pre></td></tr></table></figure><p>类型实际是一个类，如 int、str 等。</p><a id="more"></a><p>使用 type 创建一个类（强调，这样非常不好）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test = type(<span class="string">'Dog'</span>, (Animal, ), &#123;&#125;)</span><br><span class="line"><span class="comment"># 参数:(类名，父类元组，属性字典(属性+方法))</span></span><br></pre></td></tr></table></figure><p>使用类来创建实例对象，通过元类来创建类，而 type 就是一个元类。int 是创建整型的类，str 是创建字符串的类，type 就是创建类对象的类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>age = <span class="number">35</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>age.__class__</span><br><span class="line">int</span><br></pre></td></tr></table></figure><p>元类决定了类的创建方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper_attr</span><span class="params">(future_class_name, future_class_parents, future_class_attr)</span>:</span></span><br><span class="line">    newAttr = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> name, value <span class="keyword">in</span> future_class.attr.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">'__'</span>):</span><br><span class="line">            newAttr[name.upper()] = value</span><br><span class="line">    <span class="keyword">return</span> type(future_class_name, future_class_parents, newAttr)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object, metaclass=upper_attr)</span>:</span></span><br><span class="line">    bar = <span class="string">'bip'</span></span><br></pre></td></tr></table></figure><blockquote><p>元类是深度的魔法，99%的用户不用使用它，如果你想搞清楚究竟是否需要使用元类，那么你就不需要它，那些实际用到元类的人都非常清楚地知道他们需要做什么，而且不用解释为什么要使用元类。 — Tim Peters</p></blockquote><p>所以，我可能暂时不需要它 &gt;_~</p><p>The end.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;元类&quot;&gt;&lt;a href=&quot;#元类&quot; class=&quot;headerlink&quot; title=&quot;元类&quot;&gt;&lt;/a&gt;元类&lt;/h3&gt;&lt;p&gt;类也是对象。&lt;/p&gt;
&lt;p&gt;类可以动态创建（不建议这样做）：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;choose_class&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(name)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; name == &lt;span class=&quot;string&quot;&gt;&#39;foo&#39;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Foo&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; Foo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Bar&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; Bar&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;类型实际是一个类，如 int、str 等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="https://shihanmax.github.io/categories/Python/"/>
    
    
      <category term="元类" scheme="https://shihanmax.github.io/tags/%E5%85%83%E7%B1%BB/"/>
    
  </entry>
  
</feed>
