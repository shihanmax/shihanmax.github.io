<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>XGBoost理解 - Shihanmax</title>
    <meta name="description" content="本文正在整理中…🚧">

    <link href="//fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin-ext,vietnamese"rel="stylesheet">
    <link rel="stylesheet" href="//css/main.css">
    <link rel="canonical" href="http://localhost:4000//2021/03/XGBoost%E7%90%86%E8%A7%A3">
    <link rel="alternate" type="application/rss+xml" title="Shihanmax" href="http://localhost:4000//feed.xml">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
    

    

</head>

    <body>
        <main class="u-container">
        <div class="c-page">
		<article class="c-article">
   	 	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<header class="c-page__header">
        <h1><span class="half_background">Shihanmax's blog</span> 🚀</h1>
	
    <p>
        <a href="//">&lt; Back</a>
    </p>
	
</header>

    	<div class="c-article__main">
        <article class="c-article">
    <header class="c-article__header">
        <h1 class="c-article__title">XGBoost理解</h1>
        <p class="c-article__time"><time datetime="2021-03-28T20:52:03+08:00" itemprop="datePublished">Mar 28, 2021</time></p>
    </header>
    <div class="c-article__main">
        <p><strong>本文正在整理中…🚧</strong></p>

<p>XGBoost（eXtreme Gradient Boosting）是GBDT的一种高效的工程实现，在数据竞赛中应用十分广泛。</p>

<p>下文从决策树的定义开始，介绍决策树的分裂指标、决策树在boosting算法中的应用BDT、引入残差信息的Gradient boosting算法等，最后，介绍集成了多种高效算法的XGBoost的工程实现。</p>

<h2 id="一决策树">一、决策树</h2>

<p>决策树是一种描述对实例进行分类的属性结构，由节点和有向边组成，其中，节点分为内部节点和叶子结点，内部节点表示特征或属性，叶子结点表示一个类。</p>

<p>从根节点开始，对样本的某一个特征进行测试，根据结果，将样本划分到其若干个子节点中，其中每个子节点均对应该特征的一个取值；针对每一个子节点，继续选取一个特征重复执行上述过程，直到将实例划分到叶节点所对应的类别当中。</p>

<p>在上述描述过程中，有一个重要的点是，如何选择一个最好的特征，并使用其进行样本的划分？</p>

<p>直观上考虑，如果划分后的子集拥有更高的“纯度”，则表示该特征是一个好的特征。所谓纯度是指，划分后的子样本集合所对应的类别应尽可能一致，极端考虑，假设我们选择了一个特征，在使用此特征进行划分后，所有的子集合中，样本的类别几乎一致，则可以认为这个特征是一个非常强的特征。</p>

<p>在决策树实现过程中，一般使用三个指标衡量分类纯度：信息增益、信息增益比、基尼指数。首先回顾一下信息论中几个基础的概念：</p>

<p><strong>熵（Entropy）</strong></p>

<p>设$X$是随机变量，$P(X=x_i)=p_i,i=1,2,…,n$，则$X$的熵定义为：</p>

\[H(X)=-\sum_\limits{i=1}^{n}{p_i}\log{p_i}\]

<p>可以看出，$X$的熵的取值仅与$X$的分布有关，与$X$的取值无关，所以这里$H(X)$也可以写作$H(p)$。上式中的对数以$2$为底时，熵的单位为$bit$；以$e$为底时对应的单位为$nat$。</p>

<p>熵可以用来衡量随机变量的不确定性。</p>

<p><strong>条件熵</strong></p>

<p>条件熵$H(Y\mid X)=\sum_\limits{i=1}^{n}{p_i}H(Y\mid X=x_i)$，其中，$p_i=P(X=x_i),i=1,2,…,n$。</p>

<p>条件熵$H(Y\mid X)$表示，在已知随机变量$X$时，随机变量$Y$的不确定性。</p>

<p><strong>信息增益（互信息）</strong></p>

<p>信息增益定义为：$MI(Y,X)=H(Y)-H(Y\mid X)$，表示“得知$X$的信息，而使$X$的<strong>不确定性</strong>减少的程度”。</p>

<p>在决策树的特征选择过程中，使用特征$A$对数据集$D$进行分割，所带来的信息增益可以表示为：$g(D,A)=MI(D,A)=H(D)-H(D\mid A)$</p>

<p><strong>信息增益比</strong></p>

<p>有以上定义可知，通过计算信息增益来进行数据分割时，会偏向取值较多的特征的问题，针对此问题，使用信息增益比来进行修正：</p>

<p>$g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}$，其中$H_A(D)=-\sum_\limits{i=1}^{n}{\frac{\mid D_i\mid}{\mid D\mid}}\log{\frac{\mid D_i\mid}{\mid D\mid}}$，即计算数据集$D$的熵时，特征$A$的贡献。</p>

<p><strong>基尼指数Gini Index</strong></p>

<p>在分类问题中，假设有$k$个类别，样本属第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：</p>

\[Gini(p)=\sum_\limits{k=1}^{K}p_k(1-p_k)=1-\sum_\limits{k=1}^{K}p_k^2\]

<p>则数据集$D$的基尼指数为：</p>

\[Gini(D)=1-\sum_\limits{k=1}^{K}\frac{\mid C_k\mid}{\mid D \mid}\]

<p>其中，$C_k$表示第$k$类样本子集的大小。</p>

<p>假设有特征A，根据特征$A$是否满足特定条件，将数据集划分为两部分：$D_1,D_2$，则可以定义：给定特征$A$的条件下，样本集合$D$的基尼指数$Gini(D,A)$（即给定特征$A$后，样本集合$D$的不确定性减少的程度）：</p>

\[Gini(D,A)=\frac{\mid D_1\mid}{\mid D \mid}Gini(D_1)+\frac{\mid D_2\mid}{\mid D \mid}Gini(D_2)\]

<p>在决策树的构建阶段，我们便可以在进行最优特征选择时，遍历所有的特征，然后计算出其纯度指标（信息增益、信息增益比、基尼指数），然后选出一个指标最高的特征，并根据该特征的取值将样本集合分裂。</p>

<p>常用的决策树算法有ID3算法、C4.5算法和CART算法，其中ID3算法使用信息增益进行特征选择；C4.5算法进行了改进，使用信息增益比进行特征选择；对于分类树，CART算法使用基尼指数最小化准则，对于回归树，使用平方误差最小化准则。</p>

<h2 id="二boosting">二、Boosting</h2>

<p>Boosting是集成学习思想的一种（另一个是Bagging，指并列地训练多个基模型，并通过投票（分类问题）或平均（回归问题）的方式计算得到最终的集成结果）。</p>

<p>与Bagging的可并行不同，Boosting算法的执行流程是串行的，即后面的模型的训练依赖前面的模型的预测结果。典型的算法有Adaboost、GBDT等。下面简单介绍这两类算法。</p>

<h3 id="21-adaboost">2.1 Adaboost</h3>

<p>Adaboost算法（Adaptive Boosting，自适应增强）是boosting算法的代表，其特点是通过迭代训练一系列基分类器，在每次迭代过程中，提高被前一个分类器分错的样本的权重，降低被前一个分类器分对的样本的权重。最后，Adaboost算法将基分类器进行加权线性组合，每个基分类器都有一个权重，分类误差越小的基分类器，其权重越高，否则权重越低。</p>

<p>Adaboost的执行流程简述如下：</p>

<p>首先，初始化数据集为等权重。接着，依次迭代训练$M$个基模型$G_1,G_2,…,G_M$，针对模型$G_m$，执行以下四步：</p>

<ol>
  <li>
    <p>使用带权的数据训练$G_m$</p>
  </li>
  <li>
    <p>计算$G_m$在数据集上的带权分类误差率</p>
  </li>
  <li>
    <p>通过此误差率计算分类器的权重</p>
  </li>
  <li>
    <p>通过带权基模型$G_m$的预测结果，更新训练数据的权重</p>
  </li>
</ol>

<p>在$M$个基模型训练完成后，每个基模型均有一个权重，使用该权重对一系列基模型进行加权组合，即可得到最终的分类器。</p>

<h3 id="22-gbdt">2.2 GBDT</h3>

<p>GBDT全称是Gradient Boost Desision Tree，即梯度提升决策树。与Adaboost不同的是，GBDT的每次迭代目标是减小之前模型的残差，在残差减小的方向（负梯度方向）建立一个新的模型。</p>

<p>例如：</p>

<blockquote>
  <p>假设样本$[(1,2,3), 4]$，其中$(1,2,3)$是样本特征，$4$是样本标签</p>

  <p>假设模型$m_i$在上述样本上的预测结果是$3.6$</p>

  <p>则模型$m_{i+1}$的拟合目标就变为：$[(1,2,3), 0.4]$</p>
</blockquote>

<p>按上述方式得到一系列基模型后，GBDT的预测值就是样本在所有基模型上的结果的<strong>加和</strong>。</p>

<p>在回归任务中，每一次迭代中对每一个样本都有一个预测值，损失函数使用MSE（均方误差损失）：$l(y_i,\hat{y_i})=\frac{1}{2}(y_i-\hat{y_i})^2$，对损失计算梯度取负：$-[\frac{\partial l(y_i,\hat{y_i})}{\partial \hat{y_i}}]=(y_i-\hat{y_i})$。</p>

<p>由上述结果可知，当损失函数使用MSE时，每一次拟合的值就是基模型的真实值减预测值，也即“残差”。</p>

<h2 id="三xgboost">三、XGBoost</h2>

<p>XGBoost是一个高效、灵活、可移植的分布式梯度提升工具包。是一种基于梯度提升框架的并行化的提升树实现方案。</p>

<p>XGBoost的思想是，不断地在训练过程中添加树，利用特征分裂来生长一棵树，添加树的过程就是拟合一个新函数的过程，这个新函数的拟合对象是上一次预测结果的残差。</p>

\[\hat{y}=\phi(x_i)=\sum_\limits{k=1}^Kf_k(x_i)\]

<p>训练完成后得到$K$棵树，针对一个样本，该样本的特征在$K$棵树上会落到$K$个叶子结点上，得到$K$个预测分数，将这些分数相加，就能得到该样本的预测值。</p>

<h3 id="31-xgboost的目标函数和误差">3.1 XGBoost的目标函数和误差</h3>

<p>将XGBoost的误差可以简写如下：</p>

\[L(\phi)=\sum_\limits{i}l(\hat{y_i}-y_i)+\sum_\limits{t}\Omega(f_t)\]

<p>损失函数分为两部分，第一部分为误差函数，误差函数反映了当前模型对数据的拟合程度，其中$y_i$表示第$i$个样本的真实值，$\hat{y_i}$是所有基模型的输出累加；第二部分为正则化项，正则化项定义了模型的复杂度。</p>

<h4 id="311-误差函数">3.1.1 误差函数</h4>

<p>训练过程中，第$t$轮的模型预测值$\hat{y}^{(t)}=\hat{y}^{(t-1)}+\epsilon f_t(x_i)$，其中$\epsilon$为缩减因子（shrinkage），是为了防止过拟合而设置的，一般取$0.1$（为简单起见，以下推导略去$\epsilon$）。</p>

<p>误差函数可以记为：$l(y_i,\hat{y}^{(t-1)},f_t(x_i))$，第$t$轮训练的目标是学习函数$f_t$，至此，XGBoost的目标函数$Obj^{(t)}$可以写为：</p>

\[Obj^{(t)}=\sum\limits_{i=1}^n(l(y_i,\hat{y}^{(t-1)},f_t(x_i))   +  \Omega(f_t))   +constant\]

<p>上面提到，当损失函数$l(\cdot)$为MSE时，我们可以推导出，损失函数的负梯度方向即是残差，那么，当损失函数是其他形式的函数呢？</p>

<p>XGBoost提出，对于其他形式的损失函数，则使用一个二次函数来近似，这里使用泰勒展开的方式。</p>

<blockquote>
  <p>回顾泰勒定理：</p>

  <p>设有在$a$点所在区间上$n+1$阶可导的函数$f(x)$，则对这个区间上的任意$x$，都有如下泰勒展开形式：</p>

\[f(x)=f(a)+\frac{f'(a)}{1!}(x-a) +    \frac{f^{(2)}(a)}{2!}(x-a)^2+\cdot \cdot \cdot +   \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)\]

  <p>其中，$R_n(x)$为余项，是$(x-a)^n$的高阶无穷小。</p>
</blockquote>

<p>在XGBoost中，对损失函数进行了二阶泰勒展开：</p>

\[f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}\]

<p>定义$g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right), \quad h_{i}=\partial_{\hat{y}^{(t-1)}}^{2}l\left(y_{i}, \hat{y}^{(t-1)}\right)$</p>

<p>将$l(y_i,c+f_t(x_i))$中的$\hat{y_i}^{(t-1)}$看作二阶泰勒展开式中的$x$；将$f_t(x_i)$看作展开式中的$\Delta x$，则目标函数可以近似写为：</p>

\[Obj^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text { constant }\]

<p>在上式中，$l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)$为样本的真实标签和第$t-1$次预测的残差，是一个已知的数值，不影响第$t$轮的优化，常数项也可以移除，因此，目标函数可以重写为：</p>

\[Obj^{(t)}=\sum_\limits{i=1}^{n}[g_{i} f_{t(x_{i})}+\frac{1}{2} h_{i} f_{t}^{2}(x_{i})]+\Omega (f_{t})\]

<p>其中，$g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right), \quad h_{i}=\partial_{\hat{y}^{(t-1)}}^{2}l\left(y_{i}, \hat{y}^{(t-1)}\right)$。</p>

<p>可以看出，目标函数仅依赖每个样本在误差函数上的一阶导数$g$和二阶导数$h$，这里也提示了XGBoost与GBDT在目标函数的一个差别：XGBoost考虑了误差函数的二阶导数。</p>

<h4 id="312-正则化项">3.1.2 正则化项</h4>

<p>在树模型中，正则化项的加入一般是为了控制树的规模。XGBoost目标函数的正则化项的定义如下：</p>

\[\Omega(f)=\gamma T+\frac{1}{2} \lambda\Vert w\Vert^2\]

<p>其中，$T$表示叶子结点的个数，$w$表示叶子结点的分数，将二者作为惩罚项时，即要求在训练过程中，生成的树的叶子结点尽可能少，且叶子结点的数值不会过大，从而达到防止过拟合的目的。</p>

<h4 id="313-xgboost的目标函数">3.1.3 XGBoost的目标函数</h4>

<p>回顾上面得到的目标函数的表达式：</p>

\[Obj^{(t)}=\sum_\limits{i=1}^{n}[g_{i} f_{t(x_{i})}+\frac{1}{2} h_{i} f_{t}^{2}(x_{i})]+\Omega (f_{t})\]

<p>其中$f_t(x)$可以写为：</p>

\[f_t(x)=w_{q(x)},\quad w\in\mathrm{R}^T,q:\mathrm{R}^d \rightarrow\{1,2,...,T\}\]

<p>$w$表示树的叶子结点的权重，$q$表示树的结构，给定一个输入，结构函数将其<strong>映射</strong>到叶子结点的索引，再通过权重向量$w$（可以看到权重向量的维度为叶子结点的个数），得到对应的叶子结点的分数。</p>

<p>由以上定义，可以将上述目标函数修改为：</p>

\[\begin{aligned} Obj^{(t)}
&amp; \simeq \sum_\limits{i=1}^n[g_i f_t(x_i)+\frac{1}{2}h_i f_t^2(x_i)] + \Omega(f_t) \\
&amp;=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T
\end{aligned}\]

<p>其中$I_j$定义为每个叶子节点$j$上面样本索引的集合：$I_{j}={i \mid q(x_{i})=j}$，值得注意的是，这一步的目标函数的转换有两种累加形式，第一个是样本角度的累加，通过引入$f_t(x)=w_{q(x)}$后，转化为叶子结点个数的累加。（回忆前述定义的树结构映射$q$，其能够将样本$x_i$映射到树的一个叶子结点上。）</p>

<p>在上述目标函数上，定义$G_j=\sum_\limits{i \in I_{j}} g_{i}$，$H_j=\sum_\limits{i \in I_{j}} h_{i}$，将目标函数简写为：</p>

\[Obj^{(t)}=\sum_\limits{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2]+\gamma T\]

<p>这是一个二次函数的最优化问题，要计算$w_j$，可以将目标函数对$w_j$求偏导后另为$0$，可得：</p>

\[w_j^*=-\frac{G_j}{H_j+\lambda}\]

<p>将最优$w_j$代回目标函数表达式，有：</p>

\[Obj=-\frac{1}{2} \sum_\limits{j=1}^{T} \frac{G_{j}^{2}}{H_{j}+\lambda}+\gamma T\]

<h3 id="32-xgboost节点分裂">3.2 XGBoost节点分裂</h3>

<p>前面我们定义了XGBoost的目标函数$Obj$，它描述了在给定树结构的情况下，目标函数所能减少的最大幅度。我们可以将其称为“结构分数（structure score）”，基于此，接下来讨论一下XGBoost在建树过程中的一些细节。</p>

<h4 id="321-xgboost节点分裂方式">3.2.1 XGBoost节点分裂方式</h4>

<p>在树模型中，一棵树是由节点一分为二、生成的子节点继续分裂后逐渐形成的。那么XGBoost中节点的分裂方式有哪些呢？</p>

<p><strong>1）贪心地枚举所有树的结构</strong></p>

<p>由于上文可知，在确定了一个树的结构后，我们便可以计算出该结构下<strong>最好的</strong>分数。那么一个直观的想法是，枚举不同的树结构，根据打分函数确定一个具有最优结构的树，并将其加入模型当中。但这个过程的可能性几乎是无穷多种，我们可以以贪心的方式来构建，比如，从根节点开始，遍历所有的特征，针对每一个特征，**先对该特征按照取值排序 **，然后进行线性扫描，获得最好的分割点，计算由这个特征加入所带来的增益（Gain），然后选取增益最大的特征作为当前节点的最终分裂特征。增益的计算方式为：</p>

\[\text { Gain }=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]-\gamma\]

<p>表示分类后左子树的目标函数加上右子树的目标函数减去不分割时的分数，再减去加入新的叶子结点所带来的cost。</p>

<p>上述描述中有两点需要注意：</p>

<ol>
  <li>
    <p>对于一个特征，按照其取值进行排序，线性扫描计算增益</p>

    <p>这里举一个例子，假设对年龄这个特征，我们可以按照年龄的大小对样本进行排序。假设有五个人$A,B,C,D,E$，则划分方式有以下四种（使用“$\mid$”符号表示划分）：</p>

    <ul>
      <li>$A\mid B,C,D,E$</li>
      <li>$A,B\mid C,D,E$</li>
      <li>$A,B,C\mid  D,E$</li>
      <li>$A,B,C,D\mid E$</li>
    </ul>

    <p>每一种划分均将样本分成两个子集，针对每个子集，按照增益的计算方式，计算出对应的分数即可。</p>
  </li>
  <li>
    <p>引入分割不一定使情况变得更好</p>

    <p>在一些情况下，可能出现分割后的增益太小的情况，这种分割会增大模型的复杂度，得不偿失，针对这种情况，XGBoost增加了有关叶子结点的惩罚项$\gamma$，即当分割所带来的收益小于$\gamma$时，取消此次分割，这里相当于对树做了预剪枝。</p>
  </li>
</ol>

<p><strong>2）近似算法</strong></p>

<p>当数据太大，无法进行直接计算时，采用近似算法【TODO】</p>

<h4 id="322-停止分裂条件">3.2.2 停止分裂条件</h4>

<p>停止分裂条件定义了迭代算法的终止条件，一般地，XGBoost设置的停止分裂条件有如下几种：</p>

<ol>
  <li>当引入的分裂带来的增益小于给定阈值的时候，取消此次分裂，这里相当于预剪枝</li>
  <li>设置一个树的最大深度参数max_depth，当分裂进行到此深度时则停止分裂，防止过拟合</li>
  <li>当样本权重小于给定阈值时，停止建树，设置一个参数min_child_weight，当一个叶子结点包含的样本数量过少时，停止分裂</li>
  <li>树的最大数量</li>
</ol>

<h2 id="参考">参考</h2>

<ol>
  <li>统计学习方法</li>
  <li>https://blog.csdn.net/v_JULY_v/article/details/81410574</li>
</ol>


		

    </div>
    <footer class="c-article__footer">
        <p>
        
            <span class="c-tag">Machine Learning</span>
        
            <span class="c-tag">Boosting</span>
        
        </p>
    </footer>
</article>

		
			

		
    	</div>
    	<footer class="c-page__footer">
	<p>
    	
		<script type='text/javascript'>
			fortune = new Array(18);
			fortune[0] = '这座城市的中央计算机告诉你的？R2D2，你不该相信一台陌生的计算机！';
			fortune[1] = '不要信赖那些大到不能扔出窗外的计算机！';
			fortune[2] = '我终于明白‘向上兼容性’是怎么回事了。这是指我们得保留所有原有错误。';
			fortune[3] = '2038年1月19日，凌晨3点14分07秒';
			fortune[4] = '计算机就跟比基尼一样，省去了人们许多的胡思乱想。';
			fortune[5] = '我们是微软。反抗是徒劳的。你会被同化的。';
			fortune[6] = '控制复杂性是计算机编程的本质。';
			fortune[7] = '有个老套的故事说有人希望他的计算机能像他的电话机一样好用。他的愿望实现了，因为我已经不知道该如何使用自己的电话了。';
			fortune[8] = '你们当中很多人都知道程序员的美德。当然啦，有三种：那就是懒惰、急躁以及傲慢。';
            fortune[9] = '就算它工作不正常也别担心。如果一切正常，你早该失业了。'
            fortune[10] = '先解决问题再写代码。'
            fortune[11] = '我想微软之所以把它叫做.Net，是因为这样它就不会在Unix的目录里显示出来了。'
            fortune[12] = '好代码本身就是最好的文档。'
            fortune[13] = '前面90%的代码要占用开发时间的前90%。剩下的10%的代码要占用开发时间的另一90%。'
            fortune[14] = '我认为全球市场约需5台计算机。'
            fortune[15] = '长此以往，除了按键的手指外，人类的肢体将全部退化。'
            fortune[16] = 'That’s what’s cool about working with computers.  They don’t argue, they remember everything, and they don’t drink all your beer.'
            fortune[17] = 'The function of good software is to make the complex appear to be simple.'
			index = Math.floor(Math.random() * fortune.length);
			document.write(fortune[index]);
		</script>
	

    </p>
	
	<hr><br/>
	
    <br/>
	<p>&copy; Shihanmax 2021 | Super powered by Jekyll </p>
	
	<br/>
	
	<!--p>Follow me on: <a href="https://twitter.com/YourTwitterNo">Twitter</a> & <a href="https://github.com/YourGitHubNo">Github</a><span class="u-separate"></span> Subscribe: <a href="//feed.xml">RSS</a></p>
        
</footer>

</article>

        </main>
    </body>
</html>
