<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>集成学习小结 - Shihanmax</title>
    <meta name="description" content="集成学习主要有两个思路，Bagging和Boosting。">

    <link href="//fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin-ext,vietnamese"rel="stylesheet">
    <link rel="stylesheet" href="//css/main.css">
    <link rel="canonical" href="http://localhost:4000//2021/02/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%93">
    <link rel="alternate" type="application/rss+xml" title="Shihanmax" href="http://localhost:4000//feed.xml">
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
    

    

</head>

    <body>
        <main class="u-container">
        <div class="c-page">
		<article class="c-article">
   	 	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<header class="c-page__header">
        <h1><span class="half_background">Shihanmax's blog</span> 🚀</h1>
	
    <p>
        <a href="//">&lt; Back</a>
    </p>
	
</header>

    	<div class="c-article__main">
        <article class="c-article">
    <header class="c-article__header">
        <h1 class="c-article__title">集成学习小结</h1>
        <p class="c-article__time"><time datetime="2021-02-27T17:21:09+08:00" itemprop="datePublished">Feb 27, 2021</time></p>
    </header>
    <div class="c-article__main">
        <p>集成学习主要有两个思路，Bagging和Boosting。</p>

<ul>
  <li>
    <p>Bagging</p>

    <p>独立训练（可并行）多个基分类器，使用投票法或者平均法集成所有基分类器的结果。</p>

    <p>bagging的目标是为了减小方差。</p>

    <p>典型算法：RandomForest</p>
  </li>
  <li>
    <p>Boosting</p>

    <p>为每个样本赋一个相同的权重，在一次迭代中，一个基模型对样本的估计有对有错，对于分错的样本，增加其权重，分对的样本，则减少其权重。在进行N次迭代后，得到N个简单的分类器（basic learner），将这些分类器通过权重进行组合，即是boosting模型（方法 ）。</p>

    <p>典型算法：Adaboost、GBDT、XGBoost、LightGBM、CatBoost</p>
  </li>
</ul>

<p>下文对上述Boosting和Bagging典型算法的思想、实现方式和优缺点进行总结对比。</p>

<h2 id="randomforest">RandomForest</h2>

<p>RF是bagging的扩展，以CART为基学习器，学习过程为：</p>

<ol>
  <li>
    <p>随机选择样本（有放回抽样）</p>

    <p>随机选择样本是bagging的特点</p>
  </li>
  <li>
    <p><strong>随机选择特征</strong></p>

    <p>仅选择$\sqrt{n}$个特征进行决策树的生成（会一定程度上增大偏差），多棵决策树会使得RF的方差降低</p>
  </li>
  <li>
    <p>构建决策树</p>

    <p>构建决策树时，每棵决策树都最大可能的进行生成而不进行剪枝。</p>

    <p>多个决策树投票/平均：</p>

    <ul>
      <li>分类：简单投票法</li>
      <li>回归：简单平均</li>
    </ul>
  </li>
</ol>

<p>优点：</p>

<ul>
  <li>模型方差小，预测准确性高</li>
  <li>树构建可并行化，无需剪枝，大数据集、特征数量大时依然适用</li>
  <li>无需特征选择，可以给出特征重要性</li>
</ul>

<p>缺点：</p>

<ul>
  <li>容易过拟合</li>
  <li>取值划分较多的属性对RF的影响更大，这种属性权重并不可靠</li>
</ul>

<p>RF的重要特性是<strong>不用对其进行交叉验证</strong>或者使用一个独立的测试集获得无偏估计，RF在生成的过程中可以对误差进行无偏估计，由于每个基学习器仅使用了训练集中约<strong>63.2%</strong>的样本，剩下约<strong>36.8%</strong>的样本可用做验证集来对模型的泛化能力进行袋外估计。</p>

<h2 id="adaboost">Adaboost</h2>

<p>Adaboost的学习过程：</p>

<ol>
  <li>初始化样本权重（$N$个样本：每个样本权重$\frac{1}{N}$），抽样得到一批样本，训练第一个基分类器</li>
  <li>计算上一个基分类器的错误率，记为$\epsilon$，通过其计算出基分类器的权重：$\alpha=\frac{1}{2}\ln\frac{1-\epsilon}{\epsilon}$</li>
  <li>根据基分类器的分类结果调整样本的权重，正确分类的样本降低权重，错误分类的增加权重。（权重越大，表示分类难度越大）（通过此种方式，使得后续的基分类器更关注分错的样本）</li>
  <li>循环执行2～3步，直至误差小于某一个阈值或分类器个数达到上限</li>
</ol>

<p>优点：</p>

<ul>
  <li>对弱分类器进行级联，提高模型精度</li>
  <li>可以使用多样的基分类器</li>
</ul>

<p>缺点：</p>

<ul>
  <li>基分类器数量不好确定（需要通过交叉验证确定）</li>
  <li>对不平衡数据的分类效果不佳</li>
  <li>训练比较耗时</li>
</ul>

<h2 id="gbdtgradient-boost-decision-tree">GBDT（Gradient Boost Decision Tree）</h2>

<p>一般的boosting算法关注每一轮迭代中的正/误样本的权重，而GBDT的每一次迭代都是为了减少上一次迭代的<strong>残差</strong>（通过在残差减小的梯度方向上建立模型），即利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART<strong>回归树</strong>，GBDT会累加所有（回归）树的结果。</p>

<p>GBDT的执行流程：</p>

<ol>
  <li>使用训练样本训练第一个弱分类器，并将模型结果与真实结果进行比较，得到模型的残差</li>
  <li>使用上一步骤中得到的残差训练下一个分类器，再次比较结果，计算残差</li>
  <li>循环执行1～2步，直至残差小于某一个阈值或分类器个数达到上限</li>
</ol>

<p>优点：</p>

<ul>
  <li>可以处理各种类型的数据</li>
  <li>调参时间较少的情况下能够获得较高的准确度、泛化能力强</li>
</ul>

<p>缺点：</p>

<ul>
  <li>高维稀疏数据集上的表现不如支持向量机和神经网络、文本特征的处理能力弱于数值特征</li>
  <li>基于boosting思想，需要串行训练，只能在决策树内部进行一些局部的并行化手段</li>
</ul>

<h2 id="xgboost">XGBoost</h2>

<p>XGBoost基于GBDT，可以处理分类任务，也可以处理回归任务。与GBDT的最大区别是，XGBoost对目标函数进行二阶泰勒展开，从而求出下一步需要拟合的树的叶子结点的权重。</p>

<p>XGBoost与GBDT的区别：</p>

<ol>
  <li>GBDT是机器学习算法，而XGBoost是GBDT的一种实现</li>
  <li>在使用CART作为基分类器时，XGB加入了正则项（树的叶子结点个数、每个叶子结点上输出的分数的L2范数的平方和）来控制模型的复杂度，防止过拟合</li>
  <li>GBDT训练时仅使用了代价函数的一阶导数，XGB使用了一阶和二阶导数（xgboost支持自定义代价函数（二阶可导即可））</li>
  <li>在XGB中有一个Shrinkage操作，对应到XGB实现中的eta参数，在进行一轮迭代后，将叶子结点上的权重乘以该系数，来削弱当前基分类器的影响，增大后续的学习空间（实际操作中，eta设置的小一点，迭代次数可以稍微设置大一些）</li>
  <li>XGB使用列抽样技术（column sampling），能够在减少计算的同时，减弱过拟合倾向</li>
  <li>GBDT对缺失值没有处理，XGB能够自动学习缺失值的处理方法</li>
  <li>XGB支持决策树构建级别的并行化，在决策树学习过程中，确定最优分割点时，需要对特征的取值进行排序，XGB在训练之前对数据作了预排序并保存为block结构，在后续的训练过程中复用此结构，另外，在进行结点分裂时，计算各个特征的增益也可以使用多线程并行进行</li>
  <li>GBDT使用CART作为基分类器，XGB支持多种类型的基分类器</li>
  <li>GBDT每次迭代使用全部数据，XGB使用自助法抽样数据</li>
</ol>

<p>优点：</p>

<ol>
  <li>计算效率高，使用二阶导数</li>
  <li>缺点：每次迭代需要遍历整个数据集，内存占用大</li>
</ol>

<h2 id="lightgbm">LightGBM</h2>

<p>LGB是一个实现了GBDT算法的高效的分布式框架。</p>

<p>降低训练复杂度可以从两个角度入手（在尽可能保证精度的前提下）：减少特征、减少数据量，在LightGBM中，对应的手段是GOSS和EFB。</p>

<ul>
  <li>
    <p>GOSS（Gradient-based One-side Sampling）</p>

    <p>训练过程中，每个数据实例拥有不同的梯度，梯度大的实例对信息增益的影响更大，因此，在下采样时应尽可能保留梯度大的样本（通过阈值或百分比来限制），随机去掉梯度小的样本。这个技术能够获得比随机采样更精确的的结果。</p>
  </li>
  <li>
    <p>EFB（Exclusive Feature Bundling）</p>

    <p>在实际应用中，特征之间可能存在着互斥的情况，EFB对互斥特征进行绑定，将绑定问题约束到图着色问题，通过贪心算法求近似解，通过此种方式减少了特征的数量。</p>
  </li>
</ul>

<p>LightGBM与XGBoost的区别：</p>

<ol>
  <li>内存方面：在计算分类特征的增益时，XGB使用预排序法（pre-sorted）处理节点分类（比较精确，但时间开销比较大）；LGB使用基于histogram的决策树算法，节省了内存占用（～$\frac{1}{8} \cdot$pre-sorted算法）</li>
  <li>效率方面：决策树算法的主要操作有两个：分割点寻找和数据分割，在第一个操作上，pre-sorted和histogram算法的时间复杂度是一样的，在数据分割操作上，histogram比pre-sorted要快（histogram中，所有特征共享一个索引，而pre-sorted中，每个特征对应一个索引），此外，histogram还减少了计算分割点增益的次数</li>
  <li>通信方面：histogram的通信代价远小于pre-sorted，适用于分布式计算</li>
  <li>XGB的生成策略是level-wise（不容易过拟合，但效率低，在分裂时，部分增益小的树也进行了增长），LGB使用leaf-wise（高效，但容易生成深度过大的树，从而导致过拟合，因此需要限制最大深度）</li>
  <li>训练精度方面：histogram由于无法找到精确的分割点，与pre-sorted相比，相当于通过牺牲精度来换取效率</li>
</ol>

<h2 id="catboostcategory-boosting">CatBoost（Category Boosting）</h2>

<p>CatBoost是以对称决策树为基学习器的GBDT框架，能够高效地处理类别型特征，此外，CatBoost还解决了梯度偏差（Graident Bias）和预测偏移（Prediction Shift）的问题，能够减弱过拟合现象，提高预测准确性。</p>

<p>CatBoost的学习算法基于GPU实现，打分算法基于CPU实现。</p>

<p>CatBoost相对于XGB和LGB的改进：</p>

<ol>
  <li>能够自动采用特殊方式处理类别型特征</li>
  <li>对类别特征进行组合，丰富了特征种类</li>
  <li>基模型采用对称树</li>
  <li>使用排序提升的方法对抗训练集中的噪声点，从而避免梯度估计的偏差，解决了预测偏移的问题</li>
</ol>

<h3 id="catboost如何处理类别特征categorical-features">CatBoost如何处理类别特征（categorical features）</h3>

<ol>
  <li>对数据做一些统计，计算某个category出现的频率，再加上一些超参数，生成新的数值型特征</li>
  <li>对数据进行若干种排列，在每一轮建树之前，随机选择一种数据排列来生成树</li>
  <li>对category特征进行组合，在组合数量过大时，CatBoost通过贪心算法选择来选择最好的一部分组合</li>
</ol>

<p>优点：</p>

<ul>
  <li>性能强，可以匹敌任何先进的机器学习算法</li>
  <li>鲁棒性高，减少了超参数调整的需求，降低了过拟合的风险，模型的泛用性更强</li>
  <li>实用性强：体现在可以自动处理数值特征和类别特征</li>
  <li>可扩展：支持自定义loss函数</li>
</ul>

<p>缺点：</p>

<ul>
  <li>
    <p>对类别特征的处理对时间和空间的需求比较大</p>
  </li>
  <li>
    <p>模型随机种子的设定会影响模型的预测结果</p>
  </li>
</ul>

<h2 id="refs">Refs.</h2>

<ol>
  <li>
    <p><a href="https://marian5211.github.io/2018/03/12/">gbdt-xgboost-lightGBM比较</a></p>
  </li>
  <li><a href="https://zhuanlan.zhihu.com/p/129810143">集成算法</a></li>
  <li><a href="https://arxiv.org/pdf/1603.02754.pdf">XGBoost: A Scalable Tree Boosting System</a></li>
  <li><a href="https://www.msra.cn/zh-cn/news/features/lightgbm-20170105">MSRA: LighGBM介绍</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/102540344">深入理解CatBoost</a></li>
  <li><a href="http://www.audentia-gestion.fr/MICROSOFT/lightgbm.pdf">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</a></li>
  <li><a href="https://arxiv.org/pdf/1706.09516.pdf">CatBoost: unbiased boosting with categorical features</a></li>
  <li><a href="https://www.kaggle.com/abhinand05/catboost-a-deeper-dive">Catboost: A Deeper Dive</a></li>
  <li><a href="https://zhuanlan.zhihu.com/p/102570430">Catboost完全指南</a></li>
</ol>


		

    </div>
    <footer class="c-article__footer">
        <p>
        
            <span class="c-tag">集成学习</span>
        
            <span class="c-tag">Machine Learning</span>
        
        </p>
    </footer>
</article>

		
			

		
    	</div>
    	<footer class="c-page__footer">
	<p>
    	
		<script type='text/javascript'>
			fortune = new Array(18);
			fortune[0] = '这座城市的中央计算机告诉你的？R2D2，你不该相信一台陌生的计算机！';
			fortune[1] = '不要信赖那些大到不能扔出窗外的计算机！';
			fortune[2] = '我终于明白‘向上兼容性’是怎么回事了。这是指我们得保留所有原有错误。';
			fortune[3] = '2038年1月19日，凌晨3点14分07秒';
			fortune[4] = '计算机就跟比基尼一样，省去了人们许多的胡思乱想。';
			fortune[5] = '我们是微软。反抗是徒劳的。你会被同化的。';
			fortune[6] = '控制复杂性是计算机编程的本质。';
			fortune[7] = '有个老套的故事说有人希望他的计算机能像他的电话机一样好用。他的愿望实现了，因为我已经不知道该如何使用自己的电话了。';
			fortune[8] = '你们当中很多人都知道程序员的美德。当然啦，有三种：那就是懒惰、急躁以及傲慢。';
            fortune[9] = '就算它工作不正常也别担心。如果一切正常，你早该失业了。'
            fortune[10] = '先解决问题再写代码。'
            fortune[11] = '我想微软之所以把它叫做.Net，是因为这样它就不会在Unix的目录里显示出来了。'
            fortune[12] = '好代码本身就是最好的文档。'
            fortune[13] = '前面90%的代码要占用开发时间的前90%。剩下的10%的代码要占用开发时间的另一90%。'
            fortune[14] = '我认为全球市场约需5台计算机。'
            fortune[15] = '长此以往，除了按键的手指外，人类的肢体将全部退化。'
            fortune[16] = 'That’s what’s cool about working with computers.  They don’t argue, they remember everything, and they don’t drink all your beer.'
            fortune[17] = 'The function of good software is to make the complex appear to be simple.'
			index = Math.floor(Math.random() * fortune.length);
			document.write(fortune[index]);
		</script>
	

    </p>
	
	<hr><br/>
	
    <br/>
	<p>&copy; Shihanmax 2021 | Super powered by Jekyll </p>
	
	<br/>
	
	<!--p>Follow me on: <a href="https://twitter.com/YourTwitterNo">Twitter</a> & <a href="https://github.com/YourGitHubNo">Github</a><span class="u-separate"></span> Subscribe: <a href="//feed.xml">RSS</a></p>
        
</footer>

</article>

        </main>
    </body>
</html>
