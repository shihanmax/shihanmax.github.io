---
title:  "正则化"
layout: post
date: 2020-08-04 21:34:18
categories: NLP
tags: 正则化
syntaxHighlighter: yes

Mathjax: true
---

在机器学习中，模型的训练过程即是在寻找一个**足够好**的函数$F^{\star}$，使得$F^{\star}$在训练数据和未来的新数据上都具有良好的推理效果。为了从候选函数空间$\{F\}$中选择“好”的模型，人们引入损失函数的概念。一般地，对于样本$(x, y)$和模型$F$，假设模型对样本的预测值为$\hat{y}$，则损失函数被定义在$\mathbb{R}$的函数$l(y, \hat{y})$，用于描述预测值和真值的差距。

一般地，损失函数是一个有下确界的函数，这样，机器学习的优化过程即转化为了在数据集上的损失函数最小化问题。到目前为止，损失函数仅考虑了在训练数据上的经验风险，仅考虑经验风险，很有可能导致参数空间过于复杂，造成模型对训练数据过拟合。

相对训练数据量，当模型的复杂程度过高时，就会发生过拟合现象，减轻过拟合现象的措施有：

- 使用更简单的模型
- 减少参数
- 在搜索参数空间时加入限制（如对参数增加正则化）
- 获取更多的样本

本文仅讨论使用正则化手段减轻过拟合的影响。

## 模型复杂度

过拟合现象、泛化能力和模型复杂度三者之间联系紧密，模型复杂度取决于：

- 模型本身的选择
- 模型参数个数
- 模型的参数空间选择（当模型和参数个数都确定时，仍可以使用一些手段（如正则化），从参数空间中选择较优的参数）

## 引入正则化

为了减轻模型的过拟合趋势，我们需要对损失函数中加入描述模型复杂程度的正则项$\Omega(F)$，优化过程转化为：

$$F^\star:=\mathop{\arg\min}_\limits{\theta} Obj(F)=\mathop{\arg\min}_\limits{\theta}(L(F)+\gamma\Omega(F)),\gamma>0$$

目标函数$Obj(\cdot)$描述模型的结构风险，其中$L(f)$表示模型在训练数据上的损失，$\Omega(F)$为正则化项，$\gamma$用于控制正则化项的强度，一般使用$L_p$范数作为正则项，用于对参数光滑度和参数空间范数上界进行限制，提升模型的泛化能力。

常见的用于正则化的$L_p$范数有以下几种：

- $L_0: \Vert W \Vert_0=\sum\limits^{d}_{i=1}I(w_i\neq 0)$

- $L_1: \Vert W \Vert_1=\sum\limits^{d}_{i=1}\vert w_i \vert$
- $L_2: \Vert W \Vert_2=\sum\limits^{d}_{i=1} w_i^2$

常用$L_1$和$L_2$正则化来限制模型的复杂度。

## $L_1$和$L_2$正则化

两种正则化都可以使参数变“小”。不同之处在于，$L_1$正则化可以使得参数稀疏化，可以用于特征选择；$L_2$正则化可以使得参数尺度受到约束，减轻过拟合现象。我们可以根据场景选择合适的正则化项，甚至可以同时使用二者，如stanford提出的ElasticNet就同时在模型中使用了$L_1$和$L_2$正则化，参考这篇1.2w次引用的[slide](https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf)。

为什么$L_2$正则化使得参数尺度变小之后，就能起到减弱模型过拟合的效果？

如果参数尺度很大，则对输入施加一个微小的扰动就会引起目标函数发生较大幅度的变化，也即模型对输入噪声的鲁棒性变差，过拟合风险增加。

## 简单分析为什么$L_1$能够使模型参数变得稀疏

TODO

## 正则化做了什么

在参数优化过程中加入正则化，实际上是在一个确定的参数空间中，对参数的可行空间（feasible region）作了进一步约束，因此，假设$L$为模型在训练样本上的损失函数，我们可以得到如下结论：

$$L(\hat{w}_{without\ reg}) \leq L(\hat{w}_{with\ reg})$$

即，加入正则化后的参数在训练数据上的损失，一定不小于无正则化时的。

## 正则化的灵活运用

以简化的大脑神经元模型为例，假设大脑可以分为若干个区域（region），每个区域均包含一些神经元，每个神经元均具有对应的参数，$w_{ir_j}$表示大脑第$i$个神经元区域的第$r_p$个神经元的权重。

<img src="http://shihanmax.top/20201013230655_vEOXrf_%E6%88%AA%E5%B1%8F2020-10-13%2023.06.38.jpeg" alt="neu" style="zoom:50%;" />

设目标函数为$f(W)$，我们期望通过正则化手段，将以下两个假设考虑进来：

1. 某一个区域内，仅有少量神经元被激活
2. 在空间上相近的神经元，作用类似

则我们可以构造如下优化目标：

$$\mathop{minimize}: f(W)+\sum\limits_{i=1}^P \lambda_i\Vert w
_{i \cdot}\Vert_1+\sum\limits_{i=1}^{P}\sum\limits_{j=1}^{r_p}\lambda_2\Vert w_{ij}-w_{ij-1}\Vert_2$$

在上式中，我们引入了两个正则化项，第一个用于神经元区域内的稀疏化，第二个用于约束相近神经元区域的相似程度。



## Refs

