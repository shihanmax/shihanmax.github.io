---
title:  "RNN Training Tips and Tricks"
date:   2019-01-25 00:00:00
categories: NLP
tags:  RNN
syntaxHighlighter: yes
Mathjax: true
---
本文是一些在训练RNN模型的技巧和建议，来自[Andrej Karpathy](https://github.com/karpathy/char-rnn#tips-and-tricks).

## 监视训练集和验证集损失

机器学习、神经网络方面的新手可能需要掌握更多的技巧才能训练出一个好的模型。首要关注的是训练集损失（训练时输出）和验证集损失（指定迭代次数后将模型在验证集上跑一遍，得到验证集损失），特别地：

- 如果训练集损失比验证集损失低很多，这意味着我们的模型可能过拟合了，解决办法是，减小网络的规模，或者增大dropout率，比如调整到0.5
- 如果训练集/测试集误差处在同一个水平上，模型很有可能欠拟合，此时需要增大网络规模（网络层数或者每层的神经元个数）

<!--more-->

## 参数预估

LSTM模型的两个重要参数是lstm_size和num_layers，我建议将num_layers设置为2或者3，lstm_size可以基于训练数据量来调整，你需要关注两个数：

- 模型一共包含多少参数
- 数据规模，1MB的数据大约包含一百万个字符

这两者应该有着相同的数量集，可能不太好描述，下面是几个例子：

- 如果我的数据集是100MB的，我使用默认的参数设置（包含150k个参数），由于我的数据集数量远大于参数数量（100百万>>0.15百万），模型很有可能欠拟合，这个时候，可以放心地将lstm_size调大一些
- 如果我有10MB的数据，而模型拥有一千万个参数，这时候我会密切关注验证集损失，如果它比训练集损失大，我会将dropout调大一些，这样有助于降低验证集损失

## 最好的模型策略

寻找最优模型参数的“制胜法宝“是，在我们可以忍受的训练时间下，尽可能在更大的网络上试错，并且在(0, 1)之间调整dropout，在验证集上拥有最好的性能的模型，就是我们最终想要的。

通过调整不同的超参数，运行多个模型在深度学习领域十分普遍，最终我们要选择在验证集上表现最好的那个。

另外，训练集/测试集分割比率也是个超参数，要保证验证集包含足够的数据，否则，验证集性能可能会因为有噪声而不能反映出模型真实的信息。
