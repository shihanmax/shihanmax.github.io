---
title:  "XGBoost理解"
layout: post
date: 2021-03-28 12:52:03
categories: NLP
tags:  ["Machine Learning", "Boosting"]
syntaxHighlighter: yes
Mathjax: true
---

XGBoost（eXtreme Gradient Boosting）是GBDT的一种高效的工程实现，在数据竞赛中应用十分广泛。

下文从决策树的定义开始，介绍决策树的分裂指标、决策树在boosting算法中的应用BDT、引入残差信息的Gradient boosting算法等，最后，介绍集成了多种高效算法的XGBoost的工程实现。

## 一、决策树

决策树是一种描述对实例进行分类的属性结构，由节点和有向边组成，其中，节点分为内部节点和叶子结点，内部节点表示特征或属性，叶子结点表示一个类。

从根节点开始，对样本的某一个特征进行测试，根据结果，将样本划分到其若干个子节点中，其中每个子节点均对应该特征的一个取值；针对每一个子节点，继续选取一个特征重复执行上述过程，直到将实例划分到叶节点所对应的类别当中。

在上述描述过程中，有一个重要的点是，如何选择一个最好的特征，并使用其进行样本的划分？

直观上考虑，如果划分后的子集拥有更高的“纯度”，则表示该特征是一个好的特征。所谓纯度是指，划分后的子样本集合所对应的类别应尽可能一致，极端考虑，假设我们选择了一个特征，在使用此特征进行划分后，所有的子集合中，样本的类别几乎一致，则可以认为这个特征是一个非常强的特征。

在决策树实现过程中，一般使用三个指标衡量分类纯度：信息增益、信息增益比、基尼指数。首先回顾一下信息论中几个基础的概念：

**熵（Entropy）**

设$X$是随机变量，$P(X=x_i)=p_i,i=1,2,...,n$，则$X$的熵定义为：

$$H(X)=-\sum_\limits{i=1}^{n}{p_i}\log{p_i}$$

可以看出，$X$的熵的取值仅与$X$的分布有关，与$X$的取值无关，所以这里$H(X)$也可以写作$H(p)$。上式中的对数以$2$为底时，熵的单位为$bit$；以$e$为底时对应的单位为$nat$。

熵可以用来衡量随机变量的不确定性。

 

**条件熵**

条件熵$H(Y\mid X)=\sum_\limits{i=1}^{n}{p_i}H(Y\mid X=x_i)$，其中，$p_i=P(X=x_i),i=1,2,...,n$。

条件熵$H(Y\mid X)$表示，在已知随机变量$X$时，随机变量$Y$的不确定性。



**信息增益（互信息）**

信息增益定义为：$MI(Y,X)=H(Y)-H(Y\mid X)$，表示“得知$X$的信息，而使$X$的**不确定性**减少的程度”。

在决策树的特征选择过程中，使用特征$A$对数据集$D$进行分割，所带来的信息增益可以表示为：$g(D,A)=MI(D,A)=H(D)-H(D\mid A)$



**信息增益比**

有以上定义可知，通过计算信息增益来进行数据分割时，会偏向取值较多的特征的问题，针对此问题，使用信息增益比来进行修正：

$g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}$，其中$H_A(D)=-\sum_\limits{i=1}^{n}{\frac{\mid D_i\mid}{\mid D\mid}}\log{\frac{\mid D_i\mid}{\mid D\mid}}$，即计算数据集$D$的熵时，特征$A$的贡献。



**基尼指数Gini Index**

在分类问题中，假设有$k$个类别，样本属第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：

$$Gini(p)=\sum_\limits{k=1}^{K}p_k(1-p_k)=1-\sum_\limits{k=1}^{K}p_k^2$$

则数据集$D$的基尼指数为：

$$Gini(D)=1-\sum_\limits{k=1}^{K}\frac{\mid C_k\mid}{\mid D \mid}$$

其中，$C_k$表示第$k$类样本子集的大小。

假设有特征A，根据特征$A$是否满足特定条件，将数据集划分为两部分：$D_1,D_2$，则可以定义：给定特征$A$的条件下，样本集合$D$的基尼指数$Gini(D,A)$（即给定特征$A$后，样本集合$D$的不确定性减少的程度）：

$$Gini(D,A)=\frac{\mid D_1\mid}{\mid D \mid}Gini(D_1)+\frac{\mid D_2\mid}{\mid D \mid}Gini(D_2)$$

在决策树的构建阶段，我们便可以在进行最优特征选择时，遍历所有的特征，然后计算出其纯度指标（信息增益、信息增益比、基尼指数），然后选出一个指标最高的特征，并根据该特征的取值将样本集合分裂。

常用的决策树算法有ID3算法、C4.5算法和CART算法，其中ID3算法使用信息增益进行特征选择；C4.5算法进行了改进，使用信息增益比进行特征选择；对于分类树，CART算法使用基尼指数最小化准则，对于回归树，使用平方误差最小化准则。



## 二、Boosting

Boosting是集成学习思想的一种（另一个是Bagging，指并列地训练多个基模型，并通过投票（分类问题）或平均（回归问题）的方式计算得到最终的集成结果）。

与Bagging的可并行不同，Boosting算法的执行流程是串行的，即后面的模型的训练依赖前面的模型的预测结果。典型的算法有Adaboost、GBDT等。下面简单介绍这两类算法。

### 2.1 Adaboost

Adaboost算法（Adaptive Boosting，自适应增强）是boosting算法的代表，其特点是通过迭代训练一系列基分类器，在每次迭代过程中，提高被前一个分类器分错的样本的权重，降低被前一个分类器分对的样本的权重。最后，Adaboost算法将基分类器进行加权线性组合，每个基分类器都有一个权重，分类误差越小的基分类器，其权重越高，否则权重越低。

Adaboost的执行流程简述如下：

首先，初始化数据集为等权重。接着，依次迭代训练$M$个基模型$G_1,G_2,...,G_M$，针对模型$G_m$，执行以下四步：

1. 使用带权的数据训练$G_m$

2. 计算$G_m$在数据集上的带权分类误差率

3. 通过此误差率计算分类器的权重

4. 通过带权基模型$G_m$的预测结果，更新训练数据的权重

在$M$个基模型训练完成后，每个基模型均有一个权重，使用该权重对一系列基模型进行加权组合，即可得到最终的分类器。



### 2.2 GBDT

GBDT全称是Gradient Boost Desision Tree，即梯度提升决策树。与Adaboost不同的是，GBDT的每次迭代目标是减小之前模型的残差，在残差减小的方向（负梯度方向）建立一个新的模型。

例如：

>假设样本$[(1,2,3), 4]$，其中$(1,2,3)$是样本特征，$4$是样本标签
>
>假设模型$m_i$在上述样本上的预测结果是$3.6$
>
>则模型$m_{i+1}$的拟合目标就变为：$[(1,2,3), 0.4]$

按上述方式得到一系列基模型后，GBDT的预测值就是样本在所有基模型上的结果的**加和**。

在回归任务中，每一次迭代中对每一个样本都有一个预测值，损失函数使用MSE（均方误差损失）：$l(y_i,\hat{y_i})=\frac{1}{2}(y_i-\hat{y_i})^2$，对损失计算梯度取负：$-[\frac{\partial l(y_i,\hat{y_i})}{\partial \hat{y_i}}]=(y_i-\hat{y_i})$。

由上述结果可知，当损失函数使用MSE时，每一次拟合的值就是基模型的真实值减预测值，也即“残差”。



## 三、XGBoost

XGBoost的介绍为：XGBoost是一个高效、灵活、可移植的分布式梯度提升工具包。是一种基于梯度提升框架的并行化的提升树实现方案。

XGBoost的思想是，不断地在训练过程中添加树，利用特征分裂来生长一棵树，添加树的过程就是你和一个新函数的过程，这个新函数的拟合对象是上一次预测结果的残差。

$$\hat{y}=\phi(x_i)=\sum_\limits{k=1}^Kf_k(x_i)$$

训练完成后得到$K$棵树，针对一个样本，该样本的特征在$K$棵树上会落到$K$个叶子结点上，得到$K$个预测分数，将这些分数相加，就能得到该样本的预测值。

### 3.1 XGBoost的目标函数和误差

将XGBoost的目标函数简写如下：

$$L(\phi)=\sum_\limits{i}l(\hat{y_i}-y_i)+\sum_\limits{k}\Omega(f_k)$$

损失函数分为两部分，第一部分为误差函数，误差函数反映了当前模型对数据的拟合程度，其中$y_i$表示第$i$个样本的真实值，$\hat{y_i}$是所有基模型的输出累加；第二部分为正则化项，正则化项定义了模型的复杂度。

正则化项的定义如下：

$$\Omega(f)=\gamma T+\frac{1}{2} \lambda\Vert w\Vert^2$$

其中，$T$表示叶子结点的个数，$w$表示叶子结点的分数，将二者作为惩罚项时，即要求在训练过程中，生成的树的叶子结点尽可能少，且叶子结点的数值不会过大，从而达到防止过拟合的目的。

训练过程中，第$t$轮的模型预测值$\hat{y}^{(t)}=\hat{y}^{(t-1)}+f_t(x_i)$，误差函数可以记为：$l(y_i,\hat{y}^{(t-1)},f_t(x_i))$，第$t$轮训练的目标是学习函数$f$，至此，XGBoost的目标函数$Obj^{(t)}$可以写为：

$$Obj^{(t)}=\sum\limits_{i=1}^n(l(y_i,\hat{y}^{(t-1)},f_t(x_i))   +  \Omega(f_t))   +constant$$

上面提到，当损失函数$l(\cdot)$为MSE是，我们可以推导出，损失函数的负梯度方向即是残差，那么，当损失函数是其他形式的函数呢？

XGBoost提出，对于其他形式的损失函数，则使用一个二次函数近似，这里使用泰勒展开的方式。

>  回顾泰勒定理：
>
> 设有在$a$点所在区间上$n+1$阶可导的函数$f(x)$，则对这个区间上的任意$x$，都有如下泰勒展开形式：
>
> $$f(x)=f(a)+\frac{f'(a)}{1!}(x-a) +    \frac{f^{(2)}(a)}{2!}(x-a)^2+\cdot \cdot \cdot +   \frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x)$$
>
> 其中，$R_n(x)$为余项，是$(x-a)^n$的高阶无穷小。

在XGBoost中，对损失函数进行了二阶泰勒展开：

$$f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$$

定义$g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right), \quad h_{i}=\partial_{\hat{y}^{(t-1)}}^{2}l\left(y_{i}, \hat{y}^{(t-1)}\right)$

将$l(y_i,c+f_t(x_i))$中的$\hat{y_i}^{(t-1)}$看作二阶泰勒展开式中的$x$；将$f_t(x_i)$看作展开式中的$\Delta x$，则目标函数可以近似写为：

$$
Obj^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text { constant }
$$

在上式中，$l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)$为样本的真是标签和第$t-1$次预测的残差，是一个已知的数值，不影响第$t$轮的优化，常数项也可以移除，因此，目标函数可以重写为：
$$
\sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)
$$


其中，$g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right), \quad h_{i}=\partial_{\hat{y}^{(t-1)}}^{2}l\left(y_{i}, \hat{y}^{(t-1)}\right)$。

可以看出，目标函数仅依赖每个样本在误差函数上的一阶导数$g$和二阶导数$h$，从这里也可以看出，XGBoost与GBDT在目标函数的一个差别：XGBoost考虑了










## 参考

1. 统计学习方法
2. https://blog.csdn.net/v_JULY_v/article/details/81410574





