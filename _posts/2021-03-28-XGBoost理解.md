---
title:  "XGBoost理解"
layout: post
date: 2021-03-28 12:52:03
categories: NLP
tags:  ["Machine Learning", "Boosting"]
syntaxHighlighter: yes
Mathjax: true
---

XGBoost（eXtreme Gradient Boosting）是GBDT的一种高效的工程实现，在数据竞赛中应用十分广泛。

下文从决策树的定义开始，介绍决策树的分裂指标、决策树在boosting算法中的应用BDT、引入残差信息的Gradient boosting算法等，最后，介绍集成了多种高效算法的XGBoost的工程实现。

## 一、决策树

决策树是一种描述对实例进行分类的属性结构，由节点和有向边组成，其中，节点分为内部节点和叶子结点，内部节点表示特征或属性，叶子结点表示一个类。

从根节点开始，对样本的某一个特征进行测试，根据结果，将样本划分到其若干个子节点中，其中每个子节点均对应该特征的一个取值；针对每一个子节点，继续选取一个特征重复执行上述过程，直到将实例划分到叶节点所对应的类别当中。

在上述描述过程中，有一个重要的点是，如何选择一个最好的特征，并使用其进行样本的划分？

直观上考虑，如果划分后的子集拥有更高的“纯度”，则表示该特征是一个好的特征。所谓纯度是指，划分后的子样本集合所对应的类别应尽可能一致，极端考虑，假设我们选择了一个特征，在使用此特征进行划分后，所有的子集合中，样本的类别几乎一致，则可以认为这个特征是一个非常强的特征。

在决策树实现过程中，一般使用三个指标衡量分类纯度：信息增益、信息增益比、基尼指数。首先回顾一下信息论中几个基础的概念：

**熵（Entropy）**

设$X$是随机变量，$P(X=x_i)=p_i,i=1,2,...,n$，则$X$的熵定义为：

$$H(X)=-\sum_\limits{i=1}^{n}{p_i}\log{p_i}$$

可以看出，$X$的熵的取值仅与$X$的分布有关，与$X$的取值无关，所以这里$H(X)$也可以写作$H(p)$。上式中的对数以$2$为底时，熵的单位为$bit$；以$e$为底时对应的单位为$nat$。

熵可以用来衡量随机变量的不确定性。

 

**条件熵**

条件熵$H(Y\mid X)=\sum_\limits{i=1}^{n}{p_i}H(Y\mid X=x_i)$，其中，$p_i=P(X=x_i),i=1,2,...,n$。

条件熵$H(Y\mid X)$表示，在已知随机变量$X$时，随机变量$Y$的不确定性。



**信息增益（互信息）**

信息增益定义为：$MI(Y,X)=H(Y)-H(Y\mid X)$，表示“得知$X$的信息，而使$X$的**不确定性**减少的程度”。

在决策树的特征选择过程中，使用特征$A$对数据集$D$进行分割，所带来的信息增益可以表示为：$g(D,A)=MI(D,A)=H(D)-H(D\mid A)$



**信息增益比**

有以上定义可知，通过计算信息增益来进行数据分割时，会偏向取值较多的特征的问题，针对此问题，使用信息增益比来进行修正：

$g_{R}(D,A)=\frac{g(D,A)}{H_{A}(D)}$，其中$H_A(D)=-\sum_\limits{i=1}^{n}{\frac{\mid D_i\mid}{\mid D\mid}}\log{\frac{\mid D_i\mid}{\mid D\mid}}$，即计算数据集$D$的熵时，特征$A$的贡献。



**基尼指数Gini Index**

在分类问题中，假设有$k$个类别，样本属第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：

$$Gini(p)=\sum_\limits{k=1}^{K}p_k(1-p_k)=1-\sum_\limits{k=1}^{K}p_k^2$$

则数据集$D$的基尼指数为：

$$Gini(D)=1-\sum_\limits{k=1}^{K}\frac{\mid C_k\mid}{\mid D \mid}$$

其中，$C_k$表示第$k$类样本子集的大小。

假设有特征A，根据特征$A$是否满足特定条件，将数据集划分为两部分：$D_1,D_2$，则可以定义：给定特征$A$的条件下，样本集合$D$的基尼指数$Gini(D,A)$（即给定特征$A$后，样本集合$D$的不确定性减少的程度）：

$$Gini(D,A)=\frac{\mid D_1\mid}{\mid D \mid}Gini(D_1)+\frac{\mid D_2\mid}{\mid D \mid}Gini(D_2)$$

在决策树的构建阶段，我们便可以在进行最优特征选择时，遍历所有的特征，然后计算出其纯度指标（信息增益、信息增益比、基尼指数），然后选出一个指标最高的特征，并根据该特征的取值将样本集合分裂。

常用的决策树算法有ID3算法、C4.5算法和CART算法，其中ID3算法使用信息增益进行特征选择；C4.5算法进行了改进，使用信息增益比进行特征选择；对于分类树，CART算法使用基尼指数最小化准则，对于回归树，使用平方误差最小化准则。



## 二、Boosting

Boosting是集成学习思想的一种（另一个是Bagging，指并列地训练多个基模型，并通过投票（分类问题）或平均（回归问题）的方式计算得到最终的集成结果）。

与Bagging的可并行不同，Boosting算法的执行流程是串行的，即后面的模型的训练依赖前面的模型的预测结果。典型的算法有Adaboost、GBDT等。下面简单介绍这两类算法。

### 2.1 Adaboost

Adaboost算法是boosting算法的代表，其特点是通过迭代训练一系列基分类器，在每次迭代过程中，提高被前一个分类器分错的样本的权重，降低被前一个分类器分对的样本的权重。最后，Adaboost算法将基分类器进行加权线性组合，每个基分类器都有一个权重，分类误差越小的基分类器，其权重越高，否则权重越低。

Adaboost的执行流程简述如下：

首先，初始化数据集的权重为等权重。接着，依次迭代训练$M$个基模型$G_1,G_2,...,G_M$，针对模型$G_m$，使用带权的数据进行训练，然后，计算$G_m$在数据集上的带权分类误差率，并通过此误差率计算分类器的权重。然后通过带权基模型$G_m$的预测结果，更新训练数据的权重。在$M$个基模型训练完成后，每个基模型均有一个权重，使用该权重对一系列基模型进行加权组合，即可得到最终的分类器。











## 参考

1. 统计学习方法
2. https://blog.csdn.net/v_JULY_v/article/details/81410574





