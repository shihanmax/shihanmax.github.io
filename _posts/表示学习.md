---
title:  "表示学习"
layout: post
date: 2020-10-12 09:35:03
categories: NLP
tags:  "Representation Learning"
syntaxHighlighter: yes
Mathjax: true
---


现实世界中的数据模态类型很多，如图像，文本，表格，模拟信号等，表示学习的任务是，针对特定的模态，自动学习到一种对数据的表示，该表示能够方便地使用数学描述和计算的同时，对数据中的高信息进行抽象和聚合。从而尽可能减弱对特征工程的依赖。
好的表示学习可以使后续的任务更加容易，通过监督学习训练处的前馈网络可以视为一种表示学习，在学习过程中，我们并没有给中间层特征施加任何约束；在其他表示学习算法中，往往会显式地明确表示的设计。
大多数表示学习算法都会在尽可能多地保留输入相关信息和追求良好的性质（如输出独立性）之间作权衡。

## 表示学习的类型

- 贪心逐层无监督预训练



## 好的表示应当具有的性质

- Smoothness
函数光滑性是目前多数机器学习所依赖的优化算法的基础；

- Multiple explanatory factors
真实数据是在很多潜变量的控制下生成的，找到这些潜变量或至少弄清这些潜变量的影响目标对一个好的表示学习系统很重要，
探究影响真实数据的潜变量的过程，实际也是为表示增加可解释性的过程，这个过程有助于更有效的迁移学习和生成任务；

- Hierarchical representations
表示所包含的信息应当具有一定的层次结构，使得低阶特征具有更高的可复用性；

- Share factors across tasks
表示应能够具有一定的领域适应能力，方便在不同任务、不同模态间迁移（表示适用于多任务学习、迁移学习、领域自适应学习）；

- Low dimensional manifold
好的表示应在存在于更低维的空间（流型）中（“The better representation lies in lower dimensional manifold”），这个说法与Occam's Razor原理像是在描述同一个问题的不同角度，当然，这里的“低”是有一个界限的，不可能无限制地要求表示向量存在于更低的维度，当维度到达这个界限时，表示向量能够完美的容纳样本的所有有益信息，同时不包含任何噪声，这在实际应用中是不现实的，这就要求一个好的表示学习算法能够在保留有益信息的同时，使用更科学、更有效的方式，丢弃表示中包含的噪声；

- Temporal and spacial cohence
表示向量的时空一致性，要求表示向量在时间维度和空间维度上是“连续可微分”的，举一个简单的例子，用户的购物兴趣应该随着时间的变化逐渐改变，而不是在时间维度上随意转换；

- Sparsity
稀疏性与Low dimensional manifold相辅相成，好的稀疏性会降低模型的参数量，忽略掉无关的噪声，从另一个角度而言，假设输入为x。其表示为E，当E对x所受的微小扰动不敏感时，也可以认为这个表示具有较好的稀疏性；

- Simplicity of factor dependencies
一个好的表示，其factor之间一定具有一些简单的关系，如线性依赖，这个假设在很多物理定理中也有所体现。也正因如此，我们在一个表示层上方接一个线性层分类器才是合理的和符合直觉的。

	1. [https://arxiv.org/pdf/1206.5538.pdf ](Representation Learning: A Review and New Perspectives)
