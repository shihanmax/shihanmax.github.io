## NLG review

Categories:

- Text-to-Text
- Data-to-Text
- Table-to-Text
- Vision-to-Text (Image caption)

我们重点关注第一个：text-to-text。



Difficulties of NLG:

- grammatical complexity of natural language
- difficulties during the extraction, simplification and transformation of the input information (task-specific)



Applications of text-to-text NLG:

- machine translation
- fusion and summarization of related sentences or texts to make them more concise
- **simplification of complex texts**
- automatic spelling, grammar and text correction
- automatic generation of peer reviews / stories / News
- generation of paraphrases of input sentences
- automatic generation of questions, for educational and other purposes



## 文本生成模型类别

<img src="http://qiniu.shihanmax.top/20210302194833_YQAZ0Z_%E6%88%AA%E5%B1%8F2021-03-02%2019.48.15.jpeg" alt="generate models" style="zoom:50%;" />



VAE based models

- CVAE
- vae-seq2seq
- 多级隐变量vae
- PHVM（Planning-based Hierarchical Variational Model）



GAN based models

- SeqGAN

  suffer from two problems:

  - gradient vanishing

    when the discriminator is trained to be much stronger than the generator, it becomes extremely hard for the generator to have any actual updates since any output instances of the generator will be scored as almost 0. This may cause the training stops too early before it comes to the true convergence or Nash Equilibrium.

  - mode collapse

    which increases the estimated probability of sampling particular to- kens earning high evaluation from the discriminator. As a result, the generator only manages to mimic a limited part of the target distribution, which significantly reduces the diversity of the outputs.



RL based models



CNN based models

- Convolutional Sequence to Sequence Learning（FAIR）



Seq2seq based models

- seq2seq+attention（[A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)）
- CopyNet（[Incorporating Copying Mechanism in Sequence-to-Sequence Learning](https://arxiv.org/pdf/1603.06393.pdf)）
- coverage机制
- 改进注意力：[Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/N18-2009)
- seq2seq pointer generator networks（[Get To The Point: Summarization with Pointer-Generator Networks](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/P17-1099)）



我们重点关注seq2seq based models。



## 文本摘要模型



文本摘要任务有两种思路：抽取式摘要（Extractive）和生成式摘要（Abstractive），两者的特点分别是：

抽取式：抽出的内容合理，但行文不够流畅

生成式：生成内容比较流畅，但内容可控性差，易与原文产生差别



为了融合两种思路的优点，近些年有许多工作尝试了抽取+生成的方式进行摘要。下文分别选取抽取式、生成式、抽取+生成融合三种文本摘要方法中，比较典型的几篇工作，介绍目前基于seq2seq进行文本摘要的进展。



### A. 抽取式摘要：

1. [SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents](https://arxiv.org/pdf/1611.04230.pdf)

采用双层RNN，底层对每一个句子进行编码，顶层RNN对句子序列做标注，生成0/1标签，用于表示是否在摘要中选取该句子（显然，这个模型在做的是在篇章中选择重要的句子，而对于单个句子内的词，则没有进一步选择）

该模型的另一个设计是loss，可以参考如下。

<img src="http://qiniu.shihanmax.top/20210303151419_HMr5Eb_v2-27da99d4d46e629c9672c7eb695c71ce_720w.jpeg" alt="sheng" style="zoom: 67%;" />



2. [Neural Summarization by Extracting Sentences and Words](https://arxiv.org/pdf/1603.07252.pdf)

<img src="http://qiniu.shihanmax.top/20210303191433_miAmFi_%E6%88%AA%E5%B1%8F2021-03-03%2019.05.53.jpeg" alt="1" style="zoom:67%;" />

本文的文本摘要分为两个层级，首先从篇章中选择句子（Sentence extractor, SE），其次从句中选择词语（Word extractor, WE）。前者是一个序列标注任务，后者是一个文本生成的任务。

使用CNN编码句子，对于篇章中的N个句子，得到其表示后，进入RNN的encoder、decoder，得到每一个句子是否选择的结果（0/1）；Word extractor类似一个生成任务，负责选择重要的成分，并将它们合理地、流畅地组织起来。WE接收SE选择的若干个sentence的表示，作为decoder的输入，直接解码得到生成summary的下一个词。WE使用hierarchical attention结构，首先计算对各个sentence embedding的attention，然后计算对sentence中每个词的attention，使用softmax得到一个词。WE可以看作一个受限词表的条件语言模型。



### B. 生成式摘要：

1. [A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)

Attention-Based Summarization，2015年提出，比较早的模型了，借鉴NMT，典型的Seq2seq+attetion的生成模型。解码词表使用全量词表。



2. [Deep Recurrent Generative Decoder for Abstractive Text Summarization∗](https://arxiv.org/pdf/1708.00625.pdf)（2017）

<img src="http://qiniu.shihanmax.top/20210303164328_ze9oYg_%E6%88%AA%E5%B1%8F2021-03-03%2016.42.20.jpeg" alt="1" style="zoom:67%;" />

使用Seq2seq+attn结构，创新点在解码端：For latent structure modeling, we add historical dependencies on the latent variables of Variational Auto-Encoders (VAEs)。



3. [Abstractive Sentence Summarization with Attentive Recurrent Neural Networks](https://pdfs.semanticscholar.org/7a67/159fc7bc76d0b37930b55005a69b51241635.pdf)

embedding使用token embedding+可训练的位置embedding，encoder使用CNN（attentive），每个token得到一个编码表示$z_i$，解码使用RNN。



### C. 抽取+生成结合：

抽取式摘要无法生成除原文本以外的词汇；而生成式摘要无法避免OOV问题，近期有很多模型尝试将二者进行融合，即在解码阶段动态地选择从原文本中”抽取式“地得到一个词，或从候选词表中”生成式“地得到一个词。

比较有代表性的工作有：

- Pointer-Generator Network（PGN）（Google+Stanford）
- CopyNet（港大+华为）



1. [Get To The Point: Summarization with Pointer-Generator Networks](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/P17-1099)（PGN）

<img src="http://qiniu.shihanmax.top/20210303193545_u9XA0S_v2-0ec20003c6721cf61687024eab09e5a4_720w.jpeg" alt="PGN" style="zoom:80%;" />

不同于传统的Seq2seq+attention模型，PGN在解码阶段生成一个概率$P_{gen}$，用于表示当前解码时间步需要从完整词表中生成一个词的概率，则1减去该概率表示从原文本中选择一个单词的概率，由于解码过程中，pointer部分可以得到原文本中每个词的选择概率，同时，解码器也能获得词表中每一个单词的生成概率，使用概率$P_{gen}$将这两个概率分布进行整合，得到最终的词汇分布概率。



2. [Incorporating Copying Mechanism in Sequence-to-Sequence Learning](https://arxiv.org/pdf/1603.06393.pdf)

<img src="http://qiniu.shihanmax.top/20210303194546_T941SX_v2-e6e965ade7a4bc6d99fa38817f7f6d2e_r.jpeg" alt="CPN" style="zoom:67%;" />

这篇文章的思路和上文的PGN比较类似，在上图右上的红框中，解码器得到的概率分布分为两部分，词表中的词汇概率分布和原文本中的词汇概率分布，PGN使用一个软概率将二者进行融合，而本文则直接地进行加和。

![probe of word](http://qiniu.shihanmax.top/20210303200612_9CgIGU_v2-5500109d88e148ed1e2ce8a0746f5c58_720w.jpeg)

如上图，可以简单理解为：

- 如果一个词仅存在于原文本，则其来源于生成模型的概率为0，来自选择模型的概率不变
- 如果一个词仅存在于词表中，则其来源于生成模型的概率保持不变，来自选择模型的概率为0
- 如果一个词既存在于原文本中，也存在于词表中，则其生成概率为两个概率的加和

值得注意的是，生成概率和选择概率的计算方式不同：

<img src="http://qiniu.shihanmax.top/20210303201552_JqaF3F_%E6%88%AA%E5%B1%8F2021-03-03%2020.15.49.jpeg" alt="probe" style="zoom:80%;" />



3. [A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss](https://www.aclweb.org/anthology/P18-1013.pdf)

![attn_fusion](http://qiniu.shihanmax.top/20210303201923_SLHTYJ_%E6%88%AA%E5%B1%8F2021-03-03%2020.19.18.jpeg)

本文创新点如下：

- 两层attention：sentence level、word level，使用前者调节后者，能够达到”降低位于低注意力的句子中的单词被生成的概率“。

  模型分为两部分：extractor、abstracter，二者均以单个句子为输入，extractor输出一个概率序列，表征输入**句子序列**被”抽取“的概率（sentence level attention）。同时，abstracter动态地计算当前句子中每个词被生成的概率。对两种attention score的聚合则是简单地进行分数相乘，然后作归一化。

- 为了消除两层attention的矛盾性（inconsistentency），引入inconsistentency loss

  为了对两种attention分数作统一（consistent），这里引入inconsistentency loss：

  ![incloss](http://qiniu.shihanmax.top/20210303203944_moJrwm_%E6%88%AA%E5%B1%8F2021-03-03%2020.39.41.jpeg)



![extractor](http://qiniu.shihanmax.top/20210303203357_M5BLCe_%E6%88%AA%E5%B1%8F2021-03-03%2020.33.54.jpeg)

![1](http://qiniu.shihanmax.top/20210303204513_7RJXZY_%E6%88%AA%E5%B1%8F2021-03-03%2020.45.09.jpeg)

abstracter的结构和PGN是一致的。

本文同时也使用了PGN中的coverage思想：使用coverage vector来维护截至当前，在每个词汇上的注意力如何，coverage vector会参与word level attention的计算，并通过coverage loss 的方式参与训练。

![cv](http://qiniu.shihanmax.top/20210303204904_Rt2l7O_%E6%88%AA%E5%B1%8F2021-03-03%2020.49.00.jpeg)



4. [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](https://arxiv.org/pdf/1602.06023.pdf)

![1](http://qiniu.shihanmax.top/20210303164251_0FpWXm_%E6%88%AA%E5%B1%8F2021-03-03%2016.03.17.jpeg)

论文点：

- 使用概率控制从vocab中生成还是从原文中选择一个
- large vocabulary ‘trick’：the decoder-vocabulary of each mini-batch is re- stricted to words in the source documents of that batch. In addition, the most frequent words in the target dictionary are added until the vocabulary reaches a fixed size. 限制解码词表大小。优化softmax（。。。）
- Feature-rich Encoder：encoder输入：词embedding+POS+NER+TF+IDF
- Hierarchical Attention to capture sentence structure



参考：

1. comprehensive review：[Neural Abstractive Text Summarization with Sequence-to-Sequence Models](https://arxiv.org/pdf/1812.02303.pdf)
2. paper collection：https://github.com/neulab/Text-Summarization-Papers
3. [SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents](https://arxiv.org/pdf/1611.04230.pdf)
4. [Neural Summarization by Extracting Sentences and Words](https://arxiv.org/pdf/1603.07252.pdf)
5. [Deep Recurrent Generative Decoder for Abstractive Text Summarization](https://arxiv.org/pdf/1708.00625.pdf)
6. [Abstractive Sentence Summarization with Attentive Recurrent Neural Networks](https://pdfs.semanticscholar.org/7a67/159fc7bc76d0b37930b55005a69b51241635.pdf)
7. [A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss](https://www.aclweb.org/anthology/P18-1013.pdf)
8. [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](https://arxiv.org/pdf/1602.06023.pdf)
9. [Neural Text Generation: Past, Present and Beyond](https://arxiv.org/pdf/1803.07133.pdf)
10. [Survey of the State of the Art in Natural Language Generation: Core tasks, applications](https://arxiv.org/pdf/1703.09902.pdf)
11. [Text Generation Survey 2017 by Ni Lao](https://noon99jaki.github.io/publication/2017-text-gen.pdf)
12. seq2seq+attention：[A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)
13. CopyNet：[Incorporating Copying Mechanism in Sequence-to-Sequence Learning](https://arxiv.org/pdf/1603.06393.pdf)
14. coverage机制：[Extractive-abstractive summarization with pointer and coverage mechanism](https://dl.acm.org/doi/abs/10.1145/3226116.3226126)
15. 改进注意力：[Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/N18-2009)
16. seq2seq pointer generator networks：[Get To The Point: Summarization with Pointer-Generator Networks](https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/P17-1099)

