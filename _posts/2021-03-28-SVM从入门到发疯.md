---
title:  "SVM从入门到发疯"
layout: post
date: 2021-03-28 12:52:03
categories: NLP
tags:  ["Machine Learning", "SVM"]
syntaxHighlighter: yes
Mathjax: true
---

## 零、前言

支持向量机（Support vector machine, SVM）是一种用于二分类的线性分类器，区别于感知机，SVM使用中定义在特征空间上的间隔最大化分类器。支持向量机可以处理线性分类，引入核技巧后，支持向量机则成为一个非线性分类器。当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，核函数表示将输入从输入空间映射到特征空间的到的向量之间的内积。此时等价于在高维特征空间中学习一个线性分类器。

优化方面，支持向量机的学习目标是间隔最大化，本质上一个凸二次规划问题，也等价于正则化的合叶损失函数（Hinge loss）最小化问题。

下文以线性分类器开始，引入间隔最大化的目标函数，接着介绍硬间隔SVM和加入松弛因子的软间隔SVM，然后讨论用于求解带不等式约束优化问题的KKT条件、SVM目标函数的原始问题（Prime form）向对偶形式（Dual form）的转换，在此基础上，讨论在SVM中引入核技巧的方式，最后，对序列最小化算法（SMO）进行简单的介绍。

## 一、由线性分类器到间隔最大化

回顾线性分类问题：

输入：$D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\},\quad x_i \in \mathbb{R}^D, y_i\in\{-1,1\},i=1,2,...,n$

参数：$W,b$

我们可以非常容易的找到一组参数$W,b$，由二者确定一个平面$W^T x+b=0$，使得对于正样本，我们有$W^T x_i+b \ge 0, y_i=1$；对于负样本，有$W^T x_i+b<0, y_i=-1$，因此有$(W^T x_i+b)y_i \ge 0$。

由于数据线性可分，我们可以找到无数组参数$W,b$，使得在给定数据集$D$上，满足上述条件。那么，这里就引出了一个问题，我们如何确定哪一个平面才是最优的呢？或者说，满足什么样条件的超平面才叫做“优”呢？

![liner_problem](http://qiniu.shihanmax.top/20210402132120_TFTD9L_%E6%88%AA%E5%B1%8F2021-04-02%2013.21.13.jpeg)

<center>图1 线性分类问题</center>

由上图可知，平面$1,2,3$都可以将正负样本完全分开，如果我们稍微调整一下距离平面$1$或$3$比较近的样本的位置，使其约过超平面，则平面$1,3$必须重新移动以满足$(W^T x_i+b)y_i \ge 0$的条件，而平面$2$对这种情况则更鲁棒一些。也就是说，直观上看平面$2$似乎是最优的，因为平面$1,3$对噪声不够鲁棒。

这里我们似乎找到了一些有关如何衡量“平面是更优的”的一个思路：对那些距离超平面近的点的波动不敏感。

接下来引入SVM的思想：

上文中讨论了当超平面将正负样本完全分开时，满足$(W^T x_i+b)y_i \ge 0$，这里定义函数间隔$\hat{\gamma}$：$\hat{\gamma}=(w^Tx+b)y$，数据集$D$中，所有样本的函数间隔最小值定义为超平面$ W,b$关于数据集$D$的函数间隔。

函数间隔存在一个问题，即它的值随$W,b$的变化而变化，考虑一种情况，当我们等比例将$W,b$放大为原来的两倍时，平面位置并没有发生移动，但超平面关于数据集的函数间隔却变成了原来的二倍。

针对上述情况，SVM中引入了几何间隔，它在函数间隔中考虑了$W$的模：$\widetilde{\gamma}=\frac{\hat{\gamma}}{\Vert W \Vert}$



















## 二、硬间隔和软间隔

## 三、KKT条件

## 四、原始问题和对偶问题

## 五、核技巧

## 六、SMO算法

