---
title:  "MLE & MAP"
layout: post
date: 2020-08-16 09:35:03
categories: NLP
tags: ["Machine Leanrning"]
syntaxHighlighter: yes
Mathjax: true
---

绝大部分机器学习的优化过程最终都会转化为求目标函数最值的过程，MLE（Maximum Likelihood Estimation，最大似然估计）和MAP（Maximum A Posteriori，最大后验估计）是两种生成目标函数的思想。下文尝试讨论MLE和MAP的区别和联系。

MLE仅通过观测样本对参数进行估计，MLE的目标函数为$\arg\max P(D\mid \theta)$，而MAP不仅通过观测的样本，同时也通过先验（priori）进行参数的估计，其目标函数为$\arg \max P(\theta \mid D)$。

# 最优化MAP

$$\begin{aligned} \arg\max P(\theta \mid D) &= \arg\max P(D\mid\theta) \cdot P(\theta)\\ &=\arg\max \log P(D\mid \theta) + \log P(\theta) \end{aligned}$$

由上式可以看出，MAP的目标函数的第一项为MLE的目标函数，第二项$\log(\theta)$相当于优化MAP的过程中，对MLE目标函数施加的正则化项，下文将证明，随着$\theta$分布的不同，正则化项的类型也不同。

下文以Logistic Regression为例：

$$\begin{aligned} objective\ of\ MAP &= \arg\max P(\theta \mid D) \\ &=\arg\max P(D\mid \theta) \cdot P(\theta) \\ &=\arg\max \log P(D\mid \theta) + \log P(\theta) \\&=\arg\max \sum_\limits{i=1}^n \log P(y_i\mid x_i;\theta) + \log P(\theta)  \end{aligned}$$

# 从高斯先验到$L_2$正则

假设参数$\theta$服从高斯分布：

$$\begin{aligned}logP(\theta)&=log(\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{\theta^2}{2\sigma^2})) \\&=-log(\sqrt{2\pi}\sigma) - \frac{\theta^2}{2\sigma^2}  \end{aligned}$$

则MAP的目标函数可写作：

$$\begin{aligned} objective\ of\ MAP &=\arg\max \sum_\limits{i=1}^n \log P(y_i\mid x_i;\theta) -log(\sqrt{2\pi}\sigma) - \frac{\theta^2}{2\sigma^2} \\&=\arg\max \sum_\limits{i=1}^n \log P(y_i\mid x_i;\theta)-\frac{1}{2\sigma^2}\Vert \theta \Vert_2^2 \end{aligned}$$

即：当参数服从高斯分布时，MAP目标函数中的$\log P(\theta	)$项相当于对MLE目标函数施加的$L_2$正则。

# 从拉普拉斯先验到$L_1$正则

假设参数$\theta \sim Laplace(\mu,b),\mu=0$，则：$P(\theta)=\frac{1}{2b}exp(-\frac{\vert\theta\vert}{b})$

$$\begin{aligned} \log P(\theta)&=\log(\frac{1}{2b}exp(-\frac{\vert\theta\vert}{b})) \\&=log(\frac{1}{2b})-\frac{\vert\theta\vert}{b}\end{aligned}$$

MAP的目标函数可写作：

$$\begin{aligned} objective\ of\ MAP &=\arg\max \sum_\limits{i=1}^n \log P(y_i\mid x_i;\theta)  -\frac{1}{b}\Vert\theta\Vert_1  \end{aligned}$$

即：当参数服从拉普拉斯分布时，MAP目标函数中的$\log P(\theta	)$项相当于对MLE目标函数施加的$L_1$正则。

上述推导以LR为例验证了“Adding Priori is Equivalent to Regularization”的结论，事实上，在MLE的框架下，上述结论可推广至任何模型。

# MAP approaches to MLE solution

回顾MAP的目标函数：

$$\begin{aligned} objective\ of\ MAP =\arg\max \sum_\limits{i=1}^n \log P(y_i\mid x_i;\theta) + \log P(\theta)  \end{aligned}$$

目标函数的第一部分是一个概率加项，第二项仅与模型的参数有关，当样本数量趋于无限大，即$n \to \infty$时，正则化项的影响减弱至无，此时MAP退化为MLE。

# Ref.

[1. CMU-10701:2. MLE, MAP, Bayes classification](http://www.cs.cmu.edu/~aarti/Class/10701_Spring14/slides/MLE_MAP_Part1.pdf)

